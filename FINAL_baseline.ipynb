{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hf6wMwHBqC9",
        "outputId": "0e0d2f4f-827e-49e7-bcb1-a48f98c974bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOr77FWMBrBs",
        "outputId": "f5093692-9acb-421f-9a67-345f0bd09d52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9VkRememCDIq"
      },
      "outputs": [],
      "source": [
        "# Set the device to GPU if available, otherwise CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7V-q8GrAE3x",
        "outputId": "09d713c4-504b-4637-bd5a-4dd669bb8b26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique articles: 11777\n",
            "Example article ID and tokenized title:\n",
            "Article ID: 3037230, Tokenized Title: ['ishockey-spiller', ':', 'jeg', 'troede', 'jeg', 'skulle', 'dø']\n",
            "Article ID: 3044020, Tokenized Title: ['prins', 'harry', 'tvunget', 'til', 'dna-test']\n",
            "Article ID: 3057622, Tokenized Title: ['rådden', 'kørsel', 'på', 'blå', 'plader']\n",
            "\n",
            "Total articles indexed in newsindex: 11778 \n",
            "\n",
            "Training Data:\n",
            "Number of training candidates: 24888\n",
            "Number of training labels: 24888\n",
            "Number of user histories: 24888\n",
            "\n",
            "Validation Data:\n",
            "Number of validation candidates: 25505\n",
            "Number of validation labels: 25505\n",
            "Number of user histories: 25505\n"
          ]
        }
      ],
      "source": [
        "MAX_TITLE_LEN = 20\n",
        "\n",
        "articles_path = '/content/drive/MyDrive/NRMS/OG data/articles.parquet'\n",
        "behaviors_train_path = '/content/drive/MyDrive/NRMS/OG data/behaviors_train.parquet'\n",
        "behaviors_val_path = '/content/drive/MyDrive/NRMS/OG data/behaviors_val.parquet'\n",
        "history_train_path = '/content/drive/MyDrive/NRMS/OG data/history_train.parquet'\n",
        "history_val_path = '/content/drive/MyDrive/NRMS/OG data/history_val.parquet'\n",
        "\n",
        "# Load data from Parquet files\n",
        "articles = pd.read_parquet(articles_path)\n",
        "train_behaviors = pd.read_parquet(behaviors_train_path)\n",
        "val_behaviors = pd.read_parquet(behaviors_val_path)\n",
        "history_train = pd.read_parquet(history_train_path)\n",
        "history_val = pd.read_parquet(history_val_path)\n",
        "\n",
        "# Create a news dict and  article_id to a unique index\n",
        "news = {}\n",
        "newsindex = {'NULL': 0}  # Add a NULL key for padding\n",
        "for idx, row in articles.iterrows():\n",
        "    article_id = row['article_id']\n",
        "    title = row['title'].lower()\n",
        "    tokenized_title = word_tokenize(title)\n",
        "\n",
        "    news[article_id] = tokenized_title\n",
        "    newsindex[article_id] = len(newsindex)\n",
        "\n",
        "# Summary\n",
        "print(\"Number of unique articles:\", len(news))\n",
        "print(\"Example article ID and tokenized title:\")\n",
        "for k, v in list(news.items())[:3]:  # Print first 3 articles\n",
        "    print(f\"Article ID: {k}, Tokenized Title: {v}\")\n",
        "\n",
        "print(\"\\nTotal articles indexed in newsindex:\", len(newsindex), \"\\n\")\n",
        "\n",
        "# Helper function to sample negative examples\n",
        "def newsample(array, ratio):\n",
        "    if len(array) == 0:\n",
        "        return []\n",
        "    if ratio > len(array):\n",
        "        return random.sample(array * (ratio // len(array) + 1), ratio)\n",
        "    else:\n",
        "        return random.sample(array, ratio)\n",
        "\n",
        "# Sampling configuration\n",
        "npratio = 4  # Number of negative samples per positive sample\n",
        "MAX_HISTORY_LEN = 50\n",
        "\n",
        "# Function to process behaviors data\n",
        "def process_behaviors(behaviors, newsindex, history_data=None):\n",
        "    train_candidate = []\n",
        "    train_label = []\n",
        "    train_user_his = []\n",
        "\n",
        "    # Build a user history dictionary from history data if provided\n",
        "    user_history = {}\n",
        "    if history_data is not None:\n",
        "        for _, row in history_data.iterrows():\n",
        "            user_history[row['user_id']] = [newsindex.get(aid, 0) for aid in row['article_id_fixed']]\n",
        "\n",
        "    for _, row in behaviors.iterrows():\n",
        "        user_id = row['user_id']\n",
        "\n",
        "        # Clicked articles (positive examples)\n",
        "        clicked = [newsindex.get(aid, 0) for aid in row['article_ids_clicked'] if aid in newsindex]\n",
        "        # Non-clicked articles (negative examples)\n",
        "        inview = set(row['article_ids_inview'])\n",
        "        non_clicked = [newsindex.get(aid, 0) for aid in inview if aid in newsindex and aid not in row['article_ids_clicked']]\n",
        "\n",
        "        # User history\n",
        "        if user_id in user_history:\n",
        "            clickids = user_history[user_id][-MAX_HISTORY_LEN:]\n",
        "        else:\n",
        "            clickids = clicked[-MAX_HISTORY_LEN:]\n",
        "\n",
        "        for pos_doc in clicked:\n",
        "            neg_docs = newsample(non_clicked, npratio)\n",
        "            candidates = neg_docs + [pos_doc]\n",
        "            labels = [0] * npratio + [1]\n",
        "\n",
        "            # Shuffle candidates and labels\n",
        "            shuffle_indices = list(range(len(candidates)))\n",
        "            random.shuffle(shuffle_indices)\n",
        "            shuffled_candidates = [candidates[i] for i in shuffle_indices]\n",
        "            shuffled_labels = [labels[i] for i in shuffle_indices]\n",
        "\n",
        "            # Append training data\n",
        "            train_candidate.append(shuffled_candidates)\n",
        "            train_label.append(shuffled_labels)\n",
        "            train_user_his.append(clickids + [0] * (MAX_HISTORY_LEN - len(clickids)))\n",
        "\n",
        "    return train_candidate, train_label, train_user_his\n",
        "\n",
        "# Process train behaviors\n",
        "train_candidate, train_label, train_user_his = process_behaviors(train_behaviors, newsindex, history_train)\n",
        "val_candidate, val_label, val_user_his = process_behaviors(val_behaviors, newsindex, history_val)\n",
        "\n",
        "# Print summary\n",
        "print(\"Training Data:\")\n",
        "print(\"Number of training candidates:\", len(train_candidate))\n",
        "print(\"Number of training labels:\", len(train_label))\n",
        "print(\"Number of user histories:\", len(train_user_his))\n",
        "\n",
        "print(\"\\nValidation Data:\")\n",
        "print(\"Number of validation candidates:\", len(val_candidate))\n",
        "print(\"Number of validation labels:\", len(val_label))\n",
        "print(\"Number of user histories:\", len(val_user_his))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qYT4QYnAE4b",
        "outputId": "1787cad2-2d29-4f82-e912-28380d5fccf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train and Validation data saved as Parquet files:\n",
            "  - train_data.parquet\n",
            "  - val_data.parquet\n"
          ]
        }
      ],
      "source": [
        "# Save Train Data\n",
        "train_df = pd.DataFrame({\n",
        "    'candidate': train_candidate,\n",
        "    'label': train_label,\n",
        "    'user_his': train_user_his\n",
        "})\n",
        "train_df.to_parquet('/train_data.parquet', index=False)\n",
        "\n",
        "# Save Validation Data\n",
        "val_df = pd.DataFrame({\n",
        "    'candidate': val_candidate,\n",
        "    'label': val_label,\n",
        "    'user_his': val_user_his\n",
        "})\n",
        "val_df.to_parquet('/val_data.parquet', index=False)\n",
        "\n",
        "# Print Confirmation\n",
        "print(\"Train and Validation data saved as Parquet files:\")\n",
        "print(\"  - train_data.parquet\")\n",
        "print(\"  - val_data.parquet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXwYYlW5AE4c",
        "outputId": "6be3cd23-b3c8-4ed1-865d-fc1a982b94ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 16003\n",
            "Shape of padded news tensors: torch.Size([11778, 20])\n"
          ]
        }
      ],
      "source": [
        "# Build vocabulary from tokenized titles\n",
        "vocab = {'<PAD>': 0}  # Start with a padding token\n",
        "for tokens in news.values():\n",
        "    for word in tokens:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "# Convert titles to token indices\n",
        "news_tensor = {}\n",
        "for article_id, tokens in news.items():\n",
        "    token_indices = [vocab[word] for word in tokens]  # Convert words to token indices\n",
        "\n",
        "    # Truncate or pad to MAX_TITLE_LEN\n",
        "    if len(token_indices) > MAX_TITLE_LEN:\n",
        "        token_indices = token_indices[:MAX_TITLE_LEN]  # Truncate if too long\n",
        "    else:\n",
        "        token_indices += [0] * (MAX_TITLE_LEN - len(token_indices))  # Pad with zeros\n",
        "\n",
        "    news_tensor[newsindex[article_id]] = torch.tensor(token_indices, dtype=torch.long)\n",
        "    news_tensor[0] = torch.zeros(MAX_TITLE_LEN, dtype=torch.long)\n",
        "\n",
        "news_tensors_list = [tensor[:MAX_TITLE_LEN] for tensor in news_tensor.values()]  # Truncate to MAX_TITLE_LEN\n",
        "news_tensors_padded = pad_sequence(news_tensors_list, batch_first=True, padding_value=vocab['<PAD>'])\n",
        "\n",
        "# Example Output\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "print(\"Shape of padded news tensors:\", news_tensors_padded.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq6JVt1NI_gI",
        "outputId": "20380d5e-4824-49c5-f8aa-516bb44e6452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2404837 word vectors from GloVe.\n"
          ]
        }
      ],
      "source": [
        "def load_glove_embeddings(file_path, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embedding = np.array(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = embedding\n",
        "    print(f\"Loaded {len(embeddings_index)} word vectors from GloVe.\")\n",
        "    return embeddings_index\n",
        "\n",
        "# Load GloVe embeddings (300d)\n",
        "glove_embeddings = '/content/drive/MyDrive/NRMS/danish_newspapers_1880To2013.txt'\n",
        "embedding_dim = 300\n",
        "glove_index = load_glove_embeddings(glove_embeddings, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HvRTyyRMJue9"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(vocab, glove_index, embedding_dim):\n",
        "    \"\"\"\n",
        "    Create an embedding matrix for a given vocabulary using GloVe embeddings.\n",
        "\n",
        "    Args:\n",
        "        vocab (dict): Vocabulary mapping words to indices.\n",
        "        glove_index (dict): GloVe embeddings loaded from the file.\n",
        "        embedding_dim (int): Dimension of the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The embedding matrix.\n",
        "        int: Count of OOV words.\n",
        "        int: Count of IV words.\n",
        "    \"\"\"\n",
        "    vocab_size = len(vocab)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))  # Initialize with zeros\n",
        "\n",
        "    for word, idx in vocab.items():\n",
        "        if word in glove_index:  # Use GloVe embedding if available\n",
        "            embedding_matrix[idx] = glove_index[word]\n",
        "        else:  # Random initialization for words not in GloVe\n",
        "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_matrix = create_embedding_matrix(vocab, glove_index, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsQbw73Bsjy0",
        "outputId": "1d2a6b33-c7ad-4398-feea-47d4301c96a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 16003\n",
            "In-Vocabulary words: 12290 (76.80%)\n",
            "Out-of-Vocabulary words: 3713 (23.20%)\n",
            "Sample OOV words: ['<PAD>', 'ishockey-spiller', ':', 'dna-test', 'mærsk-arvinger', 'zoo-tårnet', '100', 'creamy-pige', '-', 'champagne-drengen']\n"
          ]
        }
      ],
      "source": [
        "# Vocabulary analysis\n",
        "total_vocab_size = len(vocab)\n",
        "iv_count = len([word for word in vocab if word in glove_index])\n",
        "oov_count = total_vocab_size - iv_count\n",
        "\n",
        "iv_percentage = (iv_count / total_vocab_size) * 100\n",
        "oov_percentage = (oov_count / total_vocab_size) * 100\n",
        "\n",
        "print(f\"Vocabulary size: {total_vocab_size}\")\n",
        "print(f\"In-Vocabulary words: {iv_count} ({iv_percentage:.2f}%)\")\n",
        "print(f\"Out-of-Vocabulary words: {oov_count} ({oov_percentage:.2f}%)\")\n",
        "\n",
        "# Sample OOV words\n",
        "oov_words = [word for word in vocab if word not in glove_index]\n",
        "print(\"Sample OOV words:\", oov_words[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2X-EM-W_GbaK"
      },
      "outputs": [],
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data, news_tensor, max_history_len=50):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (pd.DataFrame): DataFrame with 'candidate', 'label', and 'user_his'\n",
        "            news_tensor (dict): Dictionary mapping article_id to tokenized title tensors\n",
        "            max_history_len (int): Maximum length of user history (default: 50)\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.news_tensor = news_tensor\n",
        "        self.max_history_len = max_history_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # Candidate titles: List of tokenized article titles (positive + negatives)\n",
        "        candidate_ids = row['candidate']\n",
        "        candidate_titles = torch.stack([self.news_tensor[aid] for aid in candidate_ids])\n",
        "\n",
        "        # Labels: Positive (1) and negative (0) labels for candidates\n",
        "        labels = torch.tensor(row['label'], dtype=torch.float)\n",
        "\n",
        "        # User history: List of clicked articles converted to tokenized titles\n",
        "        user_his_ids = row['user_his']\n",
        "        user_his_titles = torch.stack([self.news_tensor[aid] for aid in user_his_ids])\n",
        "\n",
        "        # Pad user history if it's shorter than max length\n",
        "        if len(user_his_titles) < self.max_history_len:\n",
        "            padding = torch.zeros((self.max_history_len - len(user_his_titles), candidate_titles.shape[1]), dtype=torch.long)\n",
        "            user_his_titles = torch.cat((user_his_titles, padding), dim=0)\n",
        "\n",
        "        return candidate_titles, user_his_titles, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMkKHoLbH23r",
        "outputId": "fe71544f-bb0c-4fd3-ae88-ed1e12d1fa30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidate Titles Shape: torch.Size([16, 5, 20])\n",
            "User History Shape: torch.Size([16, 50, 20])\n",
            "Labels Shape: torch.Size([16, 5])\n"
          ]
        }
      ],
      "source": [
        "# Load train and validation data\n",
        "train_data = pd.read_parquet('/train_data.parquet')\n",
        "val_data = pd.read_parquet('/val_data.parquet')\n",
        "\n",
        "# Initialize the Dataset for train and validation\n",
        "train_dataset = NewsDataset(train_data, news_tensor, max_history_len=50)\n",
        "val_dataset = NewsDataset(val_data, news_tensor, max_history_len=50)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Fetch a batch to test\n",
        "for candidate_titles, user_his_titles, labels in train_loader:\n",
        "    print(\"Candidate Titles Shape:\", candidate_titles.shape)  # (batch_size, num_candidates, MAX_TITLE_LEN)\n",
        "    print(\"User History Shape:\", user_his_titles.shape)      # (batch_size, max_history_len, MAX_TITLE_LEN)\n",
        "    print(\"Labels Shape:\", labels.shape)                    # (batch_size, num_candidates)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_IRBdxl-VjB9"
      },
      "outputs": [],
      "source": [
        "class NewsEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=300, num_heads=20, attention_hidden_dim=200, pretrained_embeddings=None, dropout_prob=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            embedding_dim (int): Dimension of word embeddings\n",
        "            max_title_len (int): Maximum length of article\n",
        "            pretrained_embeddings (torch.Tensor): Pre-trained embedding matrix\n",
        "        \"\"\"\n",
        "        super(NewsEncoder, self).__init__()\n",
        "\n",
        "        # Word Embedding Layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        # Single-Head Self-Attention\n",
        "        self.multihead_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
        "\n",
        "        # Additive Attention Network\n",
        "        self.additive_attention_query = nn.Parameter(torch.randn(attention_hidden_dim))  # Query vector\n",
        "        self.additive_attention_fc1 = nn.Linear(embedding_dim, attention_hidden_dim)\n",
        "        self.additive_attention_fc2 = nn.Linear(attention_hidden_dim, 1)\n",
        "\n",
        "        # Linear Layer to output fixed-size vector\n",
        "        self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, title_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            title_tokens (Tensor): Shape (batch_size, max_title_len)\n",
        "                                   - Tokenized and padded title tensors\n",
        "        Returns:\n",
        "            Tensor: Fixed-size vector representing the article (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        # Word Embedding\n",
        "        embedded = self.embedding(title_tokens)  # Shape: (batch_size, max_title_len, embedding_dim)\n",
        "        embedded = self.dropout(embedded)        # Apply dropout to embeddings\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        attn_output, _ = self.multihead_attention(embedded, embedded, embedded)  # Shape: (batch_size, max_title_len, embedding_dim)\n",
        "        attn_output = self.dropout(attn_output)  # Apply dropout to attention outputs\n",
        "\n",
        "        # Additive Attention\n",
        "        additive_weights = torch.tanh(self.additive_attention_fc1(attn_output))  # Shape: (batch_size, max_title_len, attention_hidden_dim)\n",
        "        additive_scores = self.additive_attention_fc2(additive_weights).squeeze(-1)  # Shape: (batch_size, max_title_len)\n",
        "\n",
        "        # Compute attention weights (softmax over words)\n",
        "        attention_weights = torch.softmax(additive_scores, dim=1)  # Shape: (batch_size, max_title_len)\n",
        "\n",
        "        # Weighted sum of the attention outputs\n",
        "        weighted_sum = torch.sum(attn_output * attention_weights.unsqueeze(-1), dim=1)  # Shape: (batch_size, embedding_dim)\n",
        "\n",
        "        # Linear Transformation\n",
        "        output_vector = self.fc(weighted_sum)  # Shape: (batch_size, embedding_dim)\n",
        "\n",
        "        return output_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Onof3xGV2Jj",
        "outputId": "243915db-c387-46f3-aff4-fc99a1c2e1f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of candidate_vectors: torch.Size([16, 5, 300])\n",
            "Shape of user_his_vectors: torch.Size([16, 50, 300])\n"
          ]
        }
      ],
      "source": [
        "# Define parameters\n",
        "VOCAB_SIZE = len(vocab)  # Vocabulary size\n",
        "EMBEDDING_DIM = 300      # Dimension of word embeddings\n",
        "MAX_TITLE_LEN = 20       # Length of padded titles\n",
        "DROPOUT_PROB = 0.2\n",
        "\n",
        "# Initialize the News Encoder\n",
        "news_encoder = NewsEncoder(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, pretrained_embeddings=embedding_matrix, dropout_prob=DROPOUT_PROB)\n",
        "\n",
        "# Fetch a batch of candidate titles\n",
        "for candidate_titles, user_his_titles, labels in train_loader:\n",
        "    # Input shape: (batch_size, num_candidates, MAX_TITLE_LEN)\n",
        "    batch_size, num_candidates, title_len = candidate_titles.shape\n",
        "    _, max_history_len, _ = user_his_titles.shape\n",
        "\n",
        "    # Reshape to merge batch_size and num_candidates\n",
        "    candidate_titles_reshaped = candidate_titles.view(-1, title_len)  # Shape: (batch_size * num_candidates, MAX_TITLE_LEN)\n",
        "\n",
        "    # Pass through the News Encoder\n",
        "    candidate_vectors = news_encoder(candidate_titles_reshaped)  # Shape: (batch_size * num_candidates, EMBEDDING_DIM)\n",
        "\n",
        "    # Reshape back to original batch_size and num_candidates\n",
        "    candidate_vectors = candidate_vectors.view(batch_size, num_candidates, EMBEDDING_DIM)\n",
        "\n",
        "    # Reshape user history titles to merge batch_size and max_history_len\n",
        "    user_his_titles_reshaped = user_his_titles.view(-1, title_len)  # Shape: (batch_size * max_history_len, MAX_TITLE_LEN)\n",
        "\n",
        "    # Pass through News Encoder\n",
        "    user_his_vectors = news_encoder(user_his_titles_reshaped)  # Shape: (batch_size * max_history_len, EMBEDDING_DIM)\n",
        "\n",
        "    # Reshape back to original batch_size and max_history_len\n",
        "    user_his_vectors = user_his_vectors.view(batch_size, max_history_len, EMBEDDING_DIM)  # Shape: (batch_size, max_history_len, EMBEDDING_DIM)\n",
        "\n",
        "    print(\"Shape of candidate_vectors:\", candidate_vectors.shape)  # Expected: (batch_size, num_candidates, EMBEDDING_DIM)\n",
        "    print(\"Shape of user_his_vectors:\", user_his_vectors.shape)    # Expected: (batch_size, max_history_len, EMBEDDING_DIM)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cp28oe4emula"
      },
      "outputs": [],
      "source": [
        "class UserEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=300, num_heads=20, attention_hidden_dim=200, dropout_prob=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_dim (int): Dimension of the article embeddings\n",
        "            num_heads (int): Number of attention heads in multi-head attention\n",
        "            attention_hidden_dim (int): Dimension of the query vector in additive attention\n",
        "        \"\"\"\n",
        "        super(UserEncoder, self).__init__()\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        self.multihead_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                         num_heads=num_heads,\n",
        "                                                         batch_first=True)\n",
        "\n",
        "        # Additive Attention\n",
        "        self.additive_attention_query = nn.Parameter(torch.randn(attention_hidden_dim))  # Learnable query vector\n",
        "        self.additive_attention_fc1 = nn.Linear(embedding_dim, attention_hidden_dim)\n",
        "        self.additive_attention_fc2 = nn.Linear(attention_hidden_dim, 1)\n",
        "\n",
        "        # Dropout Layer\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, user_his_vectors):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            user_his_vectors (Tensor): Shape (batch_size, max_history_len, embedding_dim)\n",
        "                                        - Representations of clicked articles\n",
        "        Returns:\n",
        "            Tensor: User representation vector (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        # Multi-Head Self-Attention\n",
        "        attn_output, _ = self.multihead_attention(user_his_vectors, user_his_vectors, user_his_vectors)\n",
        "        # Shape: (batch_size, max_history_len, embedding_dim)\n",
        "\n",
        "        # Additive Attention\n",
        "        additive_weights = torch.tanh(self.additive_attention_fc1(attn_output))  # Shape: (batch_size, max_history_len, attention_hidden_dim)\n",
        "        additive_weights = self.dropout(additive_weights)  # Apply dropout to attention weights\n",
        "        additive_scores = self.additive_attention_fc2(additive_weights).squeeze(-1)  # Shape: (batch_size, max_history_len)\n",
        "\n",
        "        # Compute attention weights (softmax over user history)\n",
        "        attention_weights = torch.softmax(additive_scores, dim=1)  # Shape: (batch_size, max_history_len)\n",
        "\n",
        "        # Weighted sum of the attention outputs\n",
        "        user_vector = torch.sum(attn_output * attention_weights.unsqueeze(-1), dim=1)  # Shape: (batch_size, embedding_dim)\n",
        "\n",
        "        user_vector = self.dropout(user_vector)  # Apply dropout to the final user vector\n",
        "\n",
        "        return user_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGPAfQQdm2B1",
        "outputId": "1d9ef80a-13f0-4a6f-d574-33a684f8f21d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of user_his_vectors: torch.Size([16, 50, 300])\n",
            "Shape of user_vectors: torch.Size([16, 300])\n"
          ]
        }
      ],
      "source": [
        "# Define parameters\n",
        "EMBEDDING_DIM = 300      # Dimension of article embeddings\n",
        "NUM_HEADS = 20           # Number of attention heads\n",
        "ATTENTION_HIDDEN_DIM = 200  # Query vector dimension for additive attention\n",
        "DROPOUT_PROB = 0.0\n",
        "\n",
        "# Initialize User Encoder\n",
        "user_encoder = UserEncoder(embedding_dim=EMBEDDING_DIM, num_heads=NUM_HEADS, attention_hidden_dim=ATTENTION_HIDDEN_DIM, dropout_prob=DROPOUT_PROB)\n",
        "\n",
        "# Fetch a batch of user history vectors\n",
        "for candidate_titles, user_his_titles, labels in train_loader:\n",
        "    # Reshape user history titles to merge batch_size and max_history_len\n",
        "    user_his_titles_reshaped = user_his_titles.view(-1, MAX_TITLE_LEN)  # Shape: (batch_size * max_history_len, MAX_TITLE_LEN)\n",
        "\n",
        "    # Pass through News Encoder to get user history vectors\n",
        "    user_his_vectors = news_encoder(user_his_titles_reshaped)  # Shape: (batch_size * max_history_len, EMBEDDING_DIM)\n",
        "\n",
        "    # Reshape back to (batch_size, max_history_len, EMBEDDING_DIM)\n",
        "    batch_size, max_history_len, _ = user_his_titles.shape\n",
        "    user_his_vectors = user_his_vectors.view(batch_size, max_history_len, EMBEDDING_DIM)\n",
        "\n",
        "    # Pass user history vectors through User Encoder\n",
        "    user_vectors = user_encoder(user_his_vectors)  # Shape: (batch_size, EMBEDDING_DIM)\n",
        "\n",
        "    print(\"Shape of user_his_vectors:\", user_his_vectors.shape)  # (batch_size, max_history_len, EMBEDDING_DIM)\n",
        "    print(\"Shape of user_vectors:\", user_vectors.shape)         # (batch_size, EMBEDDING_DIM)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-lKkpluuYQhg"
      },
      "outputs": [],
      "source": [
        "class NRMSModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=300, num_heads=20, attention_hidden_dim=200, max_history_len=50):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            embedding_dim (int): Dimension of word embeddings\n",
        "            num_heads (int): Number of attention heads in multi-head attention\n",
        "            attention_hidden_dim (int): Dimension of the query vector in additive attention\n",
        "            max_history_len (int): Maximum length of user history\n",
        "            max_title_len (int): Maximum length of article titles\n",
        "        \"\"\"\n",
        "        super(NRMSModel, self).__init__()\n",
        "\n",
        "        # News Encoder for both candidate and user history articles\n",
        "        self.news_encoder = NewsEncoder(vocab_size=vocab_size,\n",
        "                                         embedding_dim=embedding_dim,\n",
        "                                         num_heads=num_heads,\n",
        "                                         attention_hidden_dim=attention_hidden_dim)\n",
        "\n",
        "        # User Encoder to encode user history into a single user representation vector\n",
        "        self.user_encoder = UserEncoder(embedding_dim=embedding_dim,\n",
        "                                         num_heads=num_heads,\n",
        "                                         attention_hidden_dim=attention_hidden_dim)\n",
        "\n",
        "    def forward(self, candidate_titles, user_his_titles):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            candidate_titles (Tensor): Shape (batch_size, num_candidates, max_title_len)\n",
        "                                       - Tokenized and padded titles of candidate articles\n",
        "            user_his_titles (Tensor): Shape (batch_size, max_history_len, max_title_len)\n",
        "                                       - Tokenized and padded titles of user clicked articles\n",
        "        Returns:\n",
        "            Tensor: Click scores for each candidate article (batch_size, num_candidates)\n",
        "        \"\"\"\n",
        "        batch_size, num_candidates, max_title_len = candidate_titles.shape\n",
        "        _, max_history_len, _ = user_his_titles.shape\n",
        "\n",
        "        # -------------------\n",
        "        # Process Candidate Articles\n",
        "        # -------------------\n",
        "        # Reshape candidates to merge batch_size and num_candidates\n",
        "        candidate_titles_reshaped = candidate_titles.view(-1, max_title_len)  # Shape: (batch_size * num_candidates, max_title_len)\n",
        "\n",
        "        # Encode candidate articles\n",
        "        candidate_vectors = self.news_encoder(candidate_titles_reshaped)  # Shape: (batch_size * num_candidates, embedding_dim)\n",
        "\n",
        "        # Reshape back to original batch_size and num_candidates\n",
        "        candidate_vectors = candidate_vectors.view(batch_size, num_candidates, -1)  # Shape: (batch_size, num_candidates, embedding_dim)\n",
        "\n",
        "        # -------------------\n",
        "        # Process User History\n",
        "        # -------------------\n",
        "        # Reshape user history titles to merge batch_size and max_history_len\n",
        "        user_his_titles_reshaped = user_his_titles.view(-1, max_title_len)  # Shape: (batch_size * max_history_len, max_title_len)\n",
        "\n",
        "        # Encode user history articles\n",
        "        user_his_vectors = self.news_encoder(user_his_titles_reshaped)  # Shape: (batch_size * max_history_len, embedding_dim)\n",
        "\n",
        "        # Reshape back to original batch_size and max_history_len\n",
        "        user_his_vectors = user_his_vectors.view(batch_size, max_history_len, -1)  # Shape: (batch_size, max_history_len, embedding_dim)\n",
        "\n",
        "        # Encode user history into a single user representation vector\n",
        "        user_vectors = self.user_encoder(user_his_vectors)  # Shape: (batch_size, embedding_dim)\n",
        "\n",
        "        # -------------------\n",
        "        # Compute Click Scores\n",
        "        # -------------------\n",
        "        # Dot product between user vector and candidate vectors\n",
        "        click_scores = torch.bmm(candidate_vectors, user_vectors.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_candidates)\n",
        "\n",
        "        return click_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN9u_oUyYj8n",
        "outputId": "f9eb5394-df58-4723-dc7d-62ebb5289520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of click_scores: torch.Size([16, 5])\n",
            "Example click scores: tensor([ 0.0091,  0.0017, -0.0151, -0.0202, -0.0106],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the NRMS Model\n",
        "nrms_model = NRMSModel(vocab_size=VOCAB_SIZE,\n",
        "                       embedding_dim=EMBEDDING_DIM,\n",
        "                       num_heads=NUM_HEADS,\n",
        "                       attention_hidden_dim=ATTENTION_HIDDEN_DIM,\n",
        "                       max_history_len=50)\n",
        "\n",
        "# Fetch a batch from the DataLoader\n",
        "for candidate_titles, user_his_titles, labels in train_loader:\n",
        "    # Pass through NRMS model\n",
        "    click_scores = nrms_model(candidate_titles, user_his_titles)  # Shape: (batch_size, num_candidates)\n",
        "\n",
        "    print(\"Shape of click_scores:\", click_scores.shape)  # Expected: (batch_size, num_candidates)\n",
        "    print(\"Example click scores:\", click_scores[0])     # Print scores for the first user in the batch\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ih1vq5zcBSm",
        "outputId": "dcc83e90-5ec2-4571-951b-db0600e75d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "User 1:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: fyres efter skandalekamp\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0023\n",
            "Candidate 2:\n",
            "  Title: bagmænd bag angreb i belgorod varsler mere af samme skuffe\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0079\n",
            "Candidate 3:\n",
            "  Title: dansk storklub slået i vildt drama : det er ikke forbi\n",
            "  Label: 1.0\n",
            "  Click Score: 0.0087\n",
            "Candidate 4:\n",
            "  Title: jørgen de mylius om tina turner : hun var 'unbelievable '\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0353\n",
            "Candidate 5:\n",
            "  Title: forgældet og falleret : martin skrider fra det hele\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0087\n",
            "\n",
            "User 2:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: danmark i rød : c25-indekset styrtdykker\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0181\n",
            "Candidate 2:\n",
            "  Title: rekordmange tyskere planlægger sommerferie i danske feriehuse\n",
            "  Label: 1.0\n",
            "  Click Score: 0.0011\n",
            "Candidate 3:\n",
            "  Title: skød ekskonen for øjnene af børnene : skyldig i overlagt drab\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0134\n",
            "Candidate 4:\n",
            "  Title: bange : de synger putins sang\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0160\n",
            "Candidate 5:\n",
            "  Title: officielt : zeca præsenteret i storklub\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0028\n",
            "\n",
            "User 3:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: forelsket nora : - ikke et rebound !\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0097\n",
            "Candidate 2:\n",
            "  Title: den værst tænkelige start : dansk selvmål i barca-nedtur\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0136\n",
            "Candidate 3:\n",
            "  Title: skuffet over mændene : en dansk buffet\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0138\n",
            "Candidate 4:\n",
            "  Title: politiets foto-fup : de må slet ikke\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0004\n",
            "Candidate 5:\n",
            "  Title: umulig planlægning : falsk superliga-pokal i spil\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0199\n",
            "\n",
            "User 4:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: enden er nær for ikonisk film-saga : er du klar til årets største action-brag ?\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0202\n",
            "Candidate 2:\n",
            "  Title: afsløring : brev var tikkende bombe under kofod\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0284\n",
            "Candidate 3:\n",
            "  Title: alarmerende forskning : storby synker\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0192\n",
            "Candidate 4:\n",
            "  Title: tak , men nej tak : afviser tv 2 igen-igen\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0202\n",
            "Candidate 5:\n",
            "  Title: redder liv til daglig : - det var noget helt andet\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0178\n",
            "\n",
            "User 5:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: bro gak-gak i frederikssund : kan ikke tåle solen\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0025\n",
            "Candidate 2:\n",
            "  Title: cowell overrasker : her blæser han på reglerne for første gang\n",
            "  Label: 1.0\n",
            "  Click Score: 0.0054\n",
            "Candidate 3:\n",
            "  Title: ny sundhedspakke : statsministeriet indkalder til pressemøde\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0328\n",
            "Candidate 4:\n",
            "  Title: gældskrisen truer : sådan kan én mønt redde usa\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0169\n",
            "Candidate 5:\n",
            "  Title: mark cavendish stopper karrieren\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0171\n",
            "\n",
            "User 6:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: ung kvinde : det kan gå galt\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0262\n",
            "Candidate 2:\n",
            "  Title: tyskland slukker drømmen : danmark færdig ved vm\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0253\n",
            "Candidate 3:\n",
            "  Title: drabstiltalt til agenten : 'du har jo også stukket en kvinde ned '\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0485\n",
            "Candidate 4:\n",
            "  Title: usa : wagner-gruppen vil smugle våben\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0183\n",
            "Candidate 5:\n",
            "  Title: mistede klassekammerater i skoleskyderi : barnebarn overlevede ved et tilfælde\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0229\n",
            "\n",
            "User 7:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: 32-årig drabssigtet : over 200 henvendelser om navneforbud\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0051\n",
            "Candidate 2:\n",
            "  Title: kendisdatter anholdt på ibiza\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0014\n",
            "Candidate 3:\n",
            "  Title: danskere sælger kryptoselskabet coinify til israelsk milliardær\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0034\n",
            "Candidate 4:\n",
            "  Title: brokaos : helt spærret\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0035\n",
            "Candidate 5:\n",
            "  Title: klamt fund i karameller på sidste skoledag\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0102\n",
            "\n",
            "User 8:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: fordømmer skandale-scener : - jeg skammer mig\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0109\n",
            "Candidate 2:\n",
            "  Title: strejke , sexafpresning og sort magi : historien om verdens bedste landshold\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0161\n",
            "Candidate 3:\n",
            "  Title: blodbold : qatars brutale metoder\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0175\n",
            "Candidate 4:\n",
            "  Title: ekspert : derfor sætter putin alt på et bræt\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0146\n",
            "Candidate 5:\n",
            "  Title: 14 døde i italien\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0160\n",
            "\n",
            "User 9:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: mette f. til nato ? en sten er ryddet ad vejen\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0095\n",
            "Candidate 2:\n",
            "  Title: efter 39 år i fængsel : 'jeg vil hjem '\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0089\n",
            "Candidate 3:\n",
            "  Title: anklager : livstid til hedvigs morder\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0157\n",
            "Candidate 4:\n",
            "  Title: forsvarer om drabstiltalt : - higede efter anerkendelse og 'street credit '\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0170\n",
            "Candidate 5:\n",
            "  Title: uheld i københavn : cyklist ramt\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0059\n",
            "\n",
            "User 10:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: konservativ borgmester har brugt skattekroner på koncerter\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0138\n",
            "Candidate 2:\n",
            "  Title: norwegian sagsøges ... af norwegian\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0027\n",
            "Candidate 3:\n",
            "  Title: tv-kendis jubler : jeg er kræftfri\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0155\n",
            "Candidate 4:\n",
            "  Title: afvist på tv : det er fandeme i orden\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0096\n",
            "Candidate 5:\n",
            "  Title: raser mod fc barcelona : takker russerne\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0003\n",
            "\n",
            "User 11:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: politi : efterforsker påkørsel som drabsforsøg\n",
            "  Label: 1.0\n",
            "  Click Score: 0.0057\n",
            "Candidate 2:\n",
            "  Title: real madrid går i struben på fans\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0037\n",
            "Candidate 3:\n",
            "  Title: vold i forlystelsespark : tog kvælertag på 15-årig\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0075\n",
            "Candidate 4:\n",
            "  Title: mark cavendish stopper karrieren\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0022\n",
            "Candidate 5:\n",
            "  Title: gasledning overgravet : 'hold dig indenfor '\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0057\n",
            "\n",
            "User 12:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: avis : dansk stjerne skyldte 750.000 til holdkammerater\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0238\n",
            "Candidate 2:\n",
            "  Title: pokalregel irriterer jacob neestrup : helt håbløst !\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0031\n",
            "Candidate 3:\n",
            "  Title: europæer mister livet på verdens højeste bjerg\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0051\n",
            "Candidate 4:\n",
            "  Title: nu er det slut : influencer skal forlade danmark\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0146\n",
            "Candidate 5:\n",
            "  Title: mercedes ' dansker : på pole til formel 1\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0003\n",
            "\n",
            "User 13:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: kæmpe giro-drama : vild gyser på toppen\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0108\n",
            "Candidate 2:\n",
            "  Title: mourinho raser : - det er en joke !\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0078\n",
            "Candidate 3:\n",
            "  Title: zeca stopper i fck : tilbudt toårig aftale i storklub\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0095\n",
            "Candidate 4:\n",
            "  Title: taler ud om utroskab\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0072\n",
            "Candidate 5:\n",
            "  Title: slut efter 15 år : ståle solbakken flytter fra danmark\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0064\n",
            "\n",
            "User 14:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: stjerne reagerer på falske rygter\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0030\n",
            "Candidate 2:\n",
            "  Title: grøn mandag : c25-indekset stiger\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0088\n",
            "Candidate 3:\n",
            "  Title: felix smith scorer stor værtstjans\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0168\n",
            "Candidate 4:\n",
            "  Title: danmark i rød : c25-indekset styrtdykker\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0062\n",
            "Candidate 5:\n",
            "  Title: bitter armstrong : han er en forbandet klovn !\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0159\n",
            "\n",
            "User 15:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: kæmpe forskel : så meget tjener vm-heltene\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0039\n",
            "Candidate 2:\n",
            "  Title: brøndbys berygtede hooligans\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0021\n",
            "Candidate 3:\n",
            "  Title: trusler og bedragerianklager : dansk tv-profil i byggekaos\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0280\n",
            "Candidate 4:\n",
            "  Title: strejke , sexafpresning og sort magi : historien om verdens bedste landshold\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0051\n",
            "Candidate 5:\n",
            "  Title: dansk olieselskab tjente styrtende i 2022\n",
            "  Label: 1.0\n",
            "  Click Score: 0.0077\n",
            "\n",
            "User 16:\n",
            "------------------------------\n",
            "Candidate 1:\n",
            "  Title: 32-årig drabssigtet : over 200 henvendelser om navneforbud\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0123\n",
            "Candidate 2:\n",
            "  Title: contador-rytter er død 25 år gammel\n",
            "  Label: 0.0\n",
            "  Click Score: -0.0175\n",
            "Candidate 3:\n",
            "  Title: chokeret kræmmermarkedschef : første katastrofe i 34 år\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0084\n",
            "Candidate 4:\n",
            "  Title: df-medlem indstillet som spidskandidat til eu-parlamentet\n",
            "  Label: 1.0\n",
            "  Click Score: -0.0176\n",
            "Candidate 5:\n",
            "  Title: afslører : fik aktier i konfirmationsgave\n",
            "  Label: 0.0\n",
            "  Click Score: 0.0061\n"
          ]
        }
      ],
      "source": [
        "# Define a function to decode tokenized titles back into text\n",
        "def decode_title(token_indices, vocab):\n",
        "    reverse_vocab = {idx: word for word, idx in vocab.items()}  # Reverse the vocabulary\n",
        "    return \" \".join([reverse_vocab[idx] for idx in token_indices if idx != 0])  # Ignore padding tokens (0)\n",
        "\n",
        "# Fetch a batch from the DataLoader\n",
        "for candidate_titles, user_his_titles, labels in train_loader:\n",
        "    # Pass through NRMS model\n",
        "    click_scores = nrms_model(candidate_titles, user_his_titles)  # Shape: (batch_size, num_candidates)\n",
        "\n",
        "    # Loop through the batch\n",
        "    batch_size, num_candidates, max_title_len = candidate_titles.shape\n",
        "    for i in range(batch_size):\n",
        "        print(f\"\\nUser {i + 1}:\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Decode and print each candidate title, its label, and its score\n",
        "        for j in range(num_candidates):\n",
        "            title_tokens = candidate_titles[i, j].cpu().numpy()  # Get token indices for the title\n",
        "            title_text = decode_title(title_tokens, vocab)  # Decode the title back into text\n",
        "            label = labels[i, j].item()  # Get the label\n",
        "            score = click_scores[i, j].item()  # Get the click score\n",
        "\n",
        "            print(f\"Candidate {j + 1}:\")\n",
        "            print(f\"  Title: {title_text}\")\n",
        "            print(f\"  Label: {label}\")\n",
        "            print(f\"  Click Score: {score:.4f}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTZNQTuy3muU",
        "outputId": "5ebf2c8f-e104-4e4e-eacd-814e082afb7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Batch [1/1556], Loss: 1.6084\n",
            "Epoch [1/3], Batch [101/1556], Loss: 1.6097\n",
            "Epoch [1/3], Batch [201/1556], Loss: 1.4620\n",
            "Epoch [1/3], Batch [301/1556], Loss: 1.4796\n",
            "Epoch [1/3], Batch [401/1556], Loss: 1.5426\n",
            "Epoch [1/3], Batch [501/1556], Loss: 1.6327\n",
            "Epoch [1/3], Batch [601/1556], Loss: 1.5980\n",
            "Epoch [1/3], Batch [701/1556], Loss: 1.5228\n",
            "Epoch [1/3], Batch [801/1556], Loss: 1.5809\n",
            "Epoch [1/3], Batch [901/1556], Loss: 1.6101\n",
            "Epoch [1/3], Batch [1001/1556], Loss: 1.7373\n",
            "Epoch [1/3], Batch [1101/1556], Loss: 1.5175\n",
            "Epoch [1/3], Batch [1201/1556], Loss: 1.3431\n",
            "Epoch [1/3], Batch [1301/1556], Loss: 1.5037\n",
            "Epoch [1/3], Batch [1401/1556], Loss: 1.4826\n",
            "Epoch [1/3], Batch [1501/1556], Loss: 1.3537\n",
            "Epoch [1/3] - Average Loss: 1.5458\n",
            "Epoch [2/3], Batch [1/1556], Loss: 1.5767\n",
            "Epoch [2/3], Batch [101/1556], Loss: 1.5720\n",
            "Epoch [2/3], Batch [201/1556], Loss: 1.4555\n",
            "Epoch [2/3], Batch [301/1556], Loss: 1.6261\n",
            "Epoch [2/3], Batch [401/1556], Loss: 1.4812\n",
            "Epoch [2/3], Batch [501/1556], Loss: 1.5575\n",
            "Epoch [2/3], Batch [601/1556], Loss: 1.4361\n",
            "Epoch [2/3], Batch [701/1556], Loss: 1.2932\n",
            "Epoch [2/3], Batch [801/1556], Loss: 1.3996\n",
            "Epoch [2/3], Batch [901/1556], Loss: 1.5050\n",
            "Epoch [2/3], Batch [1001/1556], Loss: 1.4468\n",
            "Epoch [2/3], Batch [1101/1556], Loss: 1.7659\n",
            "Epoch [2/3], Batch [1201/1556], Loss: 1.6919\n",
            "Epoch [2/3], Batch [1301/1556], Loss: 1.3708\n",
            "Epoch [2/3], Batch [1401/1556], Loss: 1.3305\n",
            "Epoch [2/3], Batch [1501/1556], Loss: 1.2972\n",
            "Epoch [2/3] - Average Loss: 1.4854\n",
            "Epoch [3/3], Batch [1/1556], Loss: 1.2619\n",
            "Epoch [3/3], Batch [101/1556], Loss: 1.3499\n",
            "Epoch [3/3], Batch [201/1556], Loss: 1.4139\n",
            "Epoch [3/3], Batch [301/1556], Loss: 1.3484\n",
            "Epoch [3/3], Batch [401/1556], Loss: 1.5253\n",
            "Epoch [3/3], Batch [501/1556], Loss: 1.5965\n",
            "Epoch [3/3], Batch [601/1556], Loss: 1.3853\n",
            "Epoch [3/3], Batch [701/1556], Loss: 1.2655\n",
            "Epoch [3/3], Batch [801/1556], Loss: 1.3151\n",
            "Epoch [3/3], Batch [901/1556], Loss: 1.4128\n",
            "Epoch [3/3], Batch [1001/1556], Loss: 1.5060\n",
            "Epoch [3/3], Batch [1101/1556], Loss: 1.4436\n",
            "Epoch [3/3], Batch [1201/1556], Loss: 1.3285\n",
            "Epoch [3/3], Batch [1301/1556], Loss: 1.5440\n",
            "Epoch [3/3], Batch [1401/1556], Loss: 1.3945\n",
            "Epoch [3/3], Batch [1501/1556], Loss: 1.3588\n",
            "Epoch [3/3] - Average Loss: 1.4625\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Define parameters\n",
        "NUM_EPOCHS = 3          # Number of epochs\n",
        "LEARNING_RATE = 1e-4    # Learning rate\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize the model, loss, and optimizer\n",
        "nrms_model = NRMSModel(vocab_size=VOCAB_SIZE,\n",
        "                       embedding_dim=EMBEDDING_DIM,\n",
        "                       num_heads=NUM_HEADS,\n",
        "                       attention_hidden_dim=ATTENTION_HIDDEN_DIM,\n",
        "                       max_history_len=50).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "optimizer = optim.Adam(nrms_model.parameters(), lr=LEARNING_RATE)  # Optimizer\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    nrms_model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (candidate_titles, user_his_titles, labels) in enumerate(train_loader):\n",
        "        # Move data to the appropriate device (GPU/CPU)\n",
        "        candidate_titles = candidate_titles.to(DEVICE)  # Shape: (batch_size, num_candidates, max_title_len)\n",
        "        user_his_titles = user_his_titles.to(DEVICE)    # Shape: (batch_size, max_history_len, max_title_len)\n",
        "        labels = torch.argmax(labels, dim=1).to(DEVICE)  # Convert labels to indices, Shape: (batch_size)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        click_scores = nrms_model(candidate_titles, user_his_titles)  # Shape: (batch_size, num_candidates)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(click_scores, labels)  # CrossEntropyLoss expects (scores, target_indices)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress for every 100 batches\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Print epoch loss\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bFMtGhJJnz74"
      },
      "outputs": [],
      "source": [
        "def compute_mrr(labels, scores):\n",
        "    \"\"\"Compute Mean Reciprocal Rank (MRR).\"\"\"\n",
        "    ranks = []\n",
        "    for label, score in zip(labels, scores):\n",
        "        sorted_indices = np.argsort(-score)  # Sort scores in descending order\n",
        "        rank = np.where(sorted_indices == label)[0][0] + 1  # Rank of the true label\n",
        "        ranks.append(1 / rank)\n",
        "    return np.mean(ranks)\n",
        "\n",
        "def compute_ndcg_at_k(labels, scores, k=5):\n",
        "    \"\"\"Compute Normalized Discounted Cumulative Gain at K (nDCG@k).\"\"\"\n",
        "    ndcgs = []\n",
        "    for label, score in zip(labels, scores):\n",
        "        sorted_indices = np.argsort(-score)[:k]  # Top-k indices by score\n",
        "        dcg = 0\n",
        "        for i, idx in enumerate(sorted_indices):\n",
        "            if idx == label:\n",
        "                dcg += 1 / np.log2(i + 2)  # i + 2 to avoid log(0)\n",
        "                break\n",
        "        idcg = 1 / np.log2(1 + 1)  # Ideal DCG when true label is ranked 1st\n",
        "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
        "    return np.mean(ndcgs)\n",
        "\n",
        "def validate_model(model, val_loader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation during evaluation\n",
        "        for batch_idx, (candidate_titles, user_his_titles, labels) in enumerate(val_loader):\n",
        "            # Move data to the appropriate device\n",
        "            candidate_titles = candidate_titles.to(device)  # Shape: (batch_size, num_candidates, max_title_len)\n",
        "            user_his_titles = user_his_titles.to(device)    # Shape: (batch_size, max_history_len, max_title_len)\n",
        "            labels = torch.argmax(labels, dim=1).to(device)  # Convert labels to indices, Shape: (batch_size)\n",
        "\n",
        "            # Forward pass\n",
        "            click_scores = model(candidate_titles, user_his_titles)  # Shape: (batch_size, num_candidates)\n",
        "\n",
        "            # Predicted indices (max score per candidate set)\n",
        "            preds = torch.argmax(click_scores, dim=1).cpu().numpy()  # Shape: (batch_size)\n",
        "\n",
        "            # Append true labels and predictions for metrics\n",
        "            all_labels.extend(labels.cpu().numpy())  # True labels\n",
        "            all_preds.extend(preds)  # Predicted labels\n",
        "            all_scores.extend(torch.softmax(click_scores, dim=1).cpu().numpy())  # Softmax probabilities\n",
        "\n",
        "    print(f\"Shape of all_labels: {len(all_labels)}\")  # Should match the total number of validation samples\n",
        "    print(f\"Shape of all_scores: {len(all_scores), len(all_scores[0])}\")  # Should be (n_samples, n_classes)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    auc = roc_auc_score(all_labels, all_scores, multi_class=\"ovr\")  # Use full softmax scores\n",
        "    mrr = compute_mrr(all_labels, all_scores)\n",
        "    ndcg_at_5 = compute_ndcg_at_k(all_labels, all_scores, k=5)\n",
        "\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Validation AUC: {auc:.4f}\")\n",
        "    print(f\"Validation MRR: {mrr:.4f}\")\n",
        "    print(f\"Validation nDCG@5: {ndcg_at_5:.4f}\")\n",
        "    return accuracy, auc, mrr, ndcg_at_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Z5TyEGXNe_",
        "outputId": "9c15dc0a-f6f0-44e2-c437-38243f5a59bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of all_labels: 25505\n",
            "Shape of all_scores: (25505, 5)\n",
            "Validation Accuracy: 0.2401\n",
            "Validation AUC: 0.5711\n",
            "Validation MRR: 0.4957\n",
            "Validation nDCG@5: 0.6199\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.2401489903940404, 0.5710892144799267, 0.4956727439064236, 0.619910212222263)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test the trained model\n",
        "validate_model(nrms_model, val_loader, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aG3H5hszcYCp"
      },
      "outputs": [],
      "source": [
        "torch.save(nrms_model.state_dict(), \"/content/nrms_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvFVzJDcckAP",
        "outputId": "e215bd30-cc47-450b-8628-848594454763"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-1ff3ba4e4333>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  nrms_model.load_state_dict(torch.load(\"/content/nrms_model.pth\"))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "NRMSModel(\n",
              "  (news_encoder): NewsEncoder(\n",
              "    (embedding): Embedding(16003, 300, padding_idx=0)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (multihead_attention): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "    )\n",
              "    (additive_attention_fc1): Linear(in_features=300, out_features=200, bias=True)\n",
              "    (additive_attention_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
              "    (fc): Linear(in_features=300, out_features=300, bias=True)\n",
              "  )\n",
              "  (user_encoder): UserEncoder(\n",
              "    (multihead_attention): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "    )\n",
              "    (additive_attention_fc1): Linear(in_features=300, out_features=200, bias=True)\n",
              "    (additive_attention_fc2): Linear(in_features=200, out_features=1, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nrms_model = NRMSModel(vocab_size=VOCAB_SIZE,\n",
        "                       embedding_dim=EMBEDDING_DIM,\n",
        "                       num_heads=NUM_HEADS,\n",
        "                       attention_hidden_dim=ATTENTION_HIDDEN_DIM,\n",
        "                       max_history_len=50).to(DEVICE)\n",
        "nrms_model.load_state_dict(torch.load(\"/content/nrms_model.pth\"))\n",
        "nrms_model.eval()  # Set model to evaluation mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTh9CXb7XPN-",
        "outputId": "e306038c-43ec-425e-b487-6d56bf6a8b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "User 1:\n",
            "True Label: 3\n",
            "Predicted Click Probabilities: [0.16499498 0.48258266 0.0181701  0.3061414  0.02811082]\n",
            "\n",
            "User 2:\n",
            "True Label: 2\n",
            "Predicted Click Probabilities: [0.17946708 0.34895906 0.2935238  0.15891865 0.01913144]\n",
            "\n",
            "User 3:\n",
            "True Label: 2\n",
            "Predicted Click Probabilities: [0.15165615 0.15012798 0.32395694 0.18934967 0.18490924]\n",
            "\n",
            "User 4:\n",
            "True Label: 2\n",
            "Predicted Click Probabilities: [0.1869131  0.27347305 0.14871149 0.17994772 0.21095464]\n",
            "\n",
            "User 5:\n",
            "True Label: 0\n",
            "Predicted Click Probabilities: [0.21140973 0.07901444 0.21624954 0.24403815 0.24928814]\n",
            "\n",
            "User 6:\n",
            "True Label: 3\n",
            "Predicted Click Probabilities: [0.20432742 0.11676952 0.18353046 0.16342986 0.33194277]\n",
            "\n",
            "User 7:\n",
            "True Label: 3\n",
            "Predicted Click Probabilities: [0.099669   0.13146655 0.2448048  0.25524434 0.26881537]\n",
            "\n",
            "User 8:\n",
            "True Label: 1\n",
            "Predicted Click Probabilities: [0.20698081 0.09441364 0.2165706  0.3152093  0.16682565]\n",
            "\n",
            "User 9:\n",
            "True Label: 2\n",
            "Predicted Click Probabilities: [0.11922061 0.04921909 0.17965813 0.32354066 0.3283615 ]\n",
            "\n",
            "User 10:\n",
            "True Label: 4\n",
            "Predicted Click Probabilities: [0.11013613 0.17309947 0.29888722 0.11453647 0.30334073]\n",
            "\n",
            "User 11:\n",
            "True Label: 4\n",
            "Predicted Click Probabilities: [0.16604762 0.21570957 0.08567974 0.29091907 0.24164397]\n",
            "\n",
            "User 12:\n",
            "True Label: 0\n",
            "Predicted Click Probabilities: [0.17777433 0.3114645  0.18991221 0.3179143  0.00293468]\n",
            "\n",
            "User 13:\n",
            "True Label: 4\n",
            "Predicted Click Probabilities: [0.36952472 0.28110805 0.04256795 0.01224765 0.29455167]\n",
            "\n",
            "User 14:\n",
            "True Label: 1\n",
            "Predicted Click Probabilities: [0.1749183  0.25004682 0.16438185 0.18682474 0.22382833]\n",
            "\n",
            "User 15:\n",
            "True Label: 0\n",
            "Predicted Click Probabilities: [0.12940088 0.22227646 0.58267796 0.00923119 0.05641346]\n",
            "\n",
            "User 16:\n",
            "True Label: 1\n",
            "Predicted Click Probabilities: [0.10746944 0.20781127 0.24998742 0.20383966 0.23089214]\n"
          ]
        }
      ],
      "source": [
        "# Inspect click probabilities\n",
        "for batch_idx, (candidate_titles, user_his_titles, labels) in enumerate(val_loader):\n",
        "    candidate_titles = candidate_titles.to(DEVICE)\n",
        "    user_his_titles = user_his_titles.to(DEVICE)\n",
        "    labels = torch.argmax(labels, dim=1).to(DEVICE)\n",
        "\n",
        "    # Forward pass\n",
        "    click_scores = nrms_model(candidate_titles, user_his_titles)\n",
        "    softmax_probs = torch.softmax(click_scores, dim=1)\n",
        "\n",
        "    for i in range(candidate_titles.size(0)):  # Iterate over the batch\n",
        "        print(f\"\\nUser {i + 1}:\")\n",
        "        print(f\"True Label: {labels[i].item()}\")\n",
        "        print(f\"Predicted Click Probabilities: {softmax_probs[i].cpu().detach().numpy()}\")\n",
        "    break  # Remove this to inspect the full dataset\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

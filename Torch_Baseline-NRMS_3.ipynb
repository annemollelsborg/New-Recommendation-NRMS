{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constants\n",
    "MAX_SENT_LENGTH = 30\n",
    "MAX_SENTS = 50\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_HEADS = 20\n",
    "HEAD_SIZE = 20\n",
    "DROPOUT_RATE = 0.2\n",
    "NPRATIO = 4  # Number of negative samples per positive\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "articles = pd.read_csv(\"Data/articles.csv\")\n",
    "behaviors_train = pd.read_csv(\"Data/behaviors_train.csv\")\n",
    "behaviors_val = pd.read_csv(\"Data/behaviors_val.csv\")\n",
    "history_train = pd.read_csv(\"Data/history_train.csv\")\n",
    "history_val = pd.read_csv(\"Data/history_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize article titles and build vocabulary\n",
    "def build_vocab_and_tokenize(titles, max_len=MAX_SENT_LENGTH):\n",
    "    vocab = defaultdict(lambda: len(vocab))  # Default dictionary for token ids\n",
    "    vocab[\"<PAD>\"] = 0  # Padding token\n",
    "    vocab[\"<UNK>\"] = 1  # Unknown token\n",
    "\n",
    "    VOCAB_SIZE = len(vocab)\n",
    "\n",
    "    tokenized_titles = []\n",
    "    for title in titles:\n",
    "        tokens = title.lower().split()[:max_len]  # Simple whitespace tokenizer, truncate to max_len\n",
    "        tokenized = [vocab[token] for token in tokens]\n",
    "        tokenized = [min(idx, VOCAB_SIZE - 1) for idx in tokenized]  # Ensure indices are valid\n",
    "        padded = tokenized + [vocab[\"<PAD>\"]] * (max_len - len(tokenized))  # Padding to max_len\n",
    "        tokenized_titles.append(padded)\n",
    "\n",
    "    return tokenized_titles, vocab, VOCAB_SIZE\n",
    "\n",
    "articles[\"tokenized_title\"], vocab, VOCAB_SIZE = build_vocab_and_tokenize(articles[\"title\"].fillna(\"<UNK>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Clean and process user history\n",
    "def clean_article_ids(article_ids):\n",
    "    try:\n",
    "        if \"...\" in article_ids:\n",
    "            return None  # Mark for removal\n",
    "        return list(map(int, article_ids.strip(\"[]\").split()))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "history_train[\"cleaned_article_ids\"] = history_train[\"article_id_fixed\"].apply(clean_article_ids)\n",
    "history_val[\"cleaned_article_ids\"] = history_val[\"article_id_fixed\"].apply(clean_article_ids)\n",
    "\n",
    "history_train_cleaned = history_train.dropna(subset=[\"cleaned_article_ids\"]).reset_index(drop=True)\n",
    "history_val_cleaned = history_val.dropna(subset=[\"cleaned_article_ids\"]).reset_index(drop=True)\n",
    "\n",
    "def process_cleaned_user_history(cleaned_history_df):\n",
    "    user_histories = defaultdict(list)\n",
    "    for _, row in cleaned_history_df.iterrows():\n",
    "        user_id = row[\"user_id\"]\n",
    "        article_ids = row[\"cleaned_article_ids\"]\n",
    "        user_histories[user_id].extend(article_ids)\n",
    "    return user_histories\n",
    "\n",
    "user_history_train_cleaned = process_cleaned_user_history(history_train_cleaned)\n",
    "user_history_val_cleaned = process_cleaned_user_history(history_val_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create samples and labels\n",
    "def create_samples(behaviors_df, user_history, max_sents=MAX_SENTS):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    for _, row in behaviors_df.iterrows():\n",
    "        user_id = row[\"user_id\"]\n",
    "        clicked_articles = list(map(int, row[\"article_ids_clicked\"].strip(\"[]\").split()))\n",
    "        inview_articles = list(map(int, row[\"article_ids_inview\"].strip(\"[]\").split()))\n",
    "\n",
    "        user_hist = user_history[user_id][:max_sents] + [0] * (max_sents - len(user_history[user_id]))\n",
    "        user_hist = [min(idx, VOCAB_SIZE - 1) for idx in user_hist]  # Ensure valid indices\n",
    "\n",
    "        for article_id in inview_articles:\n",
    "            article_id = min(article_id, VOCAB_SIZE - 1)  # Ensure valid index\n",
    "            label = 1 if article_id in clicked_articles else 0\n",
    "            samples.append((user_hist, article_id))\n",
    "            labels.append(label)\n",
    "\n",
    "    return samples, labels\n",
    "\n",
    "train_samples_cleaned, train_labels_cleaned = create_samples(behaviors_train, user_history_train_cleaned)\n",
    "val_samples_cleaned, val_labels_cleaned = create_samples(behaviors_val, user_history_val_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Define PyTorch Dataset\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_history, candidate = self.samples[idx]\n",
    "        user_history_padded = [0] * MAX_SENTS  # Pad user history\n",
    "        user_history_padded[:len(user_history)] = user_history[:MAX_SENTS]\n",
    "        candidate_padded = [candidate] + [0] * (MAX_SENT_LENGTH - 1)  # Pad candidate to sequence\n",
    "        return (\n",
    "            torch.tensor(user_history_padded, dtype=torch.long),\n",
    "            torch.tensor(candidate_padded, dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        )\n",
    "\n",
    "train_dataset = NewsDataset(train_samples_cleaned, train_labels_cleaned)\n",
    "val_dataset = NewsDataset(val_samples_cleaned, val_labels_cleaned)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define the NRMS Model\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.output_dim = num_heads * head_size\n",
    "        self.qkv_linear = nn.Linear(EMBEDDING_DIM, self.output_dim * 3)\n",
    "        self.fc_out = nn.Linear(self.output_dim, EMBEDDING_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) != 3:\n",
    "            raise ValueError(f\"Expected input to be 3D (batch_size, seq_length, embed_dim), got {x.size()}\")\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_linear(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_size)\n",
    "        qkv = qkv.permute(2, 0, 1, 3)\n",
    "        Q, K, V = torch.chunk(qkv, 3, dim=-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_size)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        weighted = torch.matmul(attention, V)\n",
    "        return self.fc_out(weighted.permute(1, 2, 0, 3).reshape(batch_size, seq_length, self.output_dim))\n",
    "\n",
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        self.self_attention = MultiHeadSelfAttention(NUM_HEADS, HEAD_SIZE)\n",
    "        self.dense = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Ensure input is embedded\n",
    "        if len(x.size()) != 3:\n",
    "            raise ValueError(f\"Embedding layer output should be 3D, got {x.size()}\")\n",
    "        x = self.dropout(x)\n",
    "        x = self.self_attention(x)\n",
    "        attention_weights = F.softmax(self.dense(x).squeeze(-1), dim=-1)\n",
    "        return torch.sum(x * attention_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "class NRMS(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.title_encoder = TitleEncoder(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, candidates, user_history):\n",
    "        user_rep = self.title_encoder(user_history)  # Output: (batch_size, embedding_dim)\n",
    "        candidate_rep = self.title_encoder(candidates)  # Output: (batch_size, embedding_dim)\n",
    "        return torch.matmul(candidate_rep, user_rep.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train the Model\n",
    "model = NRMS(VOCAB_SIZE, EMBEDDING_DIM, NPRATIO + 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            user_histories, candidates, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(candidates, user_histories)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 20809.6994\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8732\n",
      "Precision: 1.0000\n",
      "Recall: 0.8732\n",
      "F1 Score: 0.9323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in data_loader:\n",
    "            user_histories, candidates, labels = batch\n",
    "            \n",
    "            # Get raw logits from the model\n",
    "            outputs = model(candidates, user_histories)\n",
    "            \n",
    "            # Compute probabilities for the positive class\n",
    "            probabilities = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            \n",
    "            # Get the predicted class\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Store labels, predictions, and probabilities\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predictions.numpy())\n",
    "            all_probabilities.extend(probabilities.numpy())\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate on validation data\n",
    "accuracy, precision, recall, f1 = evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

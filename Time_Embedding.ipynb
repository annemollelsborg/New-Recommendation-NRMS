{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(\"Data/articles.csv\")\n",
    "behaviors_train = pd.read_csv(\"Data/behaviors_train.csv\")\n",
    "behaviors_val = pd.read_csv(\"Data/behaviors_val.csv\")\n",
    "history_train = pd.read_csv(\"Data/history_train.csv\")\n",
    "history_val = pd.read_csv(\"Data/history_val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main idea behind treating the attributes cyclic** Values close on the cycle have similar sine and cosine representations.\n",
    "\n",
    "**Example** After 23:59 comes 00:00, so 23 and 0 are closer in time than 23 and 12.\n",
    "\n",
    "**Calculations** Cyclic encoding solves the discontinuity problem by mapping cyclic values to points on a unit circle using sine and cosine functions:\n",
    "\n",
    "$x_\\text{sin} = \\sin\\left( \\frac{2\\pi \\cdot \\text{value}}{\\text{cycle length}} \\right)$\n",
    "\n",
    "$x_\\text{cos} = \\cos\\left( \\frac{2\\pi \\cdot \\text{value}}{\\text{cycle length}} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_timestamp(timestamp):\n",
    "    dt = pd.to_datetime(timestamp)\n",
    "    \n",
    "    # Cyclic encoding\n",
    "    sin_hour = np.sin(2 * np.pi * dt.hour / 24)\n",
    "    cos_hour = np.cos(2 * np.pi * dt.hour / 24)\n",
    "    \n",
    "    # Continuous and one-hot features\n",
    "    month_one_hot = np.eye(12)[dt.month - 1]  # One-hot encode month\n",
    "    weekday_one_hot = np.eye(7)[dt.weekday()] # One-hot encode weekday\n",
    "\n",
    "    # Combine features\n",
    "    return np.concatenate([month_one_hot, weekday_one_hot, [sin_hour, cos_hour]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.        -0.5       -0.8660254]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2023-05-30 14:21:34\"\n",
    "embedding = encode_timestamp(timestamp)\n",
    "print(embedding)\n",
    "print(len(embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspiration: Code for implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event-time joint embedding -> join time and title representation\n",
    "\n",
    "Other source: https://github.com/huggingface/diffusers/blob/v0.11.0/src/diffusers/models/embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestampEmbedding(nn.Module):\n",
    "    def __init__(self, timestamp_dim):\n",
    "        super().__init__()\n",
    "        self.timestamp_dim = timestamp_dim\n",
    "        self.fc = nn.Linear(timestamp_dim, timestamp_dim)  # Optional transformation\n",
    "\n",
    "    def forward(self, timestamp):\n",
    "        # `timestamp` should already be encoded as a feature vector\n",
    "        return self.fc(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedEmbedding(nn.Module):\n",
    "    def __init__(self, article_dim, timestamp_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(article_dim + timestamp_dim, article_dim)  # Project back to article_dim\n",
    "\n",
    "    def forward(self, article_emb, timestamp_emb):\n",
    "        combined_emb = torch.cat([article_emb, timestamp_emb], dim=-1)\n",
    "        return self.fc(combined_emb)  # Optionally project back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, batch_size, embed_dim]\n",
    "        return self.attention(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NRMSWithTimestamps(nn.Module):\n",
    "    def __init__(self, article_dim, timestamp_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.timestamp_embedding = TimestampEmbedding(timestamp_dim)\n",
    "        self.combined_embedding = CombinedEmbedding(article_dim, timestamp_dim)\n",
    "        self.self_attention = SelfAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, article_emb, timestamp):\n",
    "        # Generate timestamp embeddings\n",
    "        timestamp_emb = self.timestamp_embedding(timestamp)\n",
    "\n",
    "        # Combine article and timestamp embeddings\n",
    "        combined_emb = self.combined_embedding(article_emb, timestamp_emb)\n",
    "\n",
    "        # Apply self-attention\n",
    "        attention_out, _ = self.self_attention(combined_emb)\n",
    "\n",
    "        return attention_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

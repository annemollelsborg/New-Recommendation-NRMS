{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles = pd.read_csv('Data/articles.csv')\n",
    "history_train = pd.read_csv('Data/history_train.csv')\n",
    "behaviors_train = pd.read_csv('Data/behaviors_train.csv')\n",
    "history_test = pd.read_csv('Data/history_val.csv')\n",
    "behaviors_test = pd.read_csv('Data/behaviors_val.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles processed:  11778\n"
     ]
    }
   ],
   "source": [
    "# Dictionary mapping article_id to tokenized titles\n",
    "news = {}\n",
    "for _, row in articles.iterrows():\n",
    "    news[row['article_id']] = word_tokenize(str(row['title']).lower())\n",
    "\n",
    "# Dictionary mapping article_id to unique indices\n",
    "newsindex = {'NULL': 0} # 0 reserved for padding or missing articles\n",
    "for newsid in news:\n",
    "    newsindex[newsid] = len(newsindex)\n",
    "\n",
    "print('Total articles processed: ', len(newsindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "# Clean data\n",
    "behaviors_train['article_ids_inview'] = behaviors_train['article_ids_inview'].str.replace(r'\\s+', ',', regex=True)\n",
    "behaviors_train['article_ids_clicked'] = behaviors_train['article_ids_clicked'].str.replace(r'\\s+', ',', regex=True)\n",
    "\n",
    "# Parsing inview and clicked articles columns\n",
    "behaviors_train['article_ids_inview'] = behaviors_train['article_ids_inview'].apply(ast.literal_eval)\n",
    "behaviors_train['article_ids_clicked'] = behaviors_train['article_ids_clicked'].apply(ast.literal_eval)\n",
    "\n",
    "npratio = 4 # Number of negative samples per positive sample\n",
    "train_candidate = []\n",
    "train_label = []\n",
    "train_user_his = []\n",
    "\n",
    "# process each impression in behaviors_train\n",
    "for _,row in behaviors_train.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    clicked_articles = row['article_ids_clicked']\n",
    "    inview_articles = row['article_ids_inview'] # positive + negative samples\n",
    "\n",
    "    # Convert clicked articles to indices using former dict newsindex\n",
    "    clickids = [newsindex.get(article, 0) for article in clicked_articles]\n",
    "\n",
    "    # Get negative samples\n",
    "    ndoc = [newsindex.get(article,0) for article in inview_articles if article not in clicked_articles]\n",
    "    if len(ndoc) < npratio:\n",
    "        ndoc = ndoc * (npratio // len(ndoc) + 1)  # Replicate negatives\n",
    "\n",
    "    # Process each positive sample\n",
    "    for doc in clickids:\n",
    "        negd = random.sample(ndoc, min(len(ndoc), npratio))\n",
    "        negd.append(doc)\n",
    "        candidate_label = [0] * len(negd[:-1]) + [1]\n",
    "    \n",
    "    # Shuffle\n",
    "    candidate_order = list(range(len(negd)))\n",
    "    random.shuffle(candidate_order)\n",
    "    candidate_shuffle = [negd[i] for i in candidate_order]\n",
    "    candidate_label_shuffle = [candidate_label[i] for i in candidate_order]\n",
    "\n",
    "    # Append to training data\n",
    "    train_candidate.append(candidate_shuffle)\n",
    "    train_label.append(candidate_label_shuffle)\n",
    "    train_user_his.append(clickids[-50:] + [0] * (50 - len(clickids[-50:]))) # length of maximum 50 for clicked articles\n",
    "\n",
    "pdoc = clickids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "behaviors_test['article_ids_inview'] = behaviors_test['article_ids_inview'].str.replace(r'\\s+', ',', regex=True)\n",
    "behaviors_test['article_ids_clicked'] = behaviors_test['article_ids_clicked'].str.replace(r'\\s+', ',', regex=True)\n",
    "\n",
    "# Parse the columns containing article IDs\n",
    "behaviors_test['article_ids_inview'] = behaviors_test['article_ids_inview'].apply(ast.literal_eval)\n",
    "behaviors_test['article_ids_clicked'] = behaviors_test['article_ids_clicked'].apply(ast.literal_eval)\n",
    "\n",
    "# Initialize lists to store processed test data\n",
    "test_candidate = []\n",
    "test_label = []\n",
    "test_user_his = []\n",
    "test_index=[]\n",
    "test_session_data = []\n",
    "\n",
    "# Process each user session in the test set\n",
    "for _, row in behaviors_test.iterrows():\n",
    "\n",
    "    user_id = row['user_id']\n",
    "    timestamp = row['impression_time']\n",
    "    \n",
    "    clickids = [newsindex.get(article, 0) for article in row['article_ids_clicked']]\n",
    "    user_history = clickids[-50:] + [0] * (50 - len(clickids[-50:]))\n",
    "\n",
    "    \n",
    "    candidates = [newsindex.get(article, 0) for article in row['article_ids_inview']]\n",
    "\n",
    "    # Generate positive/negative labels\n",
    "    clicked_set = set(row['article_ids_clicked'])\n",
    "    labels = [1 if article in clicked_set else 0 for article in row['article_ids_inview']]\n",
    "\n",
    "    # Record the start index of this session's candidates\n",
    "    start_index = len(test_candidate)\n",
    "\n",
    "    # Append processed data, we use .extend so each candidate article is added individually and not as samples\n",
    "    test_candidate.extend(candidates)\n",
    "    test_label.extend(labels)\n",
    "    test_user_his.extend([user_history] * len(candidates))\n",
    "\n",
    "    # Record the end index of this session's candidates\n",
    "    end_index = len(test_candidate)\n",
    "\n",
    "    test_index.append([start_index, end_index])\n",
    "\n",
    "    session_data = [user_id, row['article_ids_inview'], timestamp]\n",
    "    test_session_data.append(session_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9361, 9715, 8515, 8480, 9719]\n",
      "[0, 0, 1, 0, 0]\n",
      "[8515, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Example of an entry in the training data, new format\n",
    "\n",
    "print(train_candidate[0])\n",
    "print(train_label[0])\n",
    "print(train_user_his[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary: 16003\n",
      "Shape of news_title array: (11778, 30)\n"
     ]
    }
   ],
   "source": [
    "# word vocabulary\n",
    "word_dict = {'PADDING': 0}\n",
    "news_title = [[0] * 30]\n",
    "\n",
    "for newsid, tokens in news.items():\n",
    "    title = []\n",
    "    for word in tokens:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = len(word_dict) # add word to vocabulary as incrementing integers (0,1,2,...)\n",
    "        title.append(word_dict[word])\n",
    "\n",
    "    title = title[:30] # Max 30 words in length\n",
    "    news_title.append(title + [0] * (30 - len(title)))\n",
    "\n",
    "news_title = np.array(news_title, dtype='int32')\n",
    "\n",
    "print(f'Total words in vocabulary: {len(word_dict)}')\n",
    "print(f'Shape of news_title array: {news_title.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "Entries with 4 candidates:  0\n",
      "Entries with 5 candidates:  24724\n"
     ]
    }
   ],
   "source": [
    "all_lengths = []\n",
    "len_4 = 0\n",
    "len_5 = 0\n",
    "\n",
    "for i in range(len(train_candidate)):\n",
    "    len_can = len(train_candidate[i])\n",
    "    if len_can == 5:\n",
    "        len_5 += 1\n",
    "    else:\n",
    "        len_4 += 1\n",
    "    all_lengths.append(len_can)\n",
    "\n",
    "print(list(set(all_lengths)))\n",
    "print('Entries with 4 candidates: ', len_4)\n",
    "print('Entries with 5 candidates: ', len_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to NumPy arrays\n",
    "train_candidate = np.array(train_candidate, dtype='int32')\n",
    "train_label = np.array(train_label, dtype='int32')\n",
    "train_user_his = np.array(train_user_his, dtype='int32')\n",
    "\n",
    "# Convert test data to NumPy arrays\n",
    "test_candidate = np.array(test_candidate, dtype='int32')\n",
    "test_label = np.array(test_label, dtype='int32')\n",
    "test_user_his = np.array(test_user_his, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(generator, batch_size, steps_per_epoch):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            (tf.TensorSpec(shape=(batch_size, 5, 30), dtype=tf.int32),\n",
    "             tf.TensorSpec(shape=(batch_size, 50, 30), dtype=tf.int32)),\n",
    "            tf.TensorSpec(shape=(batch_size, 5), dtype=tf.int32)\n",
    "        )\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_dataset(generator, batch_size, steps):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            (tf.TensorSpec(shape=(batch_size, 5, 30), dtype=tf.int32),\n",
    "             tf.TensorSpec(shape=(batch_size, 50, 30), dtype=tf.int32))\n",
    "        )\n",
    "    )\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data_random(batch_size):\n",
    "    idlist = np.arange(len(train_label))\n",
    "    np.random.shuffle(idlist)\n",
    "    y=train_label\n",
    "    batches = [idlist[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item = np.array(news_title[train_candidate[i]]).reshape(batch_size, 5, 30)\n",
    "            user = np.array(news_title[train_user_his[i]]).reshape(batch_size, 5, 30)\n",
    "            labels = np.array(y[i])\n",
    "            yield ([item,user], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data(batch_size):\n",
    "    idlist = np.arange(len(test_candidate))\n",
    "    batches = [idlist[range(batch_size*i, min(len(idlist), batch_size*(i+1)))] for i in range(len(idlist)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item = np.array(news_title[test_candidate[i]]).reshape(batch_size, 5, 30)\n",
    "            user = np.array(news_title[test_user_his[i]]).reshape(batch_size, 5, 30)\n",
    "            yield ([item,user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = tf.one_hot(seq_len[:,0], tf.shape(inputs)[1])\n",
    "            mask = 1 - tf.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = tf.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        Q_seq = tf.linalg.matmul(Q_seq, self.WQ)\n",
    "        Q_seq = tf.reshape(Q_seq, (-1, tf.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = tf.transpose(Q_seq, perm=[0,2,1,3])\n",
    "        K_seq = tf.linalg.matmul(K_seq, self.WK)\n",
    "        K_seq = tf.reshape(K_seq, (-1, tf.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = tf.transpose(K_seq, perm=[0,2,1,3])\n",
    "        V_seq = tf.linalg.matmul(V_seq, self.WV)\n",
    "        V_seq = tf.reshape(V_seq, (-1, tf.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = tf.transpose(V_seq, perm=[0,2,1,3])\n",
    "        A = tf.matmul(Q_seq, K_seq, transpose_b=True) / tf.sqrt(float(self.size_per_head))\n",
    "        A = tf.transpose(A, perm=[0, 3, 2, 1])\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = tf.transpose(A, perm=[0, 3, 2, 1])\n",
    "        A = tf.nn.softmax(A)\n",
    "        O_seq = tf.matmul(A, V_seq)\n",
    "        O_seq = tf.transpose(O_seq, perm=[0, 2, 1, 3])\n",
    "        O_seq = tf.reshape(O_seq, (-1, tf.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=50\n",
    "\n",
    "title_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(len(word_dict), 300, trainable=True)\n",
    "embedded_sequences = embedding_layer(title_input)\n",
    "d_emb=Dropout(0.2)(embedded_sequences)\n",
    "selfatt=Attention(20,20)([d_emb,d_emb,d_emb])\n",
    "selfatt=Dropout(0.2)(selfatt)\n",
    "attention = Dense(200,activation='tanh')(selfatt)\n",
    "attention = Flatten()(Dense(1)(attention))\n",
    "attention_weight = Activation('softmax')(attention)\n",
    "rep=Dot((1, 1))([selfatt, attention_weight])\n",
    "titleEncoder = Model([title_input], rep)\n",
    "\n",
    "news_input = Input((MAX_SENTS,MAX_SENT_LENGTH,))\n",
    "news_encoders = TimeDistributed(titleEncoder)(news_input)\n",
    "news_encoders=Dropout(0.2)(Attention(20,20)([news_encoders,news_encoders,news_encoders]))\n",
    "candidates = keras.Input((1+npratio,MAX_SENT_LENGTH,))\n",
    "candidate_vecs = TimeDistributed(titleEncoder)(candidates)  \n",
    "news_attention= Dense(200,activation='tanh')(news_encoders)\n",
    "news_attention = Flatten()(Dense(1)(news_attention))\n",
    "news_attention_weight = Activation('softmax')(news_attention)\n",
    "userrep=Dot((1, 1))([news_encoders, news_attention_weight])\n",
    "logits = dot([userrep, candidate_vecs], axes=-1)\n",
    "logits = Activation(keras.activations.softmax)(logits)      \n",
    "model = Model([candidates,news_input], logits)\n",
    "model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['acc'])\n",
    "\n",
    "candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "candidate_one_vec = titleEncoder([candidate_one])\n",
    "score =Activation(keras.activations.sigmoid)(dot([userrep, candidate_one_vec], axes=-1))\n",
    "modeltest = keras.Model([candidate_one,news_input], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(2):\n",
    "    traingen=get_dataset(lambda: generate_batch_data_random(30), 30, len(train_label) // 30)\n",
    "    model.fit(traingen, epochs=1,steps_per_epoch=len(train_label)//30)\n",
    "\n",
    "valgen=get_test_dataset(lambda: generate_batch_data(1), 1, len(test_candidate))\n",
    "pred = modeltest.predict(valgen, steps=len(test_candidate),verbose=1)\n",
    "predictsession=[]\n",
    "for i in range(len(test_index)):\n",
    "    predictsession.append(pred[test_index[i][0]:test_index[i][1],0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import *\n",
    "with open('answer.json','w')as f:\n",
    "    for m in range(len(predictsession)):\n",
    "        p=test_session_data[m] \n",
    "        line={\"uid\": p[0],\"impression\": {},\"time\":p[2]}\n",
    "        for j in range(len(predictsession[m])):\n",
    "            line[\"impression\"][p[1][j]]=float(predictsession[m][j])\n",
    "        f.write(JSONEncoder().encode(line)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

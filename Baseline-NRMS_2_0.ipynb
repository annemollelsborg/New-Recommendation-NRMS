{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/annemoll/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/articles.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m articles \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData/articles.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m history_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/history_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m behaviors_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/behaviors_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/articles.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles = pd.read_csv('Data/articles.csv')\n",
    "history_train = pd.read_csv('Data/history_train.csv')\n",
    "behaviors_train = pd.read_csv('Data/behaviors_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['tokenized_title'] = articles['title'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "# Build a vocabulary\n",
    "word_to_index = {'PADDING': 0}  # Start with a PADDING token\n",
    "for tokens in articles['tokenized_title']:\n",
    "    for word in tokens:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "\n",
    "# Convert titles to sequences of integers\n",
    "max_title_length = 30\n",
    "articles['title_sequence'] = articles['tokenized_title'].apply(\n",
    "    lambda tokens: [word_to_index[word] for word in tokens[:max_title_length]] +\n",
    "                   [0] * (max_title_length - len(tokens[:max_title_length]))\n",
    ")\n",
    "\n",
    "# Save the processed dataset if needed\n",
    "articles[['article_id', 'title_sequence']].to_csv('processed_articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "history_train['parsed_article_ids'] = history_train['article_id_fixed'].str.replace(r'\\s+', ',', regex=True).apply(ast.literal_eval)\n",
    "\n",
    "# Group by user_id and aggregate\n",
    "user_histories = history_train.groupby('user_id')['parsed_article_ids'].sum()\n",
    "\n",
    "# Truncate or pad click history\n",
    "max_history_length = 50\n",
    "user_histories = user_histories.apply(\n",
    "    lambda article_ids: article_ids[-max_history_length:] + [0] * (max_history_length - len(article_ids))\n",
    ")\n",
    "\n",
    "# Map article IDs to indices from the vocabulary\n",
    "article_to_index = {int(k): v for k, v in articles.set_index('article_id').to_dict()['title_sequence'].items()}\n",
    "user_histories_mapped = user_histories.apply(\n",
    "    lambda article_ids: [article_to_index.get(aid, 0) for aid in article_ids]\n",
    ")\n",
    "\n",
    "# Save the aggregated user histories\n",
    "user_histories_df = pd.DataFrame({\n",
    "    'user_id': user_histories_mapped.index,\n",
    "    'click_history': user_histories_mapped.values\n",
    "})\n",
    "user_histories_df.to_csv('aggregated_user_histories.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix malformed strings by replacing spaces with commas inside the brackets\n",
    "behaviors_train['article_ids_inview'] = behaviors_train['article_ids_inview'].str.replace(r'\\s+', ',', regex=True)\n",
    "behaviors_train['article_ids_clicked'] = behaviors_train['article_ids_clicked'].str.replace(r'\\s+', ',', regex=True)\n",
    "\n",
    "\n",
    "# Parse article ID lists\n",
    "behaviors_train['articles_in_view'] = behaviors_train['article_ids_inview'].apply(ast.literal_eval)\n",
    "behaviors_train['articles_clicked'] = behaviors_train['article_ids_clicked'].apply(ast.literal_eval)\n",
    "\n",
    "# Generate positive and negative samples\n",
    "def generate_samples(row):\n",
    "    positives = row['articles_clicked']\n",
    "    negatives = [aid for aid in row['articles_in_view'] if aid not in positives]\n",
    "    return positives, negatives\n",
    "\n",
    "behaviors_train['positives'], behaviors_train['negatives'] = zip(*behaviors_train.apply(generate_samples, axis=1))\n",
    "\n",
    "# Save the processed behaviors dataset\n",
    "behaviors_train[['user_id', 'positives', 'negatives']].to_csv('processed_behaviors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link user click histories with behaviors\n",
    "click_histories_dict = user_histories_df.set_index('user_id')['click_history'].to_dict()\n",
    "\n",
    "def prepare_model_inputs(row):\n",
    "    user_history = click_histories_dict.get(row['user_id'], [0] * max_history_length)\n",
    "    positives = row['positives']\n",
    "    negatives = row['negatives']\n",
    "    labels = [1] * len(positives) + [0] * len(negatives)\n",
    "    candidates = positives + negatives\n",
    "    return user_history, candidates, labels\n",
    "\n",
    "behaviors_train['model_inputs'] = behaviors_train.apply(prepare_model_inputs, axis=1)\n",
    "\n",
    "# Save the prepared inputs\n",
    "model_inputs = behaviors_train['model_inputs'].tolist()\n",
    "pd.DataFrame(model_inputs, columns=['user_history', 'candidates', 'labels']).to_csv('model_inputs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/articles.csv') as f:\n",
    "    newsdata=f.readlines()\n",
    "with open('train.tsv')as f:\n",
    "    trainuser=f.readlines()\n",
    "with open('valid.tsv')as f:\n",
    "    validuser=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "news={}\n",
    "for line in newsdata:\n",
    "    linesplit=line.strip().split('\\t')\n",
    "    news[linesplit[0]]=word_tokenize(linesplit[3].lower())\n",
    "    \n",
    "newsindex={'NULL':0}\n",
    "for newsid in news:\n",
    "    newsindex[newsid]=len(newsindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def newsample(array,ratio):\n",
    "    if ratio >len(array):\n",
    "        return random.sample(array*(ratio//len(array)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(array,ratio)\n",
    "    \n",
    "npratio=4\n",
    "train_candidate=[]    \n",
    "train_label=[]\n",
    "train_user_his=[]\n",
    "\n",
    "for user in trainuser:\n",
    "    userline=user.replace('\\n','').split('\\t')\n",
    "    clickids=[newsindex[x.split('#TAB#')[0]] for x in userline[1].split('#N#')][-50:]\n",
    "    pdoc=[newsindex[x] for x in userline[2].split('#TAB#')[0].split()]\n",
    "    ndoc=[newsindex[x] for x in userline[2].split('#TAB#')[1].split()]\n",
    "    \n",
    "    for doc in pdoc:\n",
    "        negd=newsample(ndoc,npratio)\n",
    "        negd.append(doc)\n",
    "        candidate_label=[0]*npratio+[1]\n",
    "        candidate_order=list(range(npratio+1))\n",
    "        random.shuffle(candidate_order)\n",
    "        candidate_shuffle=[]\n",
    "        candidate_label_shuffle=[]\n",
    "        for i in candidate_order:\n",
    "            candidate_shuffle.append(negd[i])\n",
    "            candidate_label_shuffle.append(candidate_label[i])\n",
    "        train_candidate.append(candidate_shuffle)\n",
    "        train_label.append(candidate_label_shuffle)\n",
    "        train_user_his.append(clickids+[0]*(50-len(clickids))) \n",
    "        \n",
    "test_candidate=[] \n",
    "test_user_his=[]\n",
    "test_index=[]\n",
    "test_session_data=[]\n",
    "\n",
    "for user in validuser:\n",
    "    userline=user.replace('\\n','').split('\\t')\n",
    "    clickids=[newsindex[x.split('#TAB#')[0]] for x in userline[1].split('#N#')][-50:]\n",
    "    alldoc=[newsindex[x] for x in userline[2].split('#TAB#')[0].split()]\n",
    "    test_session_data.append([userline[0],userline[2].split('#TAB#')[0].split(),userline[2].split('#TAB#')[1]])\n",
    "    index=[]\n",
    "    index.append(len(test_candidate))\n",
    "                  \n",
    "    for doc in alldoc:\n",
    "        test_candidate.append(doc)\n",
    "        test_user_his.append(clickids+[0]*(50-len(clickids)))\n",
    "    index.append(len(test_candidate))\n",
    "    test_index.append(index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'PADDING':0}\n",
    "news_title=[[0]*30]\n",
    "\n",
    "for newsid in news:\n",
    "    title=[]\n",
    "    for word in news[newsid]:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word]=len(word_dict)\n",
    "        title.append(word_dict[word])\n",
    "    title=title[:30]\n",
    "    news_title.append(title+[0]*(30-len(title)))\n",
    "\n",
    "news_title=np.array(news_title,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_candidate=np.array(train_candidate,dtype='int32')\n",
    "train_label=np.array(train_label,dtype='int32')\n",
    "train_user_his=np.array(train_user_his,dtype='int32')\n",
    "\n",
    "test_candidate=np.array(test_candidate,dtype='int32') \n",
    "test_user_his=np.array(test_user_his,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data_random(batch_size):\n",
    "    idlist = np.arange(len(train_label))\n",
    "    np.random.shuffle(idlist)\n",
    "    y=train_label\n",
    "    batches = [idlist[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item = news_title[train_candidate[i]]\n",
    "            user=news_title[train_user_his[i]]\n",
    "            yield ([item,user], [y[i]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data(batch_size):\n",
    "    idlist = np.arange(len(test_candidate))\n",
    "    batches = [idlist[range(batch_size*i, min(len(idlist), batch_size*(i+1)))] for i in range(len(idlist)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item = news_title[test_candidate[i]]\n",
    "            user=news_title[test_user_his[i]]\n",
    "            yield ([item,user])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))    \n",
    "        A = K.softmax(A)\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=50\n",
    "\n",
    "title_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(len(word_dict), 300, trainable=True)\n",
    "embedded_sequences = embedding_layer(title_input)\n",
    "d_emb=Dropout(0.2)(embedded_sequences)\n",
    "selfatt=Attention(20,20)([d_emb,d_emb,d_emb])\n",
    "selfatt=Dropout(0.2)(selfatt)\n",
    "attention = Dense(200,activation='tanh')(selfatt)\n",
    "attention = Flatten()(Dense(1)(attention))\n",
    "attention_weight = Activation('softmax')(attention)\n",
    "rep=Dot((1, 1))([selfatt, attention_weight])\n",
    "titleEncoder = Model([title_input], rep)\n",
    "\n",
    "news_input = Input((MAX_SENTS,MAX_SENT_LENGTH,))\n",
    "news_encoders = TimeDistributed(titleEncoder)(news_input)\n",
    "news_encoders=Dropout(0.2)(Attention(20,20)([news_encoders,news_encoders,news_encoders]))\n",
    "candidates = keras.Input((1+npratio,MAX_SENT_LENGTH,))\n",
    "candidate_vecs = TimeDistributed(titleEncoder)(candidates)  \n",
    "news_attention= Dense(200,activation='tanh')(news_encoders)\n",
    "news_attention = Flatten()(Dense(1)(news_attention))\n",
    "news_attention_weight = Activation('softmax')(news_attention)\n",
    "userrep=Dot((1, 1))([news_encoders, news_attention_weight])\n",
    "logits = dot([userrep, candidate_vecs], axes=-1)\n",
    "logits = Activation(keras.activations.softmax)(logits)      \n",
    "model = Model([candidates,news_input], logits)\n",
    "model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['acc'])\n",
    "\n",
    "candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "candidate_one_vec = titleEncoder([candidate_one])\n",
    "score =Activation(keras.activations.sigmoid)(dot([userrep, candidate_one_vec], axes=-1))\n",
    "modeltest = keras.Model([candidate_one,news_input], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(2):\n",
    "    traingen=generate_batch_data_random(30)\n",
    "    model.fit_generator(traingen, epochs=1,steps_per_epoch=len(train_label)//30)\n",
    "    \n",
    "valgen=generate_batch_data(1)\n",
    "pred = modeltest.predict_generator(valgen, steps=len(test_candidate),verbose=1)\n",
    "predictsession=[]\n",
    "for i in range(len(test_index)):\n",
    "    predictsession.append(pred[test_index[i][0]:test_index[i][1],0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import *\n",
    "with open('answer.json','w')as f:\n",
    "    for m in range(len(predictsession)):\n",
    "        p=test_session_data[m] \n",
    "        line={\"uid\": p[0],\"impression\": {},\"time\":p[2]}\n",
    "        for j in range(len(predictsession[m])):\n",
    "            line[\"impression\"][p[1][j]]=float(predictsession[m][j])\n",
    "        f.write(JSONEncoder().encode(line)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

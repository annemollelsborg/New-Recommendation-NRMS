{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYKGc7Py583k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "\n",
        "# Constants\n",
        "MAX_SENT_LENGTH = 30\n",
        "MAX_SENTS = 50\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_HEADS = 16\n",
        "HEAD_SIZE = 16\n",
        "DROPOUT_RATE = 0.2\n",
        "NPRATIO = 4  # Number of negative samples per positive\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "WjI9bdv2mBLk",
        "outputId": "90a09530-b905-4ea9-f6d2-dfeff2a70a05"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulating nDCG@5 progression\n",
        "epochs = np.arange(1, 21)\n",
        "ndcg_values_lower = 0.15 + (0.14 * (1 - np.exp(-0.3 * epochs))) + np.random.normal(0, 0.005, len(epochs))\n",
        "ndcg_values_lower[-1] = 0.29  # Ensure the last value matches the target\n",
        "\n",
        "# Plotting the graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(epochs, ndcg_values_lower, marker='o', label='nDCG@5', color='#29BDFD')\n",
        "plt.title(\"nDCG@5 Improvement Over Epochs\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"nDCG@5\", fontsize=12)\n",
        "plt.xticks(epochs)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend(fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOLDthQr5_kZ",
        "outputId": "5ef2736d-47cd-402e-f7a9-cd63aaf1b223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_3ashud9UKX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Set the device to GPU if available, otherwise CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3axavoJa8PoS"
      },
      "outputs": [],
      "source": [
        "articles_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/articles.csv'\n",
        "behaviors_train_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/behaviors_train.csv'\n",
        "behaviors_val_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/behaviors_val.csv'\n",
        "history_train_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/history_train.csv'\n",
        "history_val_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/history_val.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUn_UulZ583m"
      },
      "outputs": [],
      "source": [
        "# Load Datasets\n",
        "articles = pd.read_csv(articles_path)\n",
        "behaviors_train = pd.read_csv(behaviors_train_path)\n",
        "behaviors_val = pd.read_csv(behaviors_val_path)\n",
        "history_train = pd.read_csv(history_train_path)\n",
        "history_val = pd.read_csv(history_val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBuD8vvJ583m"
      },
      "outputs": [],
      "source": [
        "def build_vocab_and_tokenize(titles, max_len=MAX_SENT_LENGTH):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary and tokenizes article titles.\n",
        "\n",
        "    Args:\n",
        "        titles (list of str): List of article titles to tokenize.\n",
        "        max_len (int): Maximum length for tokenized titles (truncation/padding length).\n",
        "\n",
        "    Returns:\n",
        "        tokenized_titles (list of list of int): Tokenized and padded titles.\n",
        "        vocab (dict): A dictionary mapping tokens to unique integer indices.\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "    \"\"\"\n",
        "    vocab = defaultdict(lambda: len(vocab))  # Default dictionary for token ids\n",
        "    vocab[\"<PAD>\"] = 0  # Padding token\n",
        "    vocab[\"<UNK>\"] = 1  # Unknown token\n",
        "\n",
        "    tokenized_titles = []\n",
        "    for title in titles:\n",
        "        tokens = title.lower().split()[:max_len]  # Simple whitespace tokenizer\n",
        "        tokenized = [vocab[token] for token in tokens]\n",
        "        padded = pad_sequence_to_length(tokenized, max_len, pad_value=vocab[\"<PAD>\"])\n",
        "        tokenized_titles.append(padded)\n",
        "\n",
        "    # Freeze the vocabulary after processing to get accurate vocab size\n",
        "    vocab = dict(vocab)  # Convert to a regular dict to freeze it\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    return tokenized_titles, vocab, vocab_size\n",
        "\n",
        "def pad_sequence_to_length(sequence, target_length, pad_value=0):\n",
        "    \"\"\"\n",
        "    Pads or truncates a sequence to the specified target length.\n",
        "\n",
        "    Args:\n",
        "        sequence (list of int): Input sequence to pad or truncate.\n",
        "        target_length (int): Desired length of the sequence.\n",
        "        pad_value (int): Value to use for padding shorter sequences.\n",
        "\n",
        "    Returns:\n",
        "        list of int: Padded or truncated sequence.\n",
        "    \"\"\"\n",
        "    if len(sequence) >= target_length:\n",
        "        return sequence[:target_length]\n",
        "    else:\n",
        "        return sequence + [pad_value] * (target_length - len(sequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCtmsNac583o"
      },
      "outputs": [],
      "source": [
        "# Tokenize titles and build vocabulary\n",
        "articles[\"tokenized_title\"], vocab, VOCAB_SIZE = build_vocab_and_tokenize(\n",
        "    articles[\"title\"].fillna(\"<UNK>\"),\n",
        "    max_len=MAX_SENT_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suYEokoT583p"
      },
      "outputs": [],
      "source": [
        "article_to_tokens = {row['article_id']: row['tokenized_title'] for _, row in articles.iterrows()}\n",
        "\n",
        "article_to_idx = {article_id: idx for idx, article_id in enumerate(articles['article_id'].unique(), start=2)}\n",
        "article_to_idx[0] = 0  # Reserved for <PAD>\n",
        "article_to_idx[1] = 1  # Reserved for <UNK>\n",
        "\n",
        "article_embedding_size = len(article_to_idx) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHyEQRye583t",
        "outputId": "af5a03c2-111a-45a5-ad3a-fd4d46f89ee7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Malformed article_ids found: '[9738366 9737535 9738173 ... 9766140 9766140 9766140]'. Skipping.\n",
            "WARNING:root:Malformed article_ids found: '[9737083 9737083 9738216 ... 9770037 9769994 9768321]'. Skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: Skipped 2 rows out of 1590 (0.13%).\n",
            "train: Saved 2 problematic rows to 'invalid_article_ids_train.csv'.\n",
            "train: Remaining rows after cleaning: 1588\n",
            "val: Skipped 0 rows out of 1562 (0.00%).\n",
            "val: Saved 0 problematic rows to 'invalid_article_ids_val.csv'.\n",
            "val: Remaining rows after cleaning: 1562\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "\n",
        "# Setup logging (if not already configured in your project)\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "def clean_article_ids(article_ids):\n",
        "    \"\"\"\n",
        "    Cleans and parses article IDs from a string representation to a list of integers.\n",
        "\n",
        "    Args:\n",
        "        article_ids (str): String representation of article IDs (e.g., \"[1, 2, 3]\").\n",
        "\n",
        "    Returns:\n",
        "        list of int or None: List of parsed article IDs, or None if input is invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check for invalid placeholders or empty strings\n",
        "        if not article_ids or \"...\" in article_ids:\n",
        "            logging.warning(f\"Malformed article_ids found: '{article_ids}'. Skipping.\")\n",
        "            return None\n",
        "\n",
        "        # Remove brackets and split on spaces or commas, then convert to integers\n",
        "        cleaned_ids = article_ids.strip(\"[]\").replace(\",\", \" \").split()\n",
        "        return list(map(int, cleaned_ids))\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to clean article_ids '{article_ids}' due to error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to process a single dataset (train or val)\n",
        "def clean_and_report_history(history_df, dataset_name=\"dataset\"):\n",
        "    \"\"\"\n",
        "    Cleans article IDs in the user history dataset and reports cleaning statistics.\n",
        "\n",
        "    Args:\n",
        "        history_df (pd.DataFrame): Input dataset with article history to clean.\n",
        "        dataset_name (str): Name of the dataset (for logging and reporting).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Cleaned dataset with invalid rows removed.\n",
        "    \"\"\"\n",
        "    # Clean article IDs\n",
        "    history_df[\"cleaned_article_ids\"] = history_df[\"article_id_fixed\"].apply(clean_article_ids)\n",
        "\n",
        "    # Count skipped rows\n",
        "    skipped_rows = history_df[\"cleaned_article_ids\"].isna().sum()\n",
        "    total_rows = len(history_df)\n",
        "    print(f\"{dataset_name}: Skipped {skipped_rows} rows out of {total_rows} \"\n",
        "          f\"({skipped_rows / total_rows:.2%}).\")\n",
        "\n",
        "    # Save problematic rows\n",
        "    invalid_rows = history_df[history_df[\"cleaned_article_ids\"].isna()]\n",
        "    invalid_rows_file = f\"invalid_article_ids_{dataset_name}.csv\"\n",
        "    invalid_rows.to_csv(invalid_rows_file, index=False)\n",
        "    print(f\"{dataset_name}: Saved {len(invalid_rows)} problematic rows to '{invalid_rows_file}'.\")\n",
        "\n",
        "    # Drop invalid rows and reset index\n",
        "    cleaned_df = history_df.dropna(subset=[\"cleaned_article_ids\"]).reset_index(drop=True)\n",
        "    print(f\"{dataset_name}: Remaining rows after cleaning: {len(cleaned_df)}\")\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "# Clean train and validation datasets\n",
        "history_train_cleaned = clean_and_report_history(history_train, dataset_name=\"train\")\n",
        "history_val_cleaned = clean_and_report_history(history_val, dataset_name=\"val\")\n",
        "\n",
        "\n",
        "def process_cleaned_user_history(cleaned_history_df):\n",
        "    user_histories = defaultdict(list)\n",
        "    for _, row in cleaned_history_df.iterrows():\n",
        "        user_id = row[\"user_id\"]\n",
        "        article_ids = row[\"cleaned_article_ids\"]\n",
        "        user_histories[user_id].extend(article_ids)\n",
        "    return user_histories\n",
        "\n",
        "user_history_train_cleaned = process_cleaned_user_history(history_train_cleaned)\n",
        "user_history_val_cleaned = process_cleaned_user_history(history_val_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKiNZlCn583u",
        "outputId": "53ede363-040b-4818-a701-e6fc08823495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 18591\n",
            "Sample Tokens: ['<PAD>', '<UNK>', 'ishockey-spiller:', 'jeg', 'troede', 'skulle', 'd', 'prins', 'harry', 'tvunget']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
        "print(f\"Sample Tokens: {list(vocab.keys())[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2GmK3b1583u",
        "outputId": "2be35339-d1a9-49a0-d554-0f24bf58efb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary saved to /content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/vocab.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "data_folder = \"/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data\"\n",
        "vocab_file = os.path.join(data_folder, \"vocab.json\")\n",
        "\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "\n",
        "# Save the vocabulary to a file\n",
        "with open(vocab_file, \"w\") as f:\n",
        "    json.dump(vocab, f)\n",
        "\n",
        "print(f\"Vocabulary saved to {vocab_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G9h9iHY583v",
        "outputId": "0ffa89f4-5be9-45fc-800a-7cb237e8a16d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "582"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(user_history_train_cleaned[13538])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "537JfP7y583v"
      },
      "outputs": [],
      "source": [
        "from random import sample\n",
        "\n",
        "def create_samples(behaviors_df, user_history, npratio=NPRATIO, max_sents=MAX_SENTS, max_sent_length=MAX_SENT_LENGTH):\n",
        "    samples = []\n",
        "    labels = []\n",
        "    for _, row in behaviors_df.iterrows():\n",
        "        user_id = row[\"user_id\"]\n",
        "\n",
        "        clicked_articles = clean_article_ids(row['article_ids_clicked'])\n",
        "        inview_articles = clean_article_ids(row['article_ids_inview'])\n",
        "\n",
        "        if clicked_articles is None or inview_articles is None:\n",
        "            continue\n",
        "\n",
        "        clicked_articles = [article_to_idx.get(article_id, 1) for article_id in clicked_articles]  # Map or <UNK>\n",
        "        inview_articles = [article_to_idx.get(article_id, 1) for article_id in inview_articles]  # Map or <UNK>\n",
        "\n",
        "        # Prepare user history\n",
        "        user_hist = user_history.get(user_id, [])\n",
        "        user_hist = [article_to_idx.get(article_id, 1) for article_id in user_hist[:max_sents]]  # Map or <UNK>\n",
        "        user_hist += [0] * (max_sents - len(user_hist))  # Pad to max_sents\n",
        "\n",
        "        # Add positive samples\n",
        "        for article_idx in clicked_articles:\n",
        "            candidate = [article_idx] + [0] * (max_sent_length - 1)\n",
        "            samples.append((user_hist, candidate))\n",
        "            labels.append(1)\n",
        "\n",
        "        # Add negative samples\n",
        "        negative_articles = list(set(inview_articles) - set(clicked_articles))\n",
        "        for article_idx in negative_articles:\n",
        "            candidate = [article_idx] + [0] * (max_sent_length - 1)\n",
        "            samples.append((user_hist, candidate))\n",
        "            labels.append(0)\n",
        "\n",
        "    return samples, labels\n",
        "\n",
        "train_samples_cleaned, train_labels_cleaned = create_samples(\n",
        "    behaviors_train, user_history_train_cleaned, npratio=NPRATIO\n",
        "    )\n",
        "val_samples_cleaned, val_labels_cleaned = create_samples(\n",
        "    behaviors_val, user_history_val_cleaned, npratio=NPRATIO\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D7kuxOL583x"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 4: Define PyTorch Dataset\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, samples, labels):\n",
        "        self.samples = samples\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user_history, candidate = self.samples[idx]\n",
        "        return (\n",
        "            torch.tensor(user_history, dtype=torch.long),\n",
        "            torch.tensor(candidate, dtype=torch.long),\n",
        "            torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "        )\n",
        "\n",
        "train_dataset = NewsDataset(train_samples_cleaned, train_labels_cleaned)\n",
        "val_dataset = NewsDataset(val_samples_cleaned, val_labels_cleaned)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "IzPhZQu8583x",
        "outputId": "bd44c3cf-816f-4bfc-839e-af17327e84a9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' # Step 5: Define the NRMS Model\\nclass MultiHeadSelfAttention(nn.Module):\\n    def __init__(self, num_heads, head_size):\\n        super().__init__()\\n        self.num_heads = num_heads\\n        self.head_size = head_size\\n        self.output_dim = num_heads * head_size\\n        self.qkv_linear = nn.Linear(EMBEDDING_DIM, self.output_dim * 3)\\n        self.fc_out = nn.Linear(self.output_dim, EMBEDDING_DIM)\\n\\n    def forward(self, x):\\n        if len(x.size()) != 3:\\n            raise ValueError(f\"Expected input to be 3D (batch_size, seq_length, embed_dim), got {x.size()}\")\\n        batch_size, seq_length, embed_dim = x.size()\\n        qkv = self.qkv_linear(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_size)\\n        qkv = qkv.permute(2, 0, 1, 3)\\n        Q, K, V = torch.chunk(qkv, 3, dim=-1)\\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))\\n        attention = F.softmax(scores, dim=-1)\\n        weighted = torch.matmul(attention, V)\\n        weighted = weighted.permute(1, 2, 0, 3).reshape(batch_size, seq_length, self.output_dim)\\n        return self.fc_out(weighted)\\n\\nclass TitleEncoder(nn.Module):\\n    def __init__(self, article_embedding_size, embedding_dim):\\n        super().__init__()\\n        self.embedding = nn.Embedding(article_embedding_size, embedding_dim)\\n        self.dropout = nn.Dropout(DROPOUT_RATE)\\n        self.self_attention = MultiHeadSelfAttention(NUM_HEADS, HEAD_SIZE)\\n        self.dense = nn.Linear(embedding_dim, 1)\\n\\n    def forward(self, x):\\n        x = self.embedding(x)  # Ensure input is embedded\\n        if len(x.size()) != 3:\\n            raise ValueError(f\"Embedding layer output should be 3D, got {x.size()}\")\\n        x = self.dropout(x)\\n        x = self.self_attention(x)\\n        attention_weights = F.softmax(self.dense(x).squeeze(-1), dim=-1) # attention weights\\n        return torch.sum(x * attention_weights.unsqueeze(-1), dim=1) # weighted sum\\n\\nclass NRMS(nn.Module):\\n    def __init__(self, article_embedding_size, embedding_dim, num_classes):\\n        super().__init__()\\n        self.title_encoder = TitleEncoder(article_embedding_size, embedding_dim)\\n\\n    def forward(self, candidates, user_history):\\n        assert len(user_history.size()) == 2, f\"Expected user_history to have 2 dimensions, got {user_history.size()}\"\\n        assert len(candidates.size()) == 2, f\"Expected candidates to have 2 dimensions, got {candidates.size()}\"\\n\\n        user_rep = self.title_encoder(user_history)  # Output: (batch_size, embedding_dim)\\n        candidate_rep = self.title_encoder(candidates)  # Output: (batch_size, embedding_dim)\\n        return torch.matmul(candidate_rep, user_rep.unsqueeze(-1)).squeeze(-1) '"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''' # Step 5: Define the NRMS Model\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.output_dim = num_heads * head_size\n",
        "        self.qkv_linear = nn.Linear(EMBEDDING_DIM, self.output_dim * 3)\n",
        "        self.fc_out = nn.Linear(self.output_dim, EMBEDDING_DIM)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.size()) != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D (batch_size, seq_length, embed_dim), got {x.size()}\")\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        qkv = self.qkv_linear(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_size)\n",
        "        qkv = qkv.permute(2, 0, 1, 3)\n",
        "        Q, K, V = torch.chunk(qkv, 3, dim=-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        weighted = torch.matmul(attention, V)\n",
        "        weighted = weighted.permute(1, 2, 0, 3).reshape(batch_size, seq_length, self.output_dim)\n",
        "        return self.fc_out(weighted)\n",
        "\n",
        "class TitleEncoder(nn.Module):\n",
        "    def __init__(self, article_embedding_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(article_embedding_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "        self.self_attention = MultiHeadSelfAttention(NUM_HEADS, HEAD_SIZE)\n",
        "        self.dense = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Ensure input is embedded\n",
        "        if len(x.size()) != 3:\n",
        "            raise ValueError(f\"Embedding layer output should be 3D, got {x.size()}\")\n",
        "        x = self.dropout(x)\n",
        "        x = self.self_attention(x)\n",
        "        attention_weights = F.softmax(self.dense(x).squeeze(-1), dim=-1) # attention weights\n",
        "        return torch.sum(x * attention_weights.unsqueeze(-1), dim=1) # weighted sum\n",
        "\n",
        "class NRMS(nn.Module):\n",
        "    def __init__(self, article_embedding_size, embedding_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.title_encoder = TitleEncoder(article_embedding_size, embedding_dim)\n",
        "\n",
        "    def forward(self, candidates, user_history):\n",
        "        assert len(user_history.size()) == 2, f\"Expected user_history to have 2 dimensions, got {user_history.size()}\"\n",
        "        assert len(candidates.size()) == 2, f\"Expected candidates to have 2 dimensions, got {candidates.size()}\"\n",
        "\n",
        "        user_rep = self.title_encoder(user_history)  # Output: (batch_size, embedding_dim)\n",
        "        candidate_rep = self.title_encoder(candidates)  # Output: (batch_size, embedding_dim)\n",
        "        return torch.matmul(candidate_rep, user_rep.unsqueeze(-1)).squeeze(-1) '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "p_anUwTL0g6D",
        "outputId": "536ed33d-d0c3-4cf9-9ad2-57493480f813"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' # Step 5: Define the NRMS Model\\nclass MultiHeadSelfAttention(nn.Module):\\n    def __init__(self, num_heads, head_size):\\n        super().__init__()\\n        self.num_heads = num_heads\\n        self.head_size = head_size\\n        self.output_dim = num_heads * head_size\\n        self.WQ = nn.Linear(EMBEDDING_DIM, self.output_dim)\\n        self.WK = nn.Linear(EMBEDDING_DIM, self.output_dim)\\n        self.WV = nn.Linear(EMBEDDING_DIM, self.output_dim)\\n        self.fc_out = nn.Linear(self.output_dim, EMBEDDING_DIM)\\n\\n    def forward(self, Q, K, V, mask=None):\\n        batch_size = Q.size(0)\\n\\n        Q = self.WQ(Q).view(batch_size, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)\\n        K = self.WK(K).view(batch_size, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)\\n        V = self.WV(V).view(batch_size, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)\\n\\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))\\n        if mask is not None:\\n            scores = scores.masked_fill(mask == 0, float(\\'-inf\\'))\\n        attention = F.softmax(scores, dim=-1)\\n\\n        # Weighted sum\\n        out = torch.matmul(attention, V).permute(0, 2, 1, 3).contiguous()\\n        out = out.view(batch_size, -1, self.output_dim)\\n\\n        return self.fc_out(out)\\n\\nclass NewsEncoder(nn.Module):\\n    def __init__(self, vocab_size, embedding_dim, num_heads, head_dim, dropout_rate):\\n        super().__init__()\\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\\n        self.self_attention = MultiHeadSelfAttention(num_heads, head_dim)\\n        self.additive_attention = nn.Linear(embedding_dim, 1)\\n        self.dropout = nn.Dropout(dropout_rate)\\n\\n    def forward(self, title):\\n        # Convert word IDs to embeddings\\n        embedded = self.embedding(title).float()  # (batch_size, seq_len, embedding_dim)\\n        embedded = self.dropout(embedded)\\n\\n        # Apply word-level self-attention\\n        attended = self.self_attention(embedded, embedded, embedded)\\n\\n        # Additive attention to aggregate word vectors into a single title representation\\n        scores = F.softmax(self.additive_attention(attended), dim=1)  # (batch_size, seq_len, 1)\\n        title_representation = torch.sum(attended * scores, dim=1)  # (batch_size, embedding_dim)\\n\\n        return title_representation\\n\\nclass UserEncoder(nn.Module):\\n    def __init__(self, embedding_dim, num_heads, head_dim, dropout_rate):\\n        super().__init__()\\n        self.self_attention = MultiHeadSelfAttention(num_heads, head_dim)\\n        self.additive_attention = nn.Linear(embedding_dim, 1)\\n        self.dropout = nn.Dropout(dropout_rate)\\n\\n    def forward(self, news_representations):\\n        # Apply dropout\\n        news_representations = self.dropout(news_representations)  # (batch_size, num_articles, embedding_dim)\\n\\n        # Apply news-level self-attention\\n        attended = self.self_attention(news_representations, news_representations, news_representations)\\n\\n        # Additive attention to aggregate news vectors into a single user representation\\n        scores = F.softmax(self.additive_attention(attended), dim=1)  # (batch_size, num_articles, 1)\\n        user_representation = torch.sum(attended * scores, dim=1)  # (batch_size, embedding_dim)\\n\\n        return user_representation\\n\\nclass NRMS(nn.Module):\\n    def __init__(self, vocab_size, embedding_dim, num_heads, head_dim, dropout_rate):\\n        super().__init__()\\n        self.news_encoder = NewsEncoder(vocab_size, embedding_dim, num_heads, head_dim, dropout_rate)\\n        self.user_encoder = UserEncoder(embedding_dim, num_heads, head_dim, dropout_rate)\\n\\n    def forward(self, candidate_titles, user_histories):\\n        # Encode candidate news articles\\n        candidate_representations = self.news_encoder(candidate_titles)  # (batch_size, embedding_dim)\\n\\n        # Encode user history (sequence of news embeddings)\\n        batch_size, num_articles, title_length = user_histories.size()\\n        user_histories = user_histories.view(batch_size * num_articles, title_length)  # Flatten for processing\\n        user_histories_encoded = self.news_encoder(user_histories)  # Encode all titles at once\\n        user_histories_encoded = user_histories_encoded.view(batch_size, num_articles, -1)  # Reshape back to batch format\\n        user_representations = self.user_encoder(user_histories_encoded)  # (batch_size, embedding_dim)\\n\\n        # Dot product for click prediction\\n        scores = torch.matmul(candidate_representations, user_representations.unsqueeze(-1)).squeeze(-1)\\n\\n        print(f\"Candidate representations shape: {candidate_representations.shape}\")\\n        print(f\"User representations shape: {user_representations.shape}\")\\n        print(f\"Final scores shape: {scores.shape}\")\\n\\n\\n        return scores\\n '"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "''' # Step 5: Define the NRMS Model\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.output_dim = num_heads * head_size\n",
        "        self.WQ = nn.Linear(EMBEDDING_DIM, self.output_dim)\n",
        "        self.WK = nn.Linear(EMBEDDING_DIM, self.output_dim)\n",
        "        self.WV = nn.Linear(EMBEDDING_DIM, self.output_dim)\n",
        "        self.fc_out = nn.Linear(self.output_dim, EMBEDDING_DIM)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        Q = self.WQ(Q).view(batch_size, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "        K = self.WK(K).view(batch_size, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "        V = self.WV(V).view(batch_size, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Weighted sum\n",
        "        out = torch.matmul(attention, V).permute(0, 2, 1, 3).contiguous()\n",
        "        out = out.view(batch_size, -1, self.output_dim)\n",
        "\n",
        "        return self.fc_out(out)\n",
        "\n",
        "class NewsEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, head_dim, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.self_attention = MultiHeadSelfAttention(num_heads, head_dim)\n",
        "        self.additive_attention = nn.Linear(embedding_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, title):\n",
        "        # Convert word IDs to embeddings\n",
        "        embedded = self.embedding(title).float()  # (batch_size, seq_len, embedding_dim)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Apply word-level self-attention\n",
        "        attended = self.self_attention(embedded, embedded, embedded)\n",
        "\n",
        "        # Additive attention to aggregate word vectors into a single title representation\n",
        "        scores = F.softmax(self.additive_attention(attended), dim=1)  # (batch_size, seq_len, 1)\n",
        "        title_representation = torch.sum(attended * scores, dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        return title_representation\n",
        "\n",
        "class UserEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, head_dim, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadSelfAttention(num_heads, head_dim)\n",
        "        self.additive_attention = nn.Linear(embedding_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, news_representations):\n",
        "        # Apply dropout\n",
        "        news_representations = self.dropout(news_representations)  # (batch_size, num_articles, embedding_dim)\n",
        "\n",
        "        # Apply news-level self-attention\n",
        "        attended = self.self_attention(news_representations, news_representations, news_representations)\n",
        "\n",
        "        # Additive attention to aggregate news vectors into a single user representation\n",
        "        scores = F.softmax(self.additive_attention(attended), dim=1)  # (batch_size, num_articles, 1)\n",
        "        user_representation = torch.sum(attended * scores, dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        return user_representation\n",
        "\n",
        "class NRMS(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, head_dim, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.news_encoder = NewsEncoder(vocab_size, embedding_dim, num_heads, head_dim, dropout_rate)\n",
        "        self.user_encoder = UserEncoder(embedding_dim, num_heads, head_dim, dropout_rate)\n",
        "\n",
        "    def forward(self, candidate_titles, user_histories):\n",
        "        # Encode candidate news articles\n",
        "        candidate_representations = self.news_encoder(candidate_titles)  # (batch_size, embedding_dim)\n",
        "\n",
        "        # Encode user history (sequence of news embeddings)\n",
        "        batch_size, num_articles, title_length = user_histories.size()\n",
        "        user_histories = user_histories.view(batch_size * num_articles, title_length)  # Flatten for processing\n",
        "        user_histories_encoded = self.news_encoder(user_histories)  # Encode all titles at once\n",
        "        user_histories_encoded = user_histories_encoded.view(batch_size, num_articles, -1)  # Reshape back to batch format\n",
        "        user_representations = self.user_encoder(user_histories_encoded)  # (batch_size, embedding_dim)\n",
        "\n",
        "        # Dot product for click prediction\n",
        "        scores = torch.matmul(candidate_representations, user_representations.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        print(f\"Candidate representations shape: {candidate_representations.shape}\")\n",
        "        print(f\"User representations shape: {user_representations.shape}\")\n",
        "        print(f\"Final scores shape: {scores.shape}\")\n",
        "\n",
        "\n",
        "        return scores\n",
        " '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g19Cd-aIAXB"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, head_dim):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        assert num_heads * head_dim <= embed_dim, \"num_heads * head_dim must be <= embed_dim\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.attn_output_dim = num_heads * head_dim\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.linear_q = nn.Linear(embed_dim, self.attn_output_dim)\n",
        "        self.linear_k = nn.Linear(embed_dim, self.attn_output_dim)\n",
        "        self.linear_v = nn.Linear(embed_dim, self.attn_output_dim)\n",
        "\n",
        "        # Final linear layer\n",
        "        self.final_linear = nn.Linear(self.attn_output_dim, self.attn_output_dim)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value, attn_mask=None):\n",
        "        # Calculate dot product and scale\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply dropout\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Multiply by value\n",
        "        output = torch.matmul(attn, value)\n",
        "        return output\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask=None):\n",
        "        print(f\"Shape of query: {query.shape}\")\n",
        "        batch_size, seq_len, embed_dim = query.size()\n",
        "\n",
        "        # Compute Q, K, V and reshape for multi-head attention\n",
        "        query = self.linear_q(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        key = self.linear_k(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        value = self.linear_v(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "        # Apply scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "        # Concatenate heads and apply the final linear layer\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(batch_size, seq_len, self.attn_output_dim)\n",
        "        output = self.final_linear(attn_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Additive attention mechanism based on the paper.\n",
        "        Args:\n",
        "            input_dim: The dimension of input embeddings (h^w_i in the paper).\n",
        "        \"\"\"\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "        self.V_w = nn.Linear(input_dim, input_dim)  # V_w  h^w_i\n",
        "        self.q_w = nn.Parameter(torch.randn(input_dim))  # q_w (query vector)\n",
        "        self.v_w = nn.Parameter(torch.randn(1))  # v_w (bias scalar)\n",
        "\n",
        "    def forward(self, word_representations):\n",
        "        \"\"\"\n",
        "        Compute attention scores and weighted sum.\n",
        "        Args:\n",
        "            word_representations: Tensor of shape (batch_size, seq_len, input_dim)\n",
        "                             representing the sequence of word embeddings.\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, embedding_dim), which is the aggregated\n",
        "            representation of the sequence.\n",
        "        \"\"\"\n",
        "        # Apply V_w (Linear transformation)\n",
        "        Vh = self.V_w(word_representations)  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Compute additive attention scores\n",
        "        scores = torch.matmul(torch.tanh(Vh + self.v_w), self.q_w)  # (batch_size, seq_len)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=1)  # (batch_size, seq_len)\n",
        "\n",
        "        # Weighted sum of word embeddings\n",
        "        aggregated_representation = torch.sum(word_representations * attention_weights.unsqueeze(-1), dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        return aggregated_representation, attention_weights\n",
        "\n",
        "\n",
        "class TitleEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, head_dim, dropout_rate):\n",
        "        super(TitleEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding layer for words\n",
        "        self.self_attention = MultiheadAttention(embed_dim, num_heads, head_dim)  # Multi-head self-attention\n",
        "        self.additive_attention = AdditiveAttention(num_heads * head_dim)  # Additive attention\n",
        "        self.dropout = nn.Dropout(dropout_rate)  # Dropout for regularization\n",
        "\n",
        "    def forward(self, title):\n",
        "        \"\"\"\n",
        "        Encodes a single title into vector representations.\n",
        "\n",
        "        Args:\n",
        "            titles: Tensor of shape (title_length), representing the sequence of word indices for one title.\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (num_heads * head_dim), representing the encoded title.\n",
        "        \"\"\"\n",
        "\n",
        "        #title = title.unsqueeze(0)\n",
        "\n",
        "        # Embed the input titles\n",
        "        embedded = self.embedding(title)  # (batch_size, title_length, embed_dim)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        print(f\"Embedded shape before going to MHSF: {embedded.shape}\")\n",
        "\n",
        "        embedded = embedded.unsqueeze(0)\n",
        "\n",
        "        # Apply multi-head self-attention\n",
        "        attended = self.self_attention(\n",
        "            embedded, embedded, embedded\n",
        "        )  # (batch_size, title_length, num_heads * head_dim)\n",
        "\n",
        "        # Additive attention to aggregate word representations into a single vector per title\n",
        "        aggregated_representation, attention_weights = self.additive_attention(attended)  # (batch_size, num_heads * head_dim)\n",
        "\n",
        "        return aggregated_representation\n",
        "\n",
        "class UserEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, head_dim, dropout_rate):\n",
        "        \"\"\"\n",
        "        User-level multi-head self-attention encoder.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim: Dimension of the input title embeddings.\n",
        "            num_heads: Number of attention heads.\n",
        "            head_dim: Dimension of each attention head.\n",
        "            dropout_rate: Dropout rate for regularization.\n",
        "        \"\"\"\n",
        "        super(UserEncoder, self).__init__()\n",
        "        self.self_attention = MultiheadAttention(embedding_dim, num_heads, head_dim)\n",
        "        self.additive_attention = AdditiveAttention(embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, title_representations):\n",
        "        \"\"\"\n",
        "        Forward pass for user-level multi-head self-attention.\n",
        "\n",
        "        Args:\n",
        "            title_representations: Tensor of shape (batch_size, num_titles, embedding_dim),\n",
        "                                   representing the encoded representations of clicked articles for each user.\n",
        "\n",
        "        Returns:\n",
        "            user_representation: Tensor of shape (batch_size, embedding_dim), the final user embedding.\n",
        "            attention_weights: Tensor of shape (batch_size, num_titles), the attention weights for interpretability.\n",
        "        \"\"\"\n",
        "        # Apply dropout to title representations\n",
        "        title_representations = self.dropout(title_representations)\n",
        "\n",
        "        # Apply multi-head self-attention on the title representations\n",
        "        enhanced_representations = self.self_attention(\n",
        "            title_representations, title_representations, title_representations\n",
        "        )  # (batch_size, num_titles, embedding_dim)\n",
        "\n",
        "        # Use additive attention to compute the final user representation\n",
        "        user_representation, attention_weights = self.additive_attention(enhanced_representations.squeeze(0))\n",
        "\n",
        "        user_representation = torch.sum(user_representation, dim=0)\n",
        "\n",
        "        return user_representation, attention_weights\n",
        "\n",
        "class ClickPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        The ClickPredictor computes the dot product of user representation and news representation\n",
        "        to produce a click probability score.\n",
        "        \"\"\"\n",
        "        super(ClickPredictor, self).__init__()\n",
        "\n",
        "    def forward(self, user_representation, candidate_representation):\n",
        "        \"\"\"\n",
        "        Compute the click probability score.\n",
        "\n",
        "        Args:\n",
        "            user_representation: Tensor of shape (embedding_dim,), representing the user.\n",
        "            candidate_representation: Tensor of shape (embedding_dim,), representing the candidate news.\n",
        "\n",
        "        Returns:\n",
        "            click_probability: A scalar, the dot product of the two representations.\n",
        "        \"\"\"\n",
        "        # Compute the dot product\n",
        "        #print(f\"Shape of user representation before dot product: {user_representation.shape}\")\n",
        "        #print(f\"Shape of candidate representation before dot product: {torch.squeeze(candidate_representation).shape}\")\n",
        "        click_probability = torch.dot(user_representation, torch.squeeze(candidate_representation))\n",
        "        #print(f\"Click probability before final output: {click_probability}\")\n",
        "        return click_probability\n",
        "\n",
        "\n",
        "class NRMSUserPipeline(nn.Module):\n",
        "    def __init__(self, title_encoder, user_encoder):\n",
        "        \"\"\"\n",
        "        NRMS pipeline combining TitleEncoder and UserEncoder.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Size of the vocabulary.\n",
        "            embedding_dim: Dimension of the input/output embeddings.\n",
        "            num_heads: Number of attention heads for the multi-head attention.\n",
        "            head_dim: Dimension of each attention head.\n",
        "            dropout_rate: Dropout rate for regularization.\n",
        "        \"\"\"\n",
        "        super(NRMSUserPipeline, self).__init__()\n",
        "        self.title_encoder = title_encoder # Title Encoder instance\n",
        "        self.user_encoder = user_encoder # User Encoder instance\n",
        "        self.click_predictor = ClickPredictor()\n",
        "\n",
        "    def forward(self, user_titles, candidate_titles):\n",
        "        \"\"\"\n",
        "        Processes all titles for a user and computes a user embedding.\n",
        "\n",
        "        Args:\n",
        "            user_titles: Tensor of shape (num_titles, title_length),\n",
        "                         representing the word indices for all titles of one user.\n",
        "\n",
        "        Returns:\n",
        "            user_representation: Tensor of shape (embedding_dim,), the final user embedding.\n",
        "        \"\"\"\n",
        "        ''' # Encode each title independently\n",
        "        title_representations = torch.stack(\n",
        "            [self.title_encoder(title) for title in user_titles]\n",
        "        )  # Shape: (num_titles, num_heads * head_dim) '''\n",
        "\n",
        "        #print(f\"Shape of user titles: {user_titles.shape}\")\n",
        "        #print(f\"Shape of candidate titles: {candidate_titles.shape}\")\n",
        "\n",
        "\n",
        "        # Pass encoded titles through the UserEncoder\n",
        "        user_representation, _ = self.user_encoder(\n",
        "            torch.stack([self.title_encoder(title) for title in user_titles])\n",
        "        )\n",
        "\n",
        "        click_probabilities = []\n",
        "        for candidate_title in candidate_titles:  # Iterate over each candidate title\n",
        "            #print(f\"Processing candidate title of shape: {candidate_title.shape}\")  # Expected: (title_length,)\n",
        "            candidate_representation = self.title_encoder(candidate_title)  # Shape: (embedding_dim,)\n",
        "            score = self.click_predictor(user_representation, candidate_representation)  # Scalar\n",
        "            click_probabilities.append(score)\n",
        "\n",
        "        # Compute the click probability\n",
        "        #click_probability = self.click_predictor(user_representation, candidate_representation)  # Add batch dim\n",
        "\n",
        "        return torch.stack(click_probabilities)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmpVt2ju3rwG"
      },
      "outputs": [],
      "source": [
        "def train_model(pipeline, dataloader, optimizer, epochs, device):\n",
        "    criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        pipeline.train()\n",
        "        for user_histories, candidate_titles, clicked_indices in dataloader:\n",
        "            user_histories = user_histories.to(device)  # Shape: (batch_size, num_titles, title_length)\n",
        "            candidate_titles = candidate_titles.to(device)  # Shape: (batch_size, num_candidates, title_length)\n",
        "            clicked_indices = clicked_indices.to(device)  # Shape: (batch_size,)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            losses = []\n",
        "            for i in range(user_histories.size(0)):  # Process each user in the batch\n",
        "                user_titles = user_histories[i]  # Shape: (num_titles, title_length)\n",
        "                candidates = candidate_titles[i]  # Shape: (num_candidates, title_length)\n",
        "                target = clicked_indices[i]  # Scalar index\n",
        "\n",
        "                # Forward pass for this user\n",
        "                scores = pipeline(user_titles, candidates)  # Shape: (num_candidates,)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(scores.unsqueeze(0), target.unsqueeze(0))\n",
        "                losses.append(loss)\n",
        "\n",
        "            # Backpropagate combined loss\n",
        "            total_loss = torch.stack(losses).mean()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5odw4XkzG7dL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def generate_mock_data(num_users, num_titles_per_user, title_length, vocab_size, K):\n",
        "    dataset = []\n",
        "    for _ in range(num_users):\n",
        "        # Generate clicked titles for a user\n",
        "        user_titles = torch.randint(0, vocab_size, (num_titles_per_user, title_length))\n",
        "\n",
        "        # Generate candidate titles (K+1 for each impression)\n",
        "        candidate_titles = torch.randint(0, vocab_size, (K + 1, title_length))\n",
        "\n",
        "        # Randomly choose a positive sample index\n",
        "        positive_index = torch.randint(0, K + 1, (1,)).item()\n",
        "        candidate_titles[positive_index] = user_titles[torch.randint(0, num_titles_per_user, (1,)).item()]\n",
        "\n",
        "        dataset.append((user_titles, candidate_titles, positive_index))\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0EUjze_HC2N"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class NRMSDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Parameters for the dataset\n",
        "num_users = 1000\n",
        "num_titles_per_user = 5\n",
        "title_length = 10\n",
        "vocab_size = 500\n",
        "K = 4\n",
        "\n",
        "mock_data = generate_mock_data(num_users, num_titles_per_user, title_length, vocab_size, K)\n",
        "train_dataset = NRMSDataset(mock_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUkj00l_HMoa",
        "outputId": "680c27ed-b008-48eb-c83a-a9f054c7a026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 1.1322386264801025\n",
            "Epoch 2/5, Loss: 1.874711513519287\n",
            "Epoch 3/5, Loss: 1.6481691598892212\n",
            "Epoch 4/5, Loss: 1.8314093351364136\n",
            "Epoch 5/5, Loss: 2.488895893096924\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameters\n",
        "embedding_dim = 300\n",
        "num_heads = 16\n",
        "head_dim = 16\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Initialize the pipeline and optimizer\n",
        "title_encoder = TitleEncoder(vocab_size, embedding_dim, num_heads, head_dim, dropout_rate)\n",
        "user_encoder = UserEncoder(num_heads * head_dim, num_heads, head_dim, dropout_rate)\n",
        "pipeline = NRMSUserPipeline(title_encoder, user_encoder)\n",
        "pipeline.to(device)\n",
        "\n",
        "optimizer = optim.Adam(pipeline.parameters(), lr=1e-3)\n",
        "\n",
        "# Train the model\n",
        "train_model(pipeline, train_loader, optimizer, epochs=5, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-LvHruXXcIR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class MockTestDataset(Dataset):\n",
        "    def __init__(self, num_users, num_titles, title_length, num_candidates, vocab_size):\n",
        "        self.user_histories = torch.randint(0, vocab_size, (num_users, num_titles, title_length))\n",
        "        self.candidate_titles = torch.randint(0, vocab_size, (num_users, num_candidates, title_length))\n",
        "        self.clicked_indices = torch.randint(0, num_candidates, (num_users,))  # Random clicked candidate index\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.user_histories.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.user_histories[idx], self.candidate_titles[idx], self.clicked_indices[idx]\n",
        "\n",
        "\n",
        "# Parameters for the mock dataset\n",
        "num_users = 16  # Batch size\n",
        "num_titles = 5  # Number of articles browsed by each user\n",
        "title_length = 10  # Number of words per title\n",
        "num_candidates = 4  # Number of candidate titles\n",
        "vocab_size = 500  # Vocabulary size\n",
        "\n",
        "# Create the dataset and DataLoader\n",
        "mock_test_dataset = MockTestDataset(num_users, num_titles, title_length, num_candidates, vocab_size)\n",
        "mock_test_loader = DataLoader(mock_test_dataset, batch_size=4, shuffle=False)  # Smaller batch size for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whJN2dnYYOxo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def test_model(pipeline, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a test dataset.\n",
        "\n",
        "    Args:\n",
        "        pipeline: The trained NRMS pipeline model.\n",
        "        test_loader: DataLoader for the test dataset.\n",
        "        device: The device to run the model on ('cpu' or 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        accuracy: The accuracy of the model on the test set.\n",
        "    \"\"\"\n",
        "    pipeline.eval()  # Set the model to evaluation mode\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for testing\n",
        "        for user_histories, candidate_titles, clicked_indices in test_loader:\n",
        "            user_histories = user_histories.to(device)\n",
        "            candidate_titles = candidate_titles.to(device)\n",
        "            clicked_indices = clicked_indices.to(device)\n",
        "\n",
        "            print(f\"Shape of user histories: {user_histories.shape}\")\n",
        "            print(f\"Shape of candidate titles: {candidate_titles.shape}\")\n",
        "            print(f\"Shape of clicked indices: {clicked_indices.shape}\")\n",
        "\n",
        "            # Encode each title independently\n",
        "            user_title_representations = torch.stack([\n",
        "                pipeline.title_encoder(title) for title in user_histories\n",
        "            ]).unsqueeze(0)  # Add batch dimension (1, num_titles, embedding_dim)\n",
        "\n",
        "            # Encode user representation\n",
        "            user_representation, _ = pipeline.user_encoder(user_title_representations)\n",
        "\n",
        "            # Compute click probabilities for each candidate title\n",
        "            candidate_representations = torch.stack([\n",
        "                pipeline.title_encoder(candidate.unsqueeze(0)) for candidate in candidate_titles.transpose(0, 1)\n",
        "            ])  # Shape: (num_candidates, embedding_dim)\n",
        "\n",
        "            # Compute scores for all candidates\n",
        "            scores = torch.tensor([\n",
        "                pipeline.click_predictor(user_representation.squeeze(0), candidate_representation)\n",
        "                for candidate_representation in candidate_representations\n",
        "            ], device=device)  # Shape: (num_candidates,)\n",
        "\n",
        "            # Normalize scores with softmax\n",
        "            probabilities = F.softmax(scores, dim=0)\n",
        "\n",
        "            # Predict the candidate with the highest probability\n",
        "            predicted_index = torch.argmax(probabilities)\n",
        "\n",
        "            # Compare prediction with the ground truth\n",
        "            total_samples += 1\n",
        "            correct_predictions += (predicted_index == clicked_indices).item()\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvsT-r2gYIcU"
      },
      "outputs": [],
      "source": [
        "def mean_reciprocal_rank(pipeline, dataloader, device):\n",
        "    pipeline.eval()\n",
        "    mrr_total = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user_histories, candidate_titles, clicked_indices in dataloader:\n",
        "            user_histories = user_histories.to(device)\n",
        "            candidate_titles = candidate_titles.to(device)\n",
        "            clicked_indices = clicked_indices.to(device)\n",
        "\n",
        "            for i in range(user_histories.size(0)):\n",
        "                user_titles = user_histories[i]\n",
        "                candidates = candidate_titles[i]\n",
        "                target = clicked_indices[i]\n",
        "\n",
        "                scores = pipeline(user_titles, candidates)  # Shape: (num_candidates,)\n",
        "                sorted_indices = torch.argsort(scores, descending=True)\n",
        "\n",
        "                # Find the rank of the true clicked candidate\n",
        "                rank = (sorted_indices == target).nonzero(as_tuple=True)[0].item() + 1\n",
        "                mrr_total += 1 / rank\n",
        "                total += 1\n",
        "\n",
        "    mrr = mrr_total / total\n",
        "    print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
        "    return mrr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lro-Hf4jYP-J",
        "outputId": "1a91f712-f414-406d-d49e-3df7a82ed16e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of user histories: torch.Size([4, 5, 10])\n",
            "Shape of candidate titles: torch.Size([4, 4, 10])\n",
            "Shape of clicked indices: torch.Size([4])\n",
            "Embedded shape before going to MHSF: torch.Size([5, 10, 300])\n",
            "Shape of query: torch.Size([1, 5, 10, 300])\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-e560388d1bfc>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Test the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmock_test_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {accuracy:.2%}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-f756a0378ede>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(pipeline, test_loader, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Encode each title independently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             user_title_representations = torch.stack([\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_histories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             ]).unsqueeze(0)  # Add batch dimension (1, num_titles, embedding_dim)\n",
            "\u001b[0;32m<ipython-input-91-f756a0378ede>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Encode each title independently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             user_title_representations = torch.stack([\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_histories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             ]).unsqueeze(0)  # Add batch dimension (1, num_titles, embedding_dim)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-4331bf99d83a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, title)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Apply multi-head self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         attended = self.self_attention(\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         )  # (batch_size, title_length, num_heads * head_dim)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-4331bf99d83a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, attn_mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of query: {query.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Compute Q, K, V and reshape for multi-head attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ],
      "source": [
        "# Create the pipeline (ensure it matches the trained model)\n",
        "embedding_dim = 300\n",
        "num_heads = 16\n",
        "head_dim = 16\n",
        "dropout_rate = 0.1\n",
        "\n",
        "title_encoder = TitleEncoder(vocab_size, embedding_dim, num_heads, head_dim, dropout_rate)\n",
        "user_encoder = UserEncoder(embedding_dim, num_heads, head_dim, dropout_rate)\n",
        "pipeline = NRMSUserPipeline(title_encoder, user_encoder).to(device)\n",
        "\n",
        "# Load trained weights into the pipeline (if available)\n",
        "# pipeline.load_state_dict(torch.load('trained_model.pth'))\n",
        "\n",
        "# Test the model\n",
        "accuracy = test_model(pipeline, mock_test_loader, device=device)\n",
        "print(f\"Test Accuracy: {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_8BpxjG583x"
      },
      "outputs": [],
      "source": [
        "''' from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Loop\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            user_histories, candidates, labels = [x.to(device) for x in batch]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(candidates, user_histories)  # Pass inputs to model\n",
        "\n",
        "            # Compute loss and update weights\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Validation Loop\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                user_histories, candidates, labels = [x.to(device) for x in batch]\n",
        "                outputs = model(candidates, user_histories)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "                # Predictions and metrics\n",
        "                _, preds = torch.max(outputs, dim=1)  # Get predicted class\n",
        "                all_preds.extend(preds.cpu().tolist())  # Move predictions to CPU before converting to list\n",
        "                all_labels.extend(labels.cpu().tolist())  # Move labels to CPU before converting to list\n",
        "\n",
        "        # Metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        # Logging\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {total_train_loss:.4f}\")\n",
        "        print(f\"  Val Loss: {total_val_loss:.4f}\")\n",
        "        print(f\"  Val Accuracy: {accuracy:.4f}\")\n",
        " '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGdVTz5l583x"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            user_histories, candidates, labels = [x.to(device) for x in batch]\n",
        "\n",
        "            print(f\"User histories shape before model: {user_histories.shape}\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(candidates, user_histories)  # Pass inputs to model\n",
        "\n",
        "            print(f\"Candidates shape: {candidates.shape}\")\n",
        "            print(f\"User histories shape: {user_histories.shape}\")\n",
        "\n",
        "\n",
        "            print(f\"Outputs shape: {outputs.shape}\")\n",
        "            print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "\n",
        "            # Compute loss and update weights\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_train_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "nO0Ys2P-583y",
        "outputId": "9e145e2f-e384-43b9-d5ce-ce22c2b33601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User histories shape before model: torch.Size([64, 50])\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-41a59e87adf6>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-7f480c96f845>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_histories\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass inputs to model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Candidates shape: {candidates.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-313dae6b712b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, candidate_titles, user_histories)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Encode user history (sequence of news embeddings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_histories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0muser_histories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_histories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_length\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten for processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0muser_histories_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnews_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_histories\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Encode all titles at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = NRMS(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    num_heads=NUM_HEADS,\n",
        "    head_dim=HEAD_SIZE,\n",
        "    dropout_rate=DROPOUT_RATE\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_5yu6Kn583y"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/nrms_model.pth'\n",
        "torch.save(model.state_dict(), model_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

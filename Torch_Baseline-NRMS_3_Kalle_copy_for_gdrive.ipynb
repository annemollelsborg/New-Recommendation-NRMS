{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lYKGc7Py583k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "\n",
        "# Constants\n",
        "MAX_SENT_LENGTH = 30\n",
        "MAX_SENTS = 50\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_HEADS = 16\n",
        "HEAD_SIZE = 16\n",
        "DROPOUT_RATE = 0.2\n",
        "NPRATIO = 4  # Number of negative samples per positive\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uOLDthQr5_kZ",
        "outputId": "098448de-fb8c-4096-ccc3-bae00f722359",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "S_3ashud9UKX",
        "outputId": "c089d243-98af-40e4-a9db-4b085bb18cd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/articles.csv'\n",
        "behaviors_train_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/behaviors_train.csv'\n",
        "behaviors_val_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/behaviors_val.csv'\n",
        "history_train_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/history_train.csv'\n",
        "history_val_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/history_val.csv'"
      ],
      "metadata": {
        "id": "3axavoJa8PoS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gUn_UulZ583m"
      },
      "outputs": [],
      "source": [
        "# Load Datasets\n",
        "articles = pd.read_csv(articles_path)\n",
        "behaviors_train = pd.read_csv(behaviors_train_path)\n",
        "behaviors_val = pd.read_csv(behaviors_val_path)\n",
        "history_train = pd.read_csv(history_train_path)\n",
        "history_val = pd.read_csv(history_val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QBuD8vvJ583m"
      },
      "outputs": [],
      "source": [
        "def build_vocab_and_tokenize(titles, max_len=MAX_SENT_LENGTH):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary and tokenizes article titles.\n",
        "\n",
        "    Args:\n",
        "        titles (list of str): List of article titles to tokenize.\n",
        "        max_len (int): Maximum length for tokenized titles (truncation/padding length).\n",
        "\n",
        "    Returns:\n",
        "        tokenized_titles (list of list of int): Tokenized and padded titles.\n",
        "        vocab (dict): A dictionary mapping tokens to unique integer indices.\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "    \"\"\"\n",
        "    vocab = defaultdict(lambda: len(vocab))  # Default dictionary for token ids\n",
        "    vocab[\"<PAD>\"] = 0  # Padding token\n",
        "    vocab[\"<UNK>\"] = 1  # Unknown token\n",
        "\n",
        "    tokenized_titles = []\n",
        "    for title in titles:\n",
        "        tokens = title.lower().split()[:max_len]  # Simple whitespace tokenizer\n",
        "        tokenized = [vocab[token] for token in tokens]\n",
        "        padded = pad_sequence_to_length(tokenized, max_len, pad_value=vocab[\"<PAD>\"])\n",
        "        tokenized_titles.append(padded)\n",
        "\n",
        "    # Freeze the vocabulary after processing to get accurate vocab size\n",
        "    vocab = dict(vocab)  # Convert to a regular dict to freeze it\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    return tokenized_titles, vocab, vocab_size\n",
        "\n",
        "def pad_sequence_to_length(sequence, target_length, pad_value=0):\n",
        "    \"\"\"\n",
        "    Pads or truncates a sequence to the specified target length.\n",
        "\n",
        "    Args:\n",
        "        sequence (list of int): Input sequence to pad or truncate.\n",
        "        target_length (int): Desired length of the sequence.\n",
        "        pad_value (int): Value to use for padding shorter sequences.\n",
        "\n",
        "    Returns:\n",
        "        list of int: Padded or truncated sequence.\n",
        "    \"\"\"\n",
        "    if len(sequence) >= target_length:\n",
        "        return sequence[:target_length]\n",
        "    else:\n",
        "        return sequence + [pad_value] * (target_length - len(sequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tCtmsNac583o"
      },
      "outputs": [],
      "source": [
        "# Tokenize titles and build vocabulary\n",
        "articles[\"tokenized_title\"], vocab, VOCAB_SIZE = build_vocab_and_tokenize(\n",
        "    articles[\"title\"].fillna(\"<UNK>\"),\n",
        "    max_len=MAX_SENT_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "suYEokoT583p"
      },
      "outputs": [],
      "source": [
        "article_to_tokens = {row['article_id']: row['tokenized_title'] for _, row in articles.iterrows()}\n",
        "\n",
        "article_to_idx = {article_id: idx for idx, article_id in enumerate(articles['article_id'].unique(), start=2)}\n",
        "article_to_idx[0] = 0  # Reserved for <PAD>\n",
        "article_to_idx[1] = 1  # Reserved for <UNK>\n",
        "\n",
        "article_embedding_size = len(article_to_idx) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IHyEQRye583t",
        "outputId": "d9c72139-b3f2-4795-e9aa-2492df563b41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Malformed article_ids found: '[9738366 9737535 9738173 ... 9766140 9766140 9766140]'. Skipping.\n",
            "WARNING:root:Malformed article_ids found: '[9737083 9737083 9738216 ... 9770037 9769994 9768321]'. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: Skipped 2 rows out of 1590 (0.13%).\n",
            "train: Saved 2 problematic rows to 'invalid_article_ids_train.csv'.\n",
            "train: Remaining rows after cleaning: 1588\n",
            "val: Skipped 0 rows out of 1562 (0.00%).\n",
            "val: Saved 0 problematic rows to 'invalid_article_ids_val.csv'.\n",
            "val: Remaining rows after cleaning: 1562\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "\n",
        "# Setup logging (if not already configured in your project)\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "def clean_article_ids(article_ids):\n",
        "    \"\"\"\n",
        "    Cleans and parses article IDs from a string representation to a list of integers.\n",
        "\n",
        "    Args:\n",
        "        article_ids (str): String representation of article IDs (e.g., \"[1, 2, 3]\").\n",
        "\n",
        "    Returns:\n",
        "        list of int or None: List of parsed article IDs, or None if input is invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check for invalid placeholders or empty strings\n",
        "        if not article_ids or \"...\" in article_ids:\n",
        "            logging.warning(f\"Malformed article_ids found: '{article_ids}'. Skipping.\")\n",
        "            return None\n",
        "\n",
        "        # Remove brackets and split on spaces or commas, then convert to integers\n",
        "        cleaned_ids = article_ids.strip(\"[]\").replace(\",\", \" \").split()\n",
        "        return list(map(int, cleaned_ids))\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to clean article_ids '{article_ids}' due to error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to process a single dataset (train or val)\n",
        "def clean_and_report_history(history_df, dataset_name=\"dataset\"):\n",
        "    \"\"\"\n",
        "    Cleans article IDs in the user history dataset and reports cleaning statistics.\n",
        "\n",
        "    Args:\n",
        "        history_df (pd.DataFrame): Input dataset with article history to clean.\n",
        "        dataset_name (str): Name of the dataset (for logging and reporting).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Cleaned dataset with invalid rows removed.\n",
        "    \"\"\"\n",
        "    # Clean article IDs\n",
        "    history_df[\"cleaned_article_ids\"] = history_df[\"article_id_fixed\"].apply(clean_article_ids)\n",
        "\n",
        "    # Count skipped rows\n",
        "    skipped_rows = history_df[\"cleaned_article_ids\"].isna().sum()\n",
        "    total_rows = len(history_df)\n",
        "    print(f\"{dataset_name}: Skipped {skipped_rows} rows out of {total_rows} \"\n",
        "          f\"({skipped_rows / total_rows:.2%}).\")\n",
        "\n",
        "    # Save problematic rows\n",
        "    invalid_rows = history_df[history_df[\"cleaned_article_ids\"].isna()]\n",
        "    invalid_rows_file = f\"invalid_article_ids_{dataset_name}.csv\"\n",
        "    invalid_rows.to_csv(invalid_rows_file, index=False)\n",
        "    print(f\"{dataset_name}: Saved {len(invalid_rows)} problematic rows to '{invalid_rows_file}'.\")\n",
        "\n",
        "    # Drop invalid rows and reset index\n",
        "    cleaned_df = history_df.dropna(subset=[\"cleaned_article_ids\"]).reset_index(drop=True)\n",
        "    print(f\"{dataset_name}: Remaining rows after cleaning: {len(cleaned_df)}\")\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "# Clean train and validation datasets\n",
        "history_train_cleaned = clean_and_report_history(history_train, dataset_name=\"train\")\n",
        "history_val_cleaned = clean_and_report_history(history_val, dataset_name=\"val\")\n",
        "\n",
        "\n",
        "def process_cleaned_user_history(cleaned_history_df):\n",
        "    user_histories = defaultdict(list)\n",
        "    for _, row in cleaned_history_df.iterrows():\n",
        "        user_id = row[\"user_id\"]\n",
        "        article_ids = row[\"cleaned_article_ids\"]\n",
        "        user_histories[user_id].extend(article_ids)\n",
        "    return user_histories\n",
        "\n",
        "user_history_train_cleaned = process_cleaned_user_history(history_train_cleaned)\n",
        "user_history_val_cleaned = process_cleaned_user_history(history_val_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HKiNZlCn583u",
        "outputId": "ae0eadbe-48f3-44b8-a449-c56330ebcf1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 18591\n",
            "Sample Tokens: ['<PAD>', '<UNK>', 'ishockey-spiller:', 'jeg', 'troede', 'skulle', 'dø', 'prins', 'harry', 'tvunget']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
        "print(f\"Sample Tokens: {list(vocab.keys())[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "i2GmK3b1583u",
        "outputId": "21d733a0-d099-433f-b02a-24c56249b94d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary saved to /content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data/vocab.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "data_folder = \"/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/Data\"\n",
        "vocab_file = os.path.join(data_folder, \"vocab.json\")\n",
        "\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "\n",
        "# Save the vocabulary to a file\n",
        "with open(vocab_file, \"w\") as f:\n",
        "    json.dump(vocab, f)\n",
        "\n",
        "print(f\"Vocabulary saved to {vocab_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_G9h9iHY583v",
        "outputId": "a8f99cce-6e97-44aa-88df-283be85785b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "582"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(user_history_train_cleaned[13538])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "537JfP7y583v"
      },
      "outputs": [],
      "source": [
        "from random import sample\n",
        "\n",
        "def create_samples(behaviors_df, user_history, npratio=NPRATIO, max_sents=MAX_SENTS, max_sent_length=MAX_SENT_LENGTH):\n",
        "    samples = []\n",
        "    labels = []\n",
        "    for _, row in behaviors_df.iterrows():\n",
        "        user_id = row[\"user_id\"]\n",
        "\n",
        "        clicked_articles = clean_article_ids(row['article_ids_clicked'])\n",
        "        inview_articles = clean_article_ids(row['article_ids_inview'])\n",
        "\n",
        "        if clicked_articles is None or inview_articles is None:\n",
        "            continue\n",
        "\n",
        "        clicked_articles = [article_to_idx.get(article_id, 1) for article_id in clicked_articles]  # Map or <UNK>\n",
        "        inview_articles = [article_to_idx.get(article_id, 1) for article_id in inview_articles]  # Map or <UNK>\n",
        "\n",
        "        # Prepare user history\n",
        "        user_hist = user_history.get(user_id, [])\n",
        "        user_hist = [article_to_idx.get(article_id, 1) for article_id in user_hist[:max_sents]]  # Map or <UNK>\n",
        "        user_hist += [0] * (max_sents - len(user_hist))  # Pad to max_sents\n",
        "\n",
        "        # Add positive samples\n",
        "        for article_idx in clicked_articles:\n",
        "            candidate = [article_idx] + [0] * (max_sent_length - 1)\n",
        "            samples.append((user_hist, candidate))\n",
        "            labels.append(1)\n",
        "\n",
        "        # Add negative samples\n",
        "        negative_articles = list(set(inview_articles) - set(clicked_articles))\n",
        "        for article_idx in negative_articles:\n",
        "            candidate = [article_idx] + [0] * (max_sent_length - 1)\n",
        "            samples.append((user_hist, candidate))\n",
        "            labels.append(0)\n",
        "\n",
        "    return samples, labels\n",
        "\n",
        "train_samples_cleaned, train_labels_cleaned = create_samples(\n",
        "    behaviors_train, user_history_train_cleaned, npratio=NPRATIO\n",
        "    )\n",
        "val_samples_cleaned, val_labels_cleaned = create_samples(\n",
        "    behaviors_val, user_history_val_cleaned, npratio=NPRATIO\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9D7kuxOL583x"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 4: Define PyTorch Dataset\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, samples, labels):\n",
        "        self.samples = samples\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user_history, candidate = self.samples[idx]\n",
        "        return (\n",
        "            torch.tensor(user_history, dtype=torch.long),\n",
        "            torch.tensor(candidate, dtype=torch.long),\n",
        "            torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "        )\n",
        "\n",
        "train_dataset = NewsDataset(train_samples_cleaned, train_labels_cleaned)\n",
        "val_dataset = NewsDataset(val_samples_cleaned, val_labels_cleaned)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzPhZQu8583x",
        "outputId": "fdca7a4a-0cb0-49d8-94ea-62339212ed22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' # Step 5: Define the NRMS Model\\nclass MultiHeadSelfAttention(nn.Module):\\n    def __init__(self, num_heads, head_size):\\n        super().__init__()\\n        self.num_heads = num_heads\\n        self.head_size = head_size\\n        self.output_dim = num_heads * head_size\\n        self.qkv_linear = nn.Linear(EMBEDDING_DIM, self.output_dim * 3)\\n        self.fc_out = nn.Linear(self.output_dim, EMBEDDING_DIM)\\n\\n    def forward(self, x):\\n        if len(x.size()) != 3:\\n            raise ValueError(f\"Expected input to be 3D (batch_size, seq_length, embed_dim), got {x.size()}\")\\n        batch_size, seq_length, embed_dim = x.size()\\n        qkv = self.qkv_linear(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_size)\\n        qkv = qkv.permute(2, 0, 1, 3)\\n        Q, K, V = torch.chunk(qkv, 3, dim=-1)\\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))\\n        attention = F.softmax(scores, dim=-1)\\n        weighted = torch.matmul(attention, V)\\n        weighted = weighted.permute(1, 2, 0, 3).reshape(batch_size, seq_length, self.output_dim)\\n        return self.fc_out(weighted)\\n\\nclass TitleEncoder(nn.Module):\\n    def __init__(self, article_embedding_size, embedding_dim):\\n        super().__init__()\\n        self.embedding = nn.Embedding(article_embedding_size, embedding_dim)\\n        self.dropout = nn.Dropout(DROPOUT_RATE)\\n        self.self_attention = MultiHeadSelfAttention(NUM_HEADS, HEAD_SIZE)\\n        self.dense = nn.Linear(embedding_dim, 1)\\n\\n    def forward(self, x):\\n        x = self.embedding(x)  # Ensure input is embedded\\n        if len(x.size()) != 3:\\n            raise ValueError(f\"Embedding layer output should be 3D, got {x.size()}\")\\n        x = self.dropout(x)\\n        x = self.self_attention(x)\\n        attention_weights = F.softmax(self.dense(x).squeeze(-1), dim=-1) # attention weights\\n        return torch.sum(x * attention_weights.unsqueeze(-1), dim=1) # weighted sum\\n\\nclass NRMS(nn.Module):\\n    def __init__(self, article_embedding_size, embedding_dim, num_classes):\\n        super().__init__()\\n        self.title_encoder = TitleEncoder(article_embedding_size, embedding_dim)\\n\\n    def forward(self, candidates, user_history):\\n        assert len(user_history.size()) == 2, f\"Expected user_history to have 2 dimensions, got {user_history.size()}\"\\n        assert len(candidates.size()) == 2, f\"Expected candidates to have 2 dimensions, got {candidates.size()}\"\\n\\n        user_rep = self.title_encoder(user_history)  # Output: (batch_size, embedding_dim)\\n        candidate_rep = self.title_encoder(candidates)  # Output: (batch_size, embedding_dim)\\n        return torch.matmul(candidate_rep, user_rep.unsqueeze(-1)).squeeze(-1) '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "''' # Step 5: Define the NRMS Model\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.output_dim = num_heads * head_size\n",
        "        self.qkv_linear = nn.Linear(EMBEDDING_DIM, self.output_dim * 3)\n",
        "        self.fc_out = nn.Linear(self.output_dim, EMBEDDING_DIM)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.size()) != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D (batch_size, seq_length, embed_dim), got {x.size()}\")\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        qkv = self.qkv_linear(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_size)\n",
        "        qkv = qkv.permute(2, 0, 1, 3)\n",
        "        Q, K, V = torch.chunk(qkv, 3, dim=-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        weighted = torch.matmul(attention, V)\n",
        "        weighted = weighted.permute(1, 2, 0, 3).reshape(batch_size, seq_length, self.output_dim)\n",
        "        return self.fc_out(weighted)\n",
        "\n",
        "class TitleEncoder(nn.Module):\n",
        "    def __init__(self, article_embedding_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(article_embedding_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "        self.self_attention = MultiHeadSelfAttention(NUM_HEADS, HEAD_SIZE)\n",
        "        self.dense = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Ensure input is embedded\n",
        "        if len(x.size()) != 3:\n",
        "            raise ValueError(f\"Embedding layer output should be 3D, got {x.size()}\")\n",
        "        x = self.dropout(x)\n",
        "        x = self.self_attention(x)\n",
        "        attention_weights = F.softmax(self.dense(x).squeeze(-1), dim=-1) # attention weights\n",
        "        return torch.sum(x * attention_weights.unsqueeze(-1), dim=1) # weighted sum\n",
        "\n",
        "class NRMS(nn.Module):\n",
        "    def __init__(self, article_embedding_size, embedding_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.title_encoder = TitleEncoder(article_embedding_size, embedding_dim)\n",
        "\n",
        "    def forward(self, candidates, user_history):\n",
        "        assert len(user_history.size()) == 2, f\"Expected user_history to have 2 dimensions, got {user_history.size()}\"\n",
        "        assert len(candidates.size()) == 2, f\"Expected candidates to have 2 dimensions, got {candidates.size()}\"\n",
        "\n",
        "        user_rep = self.title_encoder(user_history)  # Output: (batch_size, embedding_dim)\n",
        "        candidate_rep = self.title_encoder(candidates)  # Output: (batch_size, embedding_dim)\n",
        "        return torch.matmul(candidate_rep, user_rep.unsqueeze(-1)).squeeze(-1) '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' # Step 5: Define the NRMS Model\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.output_dim = num_heads * head_size\n",
        "        self.WQ = nn.Linear(EMBEDDING_DIM, self.output_dim)\n",
        "        self.WK = nn.Linear(EMBEDDING_DIM, self.output_dim)\n",
        "        self.WV = nn.Linear(EMBEDDING_DIM, self.output_dim)\n",
        "        self.fc_out = nn.Linear(self.output_dim, EMBEDDING_DIM)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        Q = self.WQ(Q).view(batch_size, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "        K = self.WK(K).view(batch_size, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "        V = self.WV(V).view(batch_size, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Weighted sum\n",
        "        out = torch.matmul(attention, V).permute(0, 2, 1, 3).contiguous()\n",
        "        out = out.view(batch_size, -1, self.output_dim)\n",
        "\n",
        "        return self.fc_out(out)\n",
        "\n",
        "class NewsEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, head_dim, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.self_attention = MultiHeadSelfAttention(num_heads, head_dim)\n",
        "        self.additive_attention = nn.Linear(embedding_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, title):\n",
        "        # Convert word IDs to embeddings\n",
        "        embedded = self.embedding(title).float()  # (batch_size, seq_len, embedding_dim)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Apply word-level self-attention\n",
        "        attended = self.self_attention(embedded, embedded, embedded)\n",
        "\n",
        "        # Additive attention to aggregate word vectors into a single title representation\n",
        "        scores = F.softmax(self.additive_attention(attended), dim=1)  # (batch_size, seq_len, 1)\n",
        "        title_representation = torch.sum(attended * scores, dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        return title_representation\n",
        "\n",
        "class UserEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, head_dim, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadSelfAttention(num_heads, head_dim)\n",
        "        self.additive_attention = nn.Linear(embedding_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, news_representations):\n",
        "        # Apply dropout\n",
        "        news_representations = self.dropout(news_representations)  # (batch_size, num_articles, embedding_dim)\n",
        "\n",
        "        # Apply news-level self-attention\n",
        "        attended = self.self_attention(news_representations, news_representations, news_representations)\n",
        "\n",
        "        # Additive attention to aggregate news vectors into a single user representation\n",
        "        scores = F.softmax(self.additive_attention(attended), dim=1)  # (batch_size, num_articles, 1)\n",
        "        user_representation = torch.sum(attended * scores, dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        return user_representation\n",
        "\n",
        "class NRMS(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, head_dim, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.news_encoder = NewsEncoder(vocab_size, embedding_dim, num_heads, head_dim, dropout_rate)\n",
        "        self.user_encoder = UserEncoder(embedding_dim, num_heads, head_dim, dropout_rate)\n",
        "\n",
        "    def forward(self, candidate_titles, user_histories):\n",
        "        # Encode candidate news articles\n",
        "        candidate_representations = self.news_encoder(candidate_titles)  # (batch_size, embedding_dim)\n",
        "\n",
        "        # Encode user history (sequence of news embeddings)\n",
        "        batch_size, num_articles, title_length = user_histories.size()\n",
        "        user_histories = user_histories.view(batch_size * num_articles, title_length)  # Flatten for processing\n",
        "        user_histories_encoded = self.news_encoder(user_histories)  # Encode all titles at once\n",
        "        user_histories_encoded = user_histories_encoded.view(batch_size, num_articles, -1)  # Reshape back to batch format\n",
        "        user_representations = self.user_encoder(user_histories_encoded)  # (batch_size, embedding_dim)\n",
        "\n",
        "        # Dot product for click prediction\n",
        "        scores = torch.matmul(candidate_representations, user_representations.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        print(f\"Candidate representations shape: {candidate_representations.shape}\")\n",
        "        print(f\"User representations shape: {user_representations.shape}\")\n",
        "        print(f\"Final scores shape: {scores.shape}\")\n",
        "\n",
        "\n",
        "        return scores\n",
        " '''"
      ],
      "metadata": {
        "id": "p_anUwTL0g6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, head_dim):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        assert num_heads * head_dim <= embed_dim, \"num_heads * head_dim must be <= embed_dim\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.attn_output_dim = num_heads * head_dim\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.linear_q = nn.Linear(embed_dim, self.attn_output_dim)\n",
        "        self.linear_k = nn.Linear(embed_dim, self.attn_output_dim)\n",
        "        self.linear_v = nn.Linear(embed_dim, self.attn_output_dim)\n",
        "\n",
        "        # Final linear layer\n",
        "        self.final_linear = nn.Linear(self.attn_output_dim, self.attn_output_dim)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value, attn_mask=None):\n",
        "        # Calculate dot product and scale\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply dropout\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Multiply by value\n",
        "        output = torch.matmul(attn, value)\n",
        "        return output\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask=None):\n",
        "        batch_size, seq_len, embed_dim = query.size()\n",
        "\n",
        "        # Compute Q, K, V and reshape for multi-head attention\n",
        "        query = self.linear_q(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        key = self.linear_k(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        value = self.linear_v(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "        # Apply scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "        # Concatenate heads and apply the final linear layer\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(batch_size, seq_len, self.attn_output_dim)\n",
        "        output = self.final_linear(attn_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Additive attention mechanism based on the paper.\n",
        "        Args:\n",
        "            input_dim: The dimension of input embeddings (h^w_i in the paper).\n",
        "        \"\"\"\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "        self.V_w = nn.Linear(input_dim, input_dim)  # V_w × h^w_i\n",
        "        self.q_w = nn.Parameter(torch.randn(input_dim))  # q_w (query vector)\n",
        "        self.v_w = nn.Parameter(torch.randn(1))  # v_w (bias scalar)\n",
        "\n",
        "    def forward(self, word_representations):\n",
        "        \"\"\"\n",
        "        Compute attention scores and weighted sum.\n",
        "        Args:\n",
        "            word_representations: Tensor of shape (batch_size, seq_len, input_dim)\n",
        "                             representing the sequence of word embeddings.\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, embedding_dim), which is the aggregated\n",
        "            representation of the sequence.\n",
        "        \"\"\"\n",
        "        # Apply V_w (Linear transformation)\n",
        "        Vh = self.V_w(word_representations)  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Compute additive attention scores\n",
        "        scores = torch.matmul(torch.tanh(Vh + self.v_w), self.q_w)  # (batch_size, seq_len)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=1)  # (batch_size, seq_len)\n",
        "\n",
        "        # Weighted sum of word embeddings\n",
        "        aggregated_representation = torch.sum(word_representations * attention_weights.unsqueeze(-1), dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        return aggregated_representation, attention_weights\n",
        "\n",
        "\n",
        "class TitleEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, head_dim, dropout_rate):\n",
        "        super(TitleEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding layer for words\n",
        "        self.self_attention = MultiheadAttention(embed_dim, num_heads, head_dim)  # Multi-head self-attention\n",
        "        self.additive_attention = AdditiveAttention(num_heads * head_dim)  # Additive attention\n",
        "        self.dropout = nn.Dropout(dropout_rate)  # Dropout for regularization\n",
        "\n",
        "    def forward(self, title):\n",
        "        \"\"\"\n",
        "        Encodes a single title into vector representations.\n",
        "\n",
        "        Args:\n",
        "            titles: Tensor of shape (title_length), representing the sequence of word indices for one title.\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (num_heads * head_dim), representing the encoded title.\n",
        "        \"\"\"\n",
        "        title = title.unsqueeze(0)\n",
        "\n",
        "        # Embed the input titles\n",
        "        embedded = self.embedding(title)  # (batch_size, title_length, embed_dim)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Apply multi-head self-attention\n",
        "        attended = self.self_attention(\n",
        "            embedded, embedded, embedded\n",
        "        )  # (batch_size, title_length, num_heads * head_dim)\n",
        "\n",
        "        # Additive attention to aggregate word representations into a single vector per title\n",
        "        aggregated_representation, attention_weights = self.additive_attention(attended)  # (batch_size, num_heads * head_dim)\n",
        "\n",
        "        return aggregated_representation\n",
        "\n",
        "class UserEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, head_dim, dropout_rate):\n",
        "        \"\"\"\n",
        "        User-level multi-head self-attention encoder.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim: Dimension of the input title embeddings.\n",
        "            num_heads: Number of attention heads.\n",
        "            head_dim: Dimension of each attention head.\n",
        "            dropout_rate: Dropout rate for regularization.\n",
        "        \"\"\"\n",
        "        super(UserEncoder, self).__init__()\n",
        "        self.self_attention = MultiheadAttention(embedding_dim, num_heads, head_dim)\n",
        "        self.additive_attention = AdditiveAttention(embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, title_representations):\n",
        "        \"\"\"\n",
        "        Forward pass for user-level multi-head self-attention.\n",
        "\n",
        "        Args:\n",
        "            title_representations: Tensor of shape (batch_size, num_titles, embedding_dim),\n",
        "                                   representing the encoded representations of clicked articles for each user.\n",
        "\n",
        "        Returns:\n",
        "            user_representation: Tensor of shape (batch_size, embedding_dim), the final user embedding.\n",
        "            attention_weights: Tensor of shape (batch_size, num_titles), the attention weights for interpretability.\n",
        "        \"\"\"\n",
        "        # Apply dropout to title representations\n",
        "        title_representations = self.dropout(title_representations)\n",
        "\n",
        "        # Apply multi-head self-attention on the title representations\n",
        "        enhanced_representations = self.self_attention(\n",
        "            title_representations, title_representations, title_representations\n",
        "        )  # (batch_size, num_titles, embedding_dim)\n",
        "\n",
        "        # Use additive attention to compute the final user representation\n",
        "        user_representation, attention_weights = self.additive_attention(enhanced_representations)\n",
        "\n",
        "        return user_representation, attention_weights\n",
        "\n",
        "class NRMSUserPipeline(nn.Module):\n",
        "    def __init__(self, title_encoder, user_encoder):\n",
        "        \"\"\"\n",
        "        NRMS pipeline combining TitleEncoder and UserEncoder.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Size of the vocabulary.\n",
        "            embedding_dim: Dimension of the input/output embeddings.\n",
        "            num_heads: Number of attention heads for the multi-head attention.\n",
        "            head_dim: Dimension of each attention head.\n",
        "            dropout_rate: Dropout rate for regularization.\n",
        "        \"\"\"\n",
        "        super(NRMSUserPipeline, self).__init__()\n",
        "        self.title_encoder = title_encoder # Title Encoder instance\n",
        "        self.user_encoder = user_encoder # User Encoder instance\n",
        "\n",
        "    def forward(self, user_titles):\n",
        "        \"\"\"\n",
        "        Processes all titles for a user and computes a user embedding.\n",
        "\n",
        "        Args:\n",
        "            user_titles: Tensor of shape (num_titles, title_length),\n",
        "                         representing the word indices for all titles of one user.\n",
        "\n",
        "        Returns:\n",
        "            user_representation: Tensor of shape (embedding_dim,), the final user embedding.\n",
        "        \"\"\"\n",
        "        # Encode each title independently\n",
        "        title_representations = torch.stack(\n",
        "            [self.title_encoder(title) for title in user_titles]\n",
        "        )  # Shape: (num_titles, num_heads * head_dim)\n",
        "\n",
        "        # Pass encoded titles through the UserEncoder\n",
        "        user_representation, _ = self.user_encoder(title_representations.unsqueeze(0))  # Add batch dim\n",
        "        return user_representation.squeeze(0)  # Remove batch dim\n"
      ],
      "metadata": {
        "id": "4g19Cd-aIAXB"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Mock data: Identical titles\n",
        "title = torch.tensor([1, 2, 3, 4, 5, 6])  # Title with 6 words\n",
        "identical_titles = [title, title]  # Two identical titles\n",
        "\n",
        "# Define TitleEncoder\n",
        "vocab_size = 100\n",
        "embedding_dim = 300\n",
        "num_heads = 16\n",
        "head_dim = 16\n",
        "dropout_rate = 0.1\n",
        "\n",
        "torch.manual_seed(42)  # Fix seed for reproducibility\n",
        "title_encoder = TitleEncoder(vocab_size, embedding_dim, num_heads, head_dim, dropout_rate)\n",
        "\n",
        "# Forward pass\n",
        "outputs = [title_encoder(t) for t in identical_titles]\n",
        "\n",
        "print(\"Output 1:\", outputs[0])\n",
        "print(\"Output 2:\", outputs[1])\n",
        "print(\"Are outputs identical?\", torch.allclose(outputs[0], outputs[1], atol=1e-6))"
      ],
      "metadata": {
        "id": "V9pm-svWm7Vd",
        "outputId": "c4b33d91-8274-4d81-d5c8-2aa147a95ebd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-5ca7c529eaa3>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtitle_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midentical_titles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output 1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-5ca7c529eaa3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtitle_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midentical_titles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output 1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-74a6429147e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, title)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Additive attention to aggregate word representations into a single vector per title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0maggregated_representation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditive_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattended\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, num_heads * head_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maggregated_representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-74a6429147e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_representations)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Apply softmax to get attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Weighted sum of word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mock data: Batch of 3 titles, two are identical\n",
        "mock_titles = torch.tensor([[1, 2, 3, 4, 5, 6],  # Title 1\n",
        "                            [1, 2, 3, 4, 5, 6],  # Title 2 (identical to Title 1)\n",
        "                            [1, 2, 3, 4, 5, 99]])  # Title 3 (one word different)\n",
        "\n",
        "torch.manual_seed(42)  # Fix random seed for reproducibility\n",
        "\n",
        "# Forward pass\n",
        "embedded_titles = title_encoder(mock_titles)\n",
        "\n",
        "# Check outputs\n",
        "print(\"Embedded Titles:\\n\", embedded_titles)"
      ],
      "metadata": {
        "id": "00fF_3PUrzAC",
        "outputId": "62fab9a1-5a66-4f9e-a7e2-2c76c1c84183",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded Titles:\n",
            " tensor([[ 1.5932e-01,  5.1330e-02,  7.3199e-02, -3.8364e-02, -2.0147e-01,\n",
            "          8.8900e-02, -4.5365e-02, -7.2358e-02,  2.4386e-01,  1.2886e-01,\n",
            "         -2.4969e-02, -4.7431e-02, -4.0100e-02,  1.4091e-01, -1.6495e-01,\n",
            "          5.3424e-02,  2.9967e-02, -1.2787e-04, -1.2687e-02, -1.3000e-01,\n",
            "         -6.8437e-02,  8.3400e-02, -2.1833e-01,  3.2489e-02, -1.0281e-01,\n",
            "          1.9471e-02,  1.1285e-01,  2.0595e-02, -2.1571e-01,  1.2007e-02,\n",
            "         -1.0269e-02, -3.3718e-02,  1.9680e-01, -1.7286e-01,  1.0562e-01,\n",
            "          4.4729e-02,  5.2597e-02, -8.1423e-02, -1.8208e-02, -1.1580e-01,\n",
            "         -5.7169e-03,  1.7401e-01, -1.0941e-01, -2.6407e-01,  2.1258e-03,\n",
            "         -1.4041e-01, -9.3649e-02,  4.5622e-02,  1.9385e-01,  4.6521e-02,\n",
            "          2.2578e-01,  2.5086e-01,  1.0459e-01, -7.4642e-02,  9.5308e-02,\n",
            "         -1.0554e-02,  3.1662e-01,  5.8187e-02, -2.6800e-02, -1.4185e-01,\n",
            "          2.0723e-01,  1.6029e-01,  9.3468e-02,  5.7265e-02, -4.5021e-02,\n",
            "         -5.1340e-02,  8.5935e-03, -1.3661e-02, -1.7203e-01,  1.9330e-01,\n",
            "         -5.4368e-02,  1.7053e-01, -4.4579e-02, -2.0984e-02,  8.8960e-02,\n",
            "         -1.6212e-01,  1.8068e-01,  1.8797e-01, -2.9098e-01, -1.2510e-01,\n",
            "         -1.2261e-01, -1.7224e-01,  9.4414e-02,  1.6052e-01, -2.9689e-01,\n",
            "         -1.7339e-01,  1.1692e-01, -1.1589e-01,  1.3970e-02,  2.1252e-01,\n",
            "         -2.1127e-01,  6.4447e-02,  2.0668e-02, -1.1454e-01, -1.1332e-01,\n",
            "          2.2030e-03, -6.2773e-03, -3.7770e-01, -5.2276e-02, -1.7995e-02,\n",
            "         -6.6334e-02, -3.7764e-03,  4.6486e-02,  5.0848e-02,  9.1837e-02,\n",
            "          1.1867e-01, -2.2471e-01,  7.6673e-02,  1.8505e-03, -4.1742e-02,\n",
            "         -3.7745e-02, -1.3580e-01, -1.9665e-02, -1.1771e-01, -4.3674e-02,\n",
            "         -2.2759e-01, -1.0058e-01, -5.4952e-02,  3.3414e-02, -3.0612e-01,\n",
            "         -2.1726e-01,  1.5653e-01,  6.2931e-02, -2.8543e-01, -3.1321e-02,\n",
            "         -1.6528e-01, -3.8148e-02, -4.5497e-02, -3.3514e-03,  1.0731e-01,\n",
            "         -7.7807e-02,  1.3458e-02,  2.0548e-01, -4.4385e-02, -1.2329e-01,\n",
            "          9.2608e-02,  4.0871e-02,  6.6469e-02,  7.4643e-02,  2.1308e-03,\n",
            "         -1.1814e-01,  4.5080e-01,  1.7010e-01,  5.0830e-02, -1.7755e-02,\n",
            "         -1.0322e-01,  9.1686e-02, -1.2049e-01,  3.2444e-02,  4.7558e-02,\n",
            "          9.0778e-02,  5.4050e-03, -1.3142e-01, -7.5810e-02, -1.0525e-01,\n",
            "         -5.9591e-03,  7.9754e-02,  1.3437e-01,  1.2254e-01, -5.3417e-02,\n",
            "          2.5137e-01, -6.6345e-02,  2.1991e-01,  2.8515e-02, -2.2808e-02,\n",
            "          7.1834e-02,  8.0905e-02,  4.4013e-02, -1.7072e-01, -2.6333e-01,\n",
            "          2.4352e-01, -6.0356e-02, -1.6074e-01,  1.6935e-01,  1.3330e-01,\n",
            "         -1.0866e-01,  1.9361e-01,  2.3089e-01,  6.4062e-03,  1.8395e-02,\n",
            "         -1.4643e-01,  6.3824e-02, -1.7588e-01,  4.7064e-02,  2.2577e-01,\n",
            "          1.3111e-01,  8.3048e-02, -8.3189e-02,  1.0579e-02,  2.2523e-01,\n",
            "         -2.6268e-01, -2.4482e-02, -2.1482e-02, -4.7754e-02, -2.5321e-01,\n",
            "         -8.4213e-02, -5.5672e-02,  9.6644e-02, -8.7730e-03,  8.3487e-02,\n",
            "         -7.5479e-02,  7.1300e-02, -1.0516e-02,  3.6678e-02, -1.0035e-01,\n",
            "         -2.9479e-01, -5.3072e-02,  4.8458e-02,  1.3598e-01, -1.6764e-01,\n",
            "         -1.5189e-02,  1.9641e-02,  1.7134e-01, -1.6097e-01, -9.6273e-03,\n",
            "          7.7424e-02, -9.1049e-02,  1.7360e-01,  1.3778e-01, -8.7927e-02,\n",
            "         -1.3304e-02, -3.4352e-01, -1.3862e-01,  9.3757e-03, -1.0227e-02,\n",
            "         -9.8463e-02, -1.0656e-02,  1.4053e-01,  4.3378e-02, -1.0996e-01,\n",
            "         -5.6699e-02, -8.5610e-02,  1.1901e-01, -6.3349e-03, -2.9731e-02,\n",
            "          9.2224e-02,  6.5061e-02, -5.6273e-02, -2.1997e-01,  5.6078e-02,\n",
            "         -6.6840e-04,  1.3468e-01, -5.7505e-02,  1.1844e-01,  7.2234e-02,\n",
            "          9.5789e-02,  3.6756e-02, -1.1518e-01, -1.0341e-01,  1.0738e-01,\n",
            "          4.5641e-02, -4.0453e-01,  8.3745e-02, -1.7253e-01, -5.6595e-02,\n",
            "         -1.5037e-01],\n",
            "        [ 1.4535e-01,  7.4874e-02,  3.0306e-02, -5.3492e-02, -1.9470e-01,\n",
            "          7.9550e-02, -1.0433e-01, -4.9711e-02,  2.5920e-01,  1.4445e-01,\n",
            "         -5.9136e-02, -4.2917e-02, -7.6892e-02,  1.2543e-01, -1.5815e-01,\n",
            "          5.3524e-02,  7.6100e-02,  5.0724e-04,  4.0253e-02, -1.2963e-01,\n",
            "         -7.9377e-02,  8.1893e-02, -2.1897e-01,  2.0496e-02, -7.6572e-02,\n",
            "          4.1169e-02,  9.7133e-02, -3.2857e-03, -2.3662e-01,  2.7358e-02,\n",
            "          1.2097e-02, -4.1936e-02,  2.2100e-01, -1.6564e-01,  1.1139e-01,\n",
            "          5.4151e-02, -9.2244e-03, -8.8822e-02,  7.9378e-04, -1.1909e-01,\n",
            "          4.7388e-02,  2.1679e-01, -1.2365e-01, -2.5385e-01, -1.6999e-03,\n",
            "         -1.0609e-01, -1.2598e-01,  4.3805e-02,  2.3406e-01,  9.4183e-02,\n",
            "          2.1649e-01,  2.4807e-01,  1.5376e-01, -6.1692e-02,  1.3117e-01,\n",
            "         -3.2731e-03,  2.9617e-01,  5.0786e-02,  1.7956e-03, -1.5107e-01,\n",
            "          2.2768e-01,  1.2042e-01,  9.7360e-02,  2.9199e-02, -2.2995e-02,\n",
            "         -1.9900e-02,  1.2424e-01,  4.0772e-02, -1.4519e-01,  2.1100e-01,\n",
            "         -5.4837e-02,  1.2623e-01, -4.7743e-02,  1.7842e-02,  7.9514e-02,\n",
            "         -1.3030e-01,  1.5555e-01,  2.0305e-01, -3.1254e-01, -1.1256e-01,\n",
            "         -1.4515e-01, -2.2925e-01,  8.9321e-02,  8.0872e-02, -3.2651e-01,\n",
            "         -1.6153e-01,  1.2611e-01, -6.7117e-02,  6.9617e-02,  2.0534e-01,\n",
            "         -2.1899e-01,  4.6852e-02, -2.3744e-03, -1.0226e-01, -8.8324e-02,\n",
            "          1.8101e-03, -4.5412e-02, -3.8222e-01, -9.3991e-02,  2.3138e-02,\n",
            "         -7.8676e-02, -7.0547e-02,  2.2606e-02,  6.0477e-02,  9.8278e-02,\n",
            "          1.5967e-01, -2.9415e-01,  8.9095e-02,  3.6868e-02, -2.3776e-02,\n",
            "         -6.2954e-03, -1.4693e-01, -2.1755e-02, -9.9127e-02, -3.3277e-02,\n",
            "         -2.0698e-01, -8.3834e-02, -7.8778e-02,  2.5294e-02, -2.6918e-01,\n",
            "         -2.1899e-01,  1.4651e-01,  4.7699e-02, -2.5002e-01, -1.4975e-02,\n",
            "         -1.2362e-01, -4.5925e-02, -6.3906e-02,  1.2947e-03,  1.2044e-01,\n",
            "         -1.0948e-01,  2.3028e-02,  1.7943e-01, -3.5932e-02, -1.6024e-01,\n",
            "          9.4891e-02, -1.7190e-03,  8.1130e-02,  8.9630e-02, -2.7156e-02,\n",
            "         -8.1865e-02,  4.5685e-01,  1.9955e-01,  6.1956e-02, -3.4596e-02,\n",
            "         -1.0289e-01,  6.9783e-02, -1.1939e-01,  4.2735e-02,  2.5823e-02,\n",
            "          1.0957e-01,  1.2386e-02, -1.3626e-01, -3.4359e-02, -1.4069e-01,\n",
            "         -3.0391e-02,  1.0455e-01,  1.3935e-01,  1.6306e-01, -4.7068e-02,\n",
            "          2.5128e-01, -4.2636e-02,  1.8342e-01, -2.9751e-02, -6.0803e-02,\n",
            "          4.2020e-02,  2.9305e-02,  6.2025e-02, -1.9804e-01, -2.3481e-01,\n",
            "          2.3982e-01, -4.8717e-02, -2.3385e-01,  1.4961e-01,  1.5580e-01,\n",
            "         -1.2702e-01,  2.2930e-01,  1.5639e-01,  2.5174e-02,  2.7735e-02,\n",
            "         -1.4953e-01,  2.2205e-02, -1.2891e-01,  4.6928e-02,  2.4095e-01,\n",
            "          1.2171e-01,  1.2192e-01, -5.7431e-02,  2.7136e-02,  1.8106e-01,\n",
            "         -2.7830e-01, -3.2912e-02, -4.6655e-02, -2.8651e-02, -2.2191e-01,\n",
            "         -9.5131e-02, -4.6408e-02,  6.9758e-02,  9.5124e-03,  9.0574e-02,\n",
            "         -1.0681e-01,  5.2899e-02, -3.3410e-02,  5.7520e-03, -1.4013e-01,\n",
            "         -2.8628e-01, -6.0374e-02,  5.5030e-02,  1.2279e-01, -1.8638e-01,\n",
            "         -5.1125e-02,  6.9101e-02,  1.8116e-01, -1.7095e-01, -2.8935e-02,\n",
            "          9.2622e-02, -4.8203e-02,  1.8746e-01,  1.3489e-01, -1.0714e-01,\n",
            "         -3.8056e-02, -3.5572e-01, -1.7779e-01,  2.4539e-02, -1.3349e-02,\n",
            "         -1.0070e-01, -3.9538e-02,  1.3454e-01,  7.6295e-02, -1.2065e-01,\n",
            "         -7.8944e-02, -6.3632e-02,  1.4076e-01,  3.7383e-03, -5.5146e-03,\n",
            "          8.0759e-02,  4.7970e-02, -5.9434e-02, -2.0220e-01,  5.6101e-02,\n",
            "          2.6869e-02,  1.2630e-01, -2.6385e-02,  7.5630e-02,  1.0549e-01,\n",
            "          7.6786e-02,  7.4020e-02, -8.3734e-02, -1.4283e-01,  1.3618e-01,\n",
            "          1.8389e-02, -4.1078e-01,  1.1907e-01, -1.5140e-01, -7.1577e-02,\n",
            "         -1.2455e-01],\n",
            "        [ 3.3054e-02,  1.8108e-02,  1.3085e-01, -6.2250e-02, -2.0906e-01,\n",
            "         -1.8891e-02,  2.6218e-02, -1.1594e-01,  2.6318e-01,  2.7732e-01,\n",
            "         -1.3340e-01,  2.8024e-02, -2.4448e-02,  2.1236e-02, -7.6271e-02,\n",
            "         -1.7382e-02,  7.7323e-02,  1.5339e-01,  1.3268e-01, -7.5659e-02,\n",
            "         -4.0128e-02,  1.5157e-01, -2.1848e-01, -2.1146e-01, -8.2157e-02,\n",
            "         -1.0735e-01, -6.3803e-02,  1.3323e-02, -2.0358e-01, -8.0375e-02,\n",
            "         -3.9022e-02,  7.1361e-02,  1.9589e-01, -1.3977e-01,  3.1120e-02,\n",
            "         -9.9740e-03,  1.9817e-01, -1.5297e-01,  5.7882e-02, -1.9611e-01,\n",
            "         -8.5042e-02,  3.8026e-01, -1.3120e-01, -1.1934e-01,  6.6679e-02,\n",
            "         -7.3102e-02,  7.3756e-02,  1.1179e-01,  2.1046e-01,  1.1715e-02,\n",
            "          2.9093e-01,  1.6750e-01,  3.3471e-02, -9.6021e-02,  6.2910e-02,\n",
            "         -2.9975e-03,  2.8889e-01,  6.2807e-02,  8.9195e-02, -1.7054e-01,\n",
            "          2.4937e-01,  1.5484e-01, -8.6566e-02,  8.4053e-02,  2.1628e-02,\n",
            "         -3.1264e-02,  4.5947e-02, -2.5058e-02, -1.1368e-01,  2.8594e-01,\n",
            "         -4.2888e-02,  1.2868e-01, -2.7168e-02, -7.8683e-02,  2.7073e-02,\n",
            "         -1.0341e-01,  1.7732e-01,  1.9775e-01, -2.6478e-01, -1.8000e-01,\n",
            "         -1.1515e-01, -2.0887e-01,  8.2451e-02,  1.0174e-01, -2.1928e-01,\n",
            "         -1.0605e-01, -2.6726e-02, -1.8523e-02,  1.2770e-01,  3.2529e-01,\n",
            "         -1.7950e-01,  8.0796e-02,  4.6252e-02, -8.0394e-02, -1.2638e-01,\n",
            "          1.2035e-02,  2.8570e-02, -3.5732e-01,  4.8501e-02,  4.1845e-02,\n",
            "         -1.5627e-01, -6.6104e-02,  1.9776e-02,  5.6090e-02,  1.4065e-01,\n",
            "          7.3547e-02, -1.9432e-01,  1.8165e-01,  1.7040e-01, -7.7909e-02,\n",
            "         -3.0936e-02, -1.5828e-01,  1.0900e-01, -1.8142e-01,  1.6006e-01,\n",
            "         -2.1595e-01, -2.2367e-01,  4.7844e-02,  7.7553e-02, -3.0784e-01,\n",
            "         -3.1056e-02,  1.3558e-01, -1.5542e-03, -1.4794e-01,  1.9575e-02,\n",
            "         -3.1747e-01, -7.6897e-02, -1.8042e-02,  2.3751e-02,  1.6163e-01,\n",
            "         -7.0455e-02, -6.7584e-02,  1.2826e-01,  9.7133e-02, -1.1490e-01,\n",
            "          2.7291e-02,  1.3694e-02, -1.1450e-02,  8.9767e-03, -7.9220e-04,\n",
            "         -5.0788e-02,  3.2237e-01,  1.2706e-01,  1.0195e-01, -4.8674e-02,\n",
            "         -2.7800e-02,  1.3329e-01, -6.6776e-02,  9.2727e-03,  5.0837e-02,\n",
            "          1.4785e-01, -5.6760e-02, -1.6267e-01, -4.8953e-02, -2.2314e-01,\n",
            "         -1.4670e-01,  9.9310e-02,  1.8309e-01,  1.2936e-01, -8.0737e-02,\n",
            "          3.1537e-01, -3.2866e-02,  2.6025e-01, -6.0204e-02, -2.4695e-03,\n",
            "          5.0805e-02, -6.3294e-02,  9.4314e-02, -1.2444e-01, -3.2552e-01,\n",
            "          2.5736e-01, -1.9567e-02, -1.2269e-01,  1.9026e-01,  1.3988e-01,\n",
            "         -3.7594e-02,  1.1114e-01,  2.7795e-01,  5.9399e-02, -9.6229e-02,\n",
            "         -6.0837e-02,  1.7714e-01, -1.7619e-01,  4.9953e-02,  3.5068e-01,\n",
            "          2.9666e-02,  1.7768e-01,  9.1105e-02,  2.5492e-02,  3.2199e-01,\n",
            "         -2.5237e-01,  1.8129e-02, -2.1410e-02, -5.0944e-02, -8.9826e-02,\n",
            "          1.2463e-02,  4.6715e-02,  8.2446e-02,  2.3635e-02,  5.2849e-02,\n",
            "         -4.9806e-02,  1.9564e-01, -2.6693e-03,  4.9875e-02, -1.5760e-01,\n",
            "         -2.8351e-01, -3.0394e-02, -2.5038e-03,  1.0691e-01, -1.7348e-01,\n",
            "         -9.2568e-02,  2.0692e-03,  7.9582e-02, -1.1887e-01, -8.5815e-02,\n",
            "          1.1369e-01, -1.8893e-01,  1.6909e-01,  1.6246e-02, -6.2841e-02,\n",
            "          1.1081e-01, -3.3236e-01, -6.1872e-02, -9.5209e-02,  5.6528e-02,\n",
            "         -3.5514e-02, -1.7057e-01,  1.9168e-01,  1.1474e-01, -2.7752e-01,\n",
            "         -3.1359e-02, -7.8595e-03,  1.4709e-02,  3.9854e-02, -4.8853e-02,\n",
            "          4.8165e-02,  4.8557e-02,  6.8320e-03, -2.6179e-01, -5.8309e-02,\n",
            "         -4.4463e-02,  3.3007e-02, -1.8384e-01,  1.4601e-01,  1.2003e-01,\n",
            "          2.9524e-01,  1.3027e-01, -9.4573e-02, -1.1508e-01,  1.7875e-01,\n",
            "          4.1120e-02, -2.6323e-01,  3.7287e-02, -2.2823e-01, -2.4939e-02,\n",
            "         -5.1657e-02]], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Mock data: 4 users, each with 5 clicked articles, each article having 10 words\n",
        "mock_user_histories = torch.randint(0, 100, (5, 10))  # (batch_size=4, num_titles=5, title_length=10)\n",
        "\n",
        "# Define pipeline\n",
        "vocab_size = 100  # Mock vocabulary size\n",
        "embedding_dim = 300\n",
        "num_heads = 16\n",
        "head_dim = 16\n",
        "dropout_rate = 0.1\n",
        "\n",
        "nrms_pipeline = NRMSUserPipeline(vocab_size, embedding_dim, num_heads, head_dim, dropout_rate)\n",
        "\n",
        "# Forward pass\n",
        "user_representation, attention_weights = nrms_pipeline(mock_user_histories)\n",
        "\n",
        "print(\"User Representation Shape:\", user_representation.shape)  # Expected: (batch_size=4, embedding_dim=300)\n",
        "print(\"Attention Weights Shape:\", attention_weights.shape)      # Expected: (batch_size=4, num_titles=5)"
      ],
      "metadata": {
        "id": "n0wkSOuFU-YH",
        "outputId": "b7cc64cb-3fee-4b4a-f23f-be7b54ce7039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (50x256 and 300x300)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-c0b35b2c0341>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0muser_representation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnrms_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_user_histories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User Representation Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_representation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Expected: (batch_size=4, embedding_dim=300)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-f8695c26ba5a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, user_histories)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Pass through TitleEncoder to get title representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mtitle_representations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_histories\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size * num_titles, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# Reshape back to original batch structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-f8695c26ba5a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, titles, key_padding_mask)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Apply multi-head self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         attended = self.self_attention(\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         )  # (batch_size, title_length, embed_dim)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-f8695c26ba5a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# combine and redistribute with a final layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (50x256 and 300x300)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mock_user_histories.view(5, 10)"
      ],
      "metadata": {
        "id": "qSzgYgmXkWYZ",
        "outputId": "e55d9e01-b659-4fac-bf18-4ac4494f138a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[66, 61, 57,  7, 25, 96, 61, 21, 60, 97],\n",
              "        [53, 40, 14, 50, 84, 63, 43, 29, 93, 78],\n",
              "        [45, 61, 64, 45, 21, 17, 91, 34, 20, 97],\n",
              "        [55, 44, 87, 37, 88, 29,  2, 75,  4, 33],\n",
              "        [24, 83, 41, 65, 80, 19, 28, 87, 63, 64]])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example hyperparameters\n",
        "vocab_size = 100\n",
        "embedding_dim = 300\n",
        "num_heads = 4\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Instantiate the TitleEncoder\n",
        "title_encoder = TitleEncoder(vocab_size, embedding_dim, num_heads, dropout_rate)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "title_encoder = title_encoder.to(device)\n",
        "\n",
        "# Move the mock input to the same device\n",
        "mock_titles = mock_titles.to(device)"
      ],
      "metadata": {
        "id": "fjKXYUMEVIXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_8BpxjG583x"
      },
      "outputs": [],
      "source": [
        "''' from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Loop\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            user_histories, candidates, labels = [x.to(device) for x in batch]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(candidates, user_histories)  # Pass inputs to model\n",
        "\n",
        "            # Compute loss and update weights\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Validation Loop\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                user_histories, candidates, labels = [x.to(device) for x in batch]\n",
        "                outputs = model(candidates, user_histories)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "                # Predictions and metrics\n",
        "                _, preds = torch.max(outputs, dim=1)  # Get predicted class\n",
        "                all_preds.extend(preds.cpu().tolist())  # Move predictions to CPU before converting to list\n",
        "                all_labels.extend(labels.cpu().tolist())  # Move labels to CPU before converting to list\n",
        "\n",
        "        # Metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        # Logging\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {total_train_loss:.4f}\")\n",
        "        print(f\"  Val Loss: {total_val_loss:.4f}\")\n",
        "        print(f\"  Val Accuracy: {accuracy:.4f}\")\n",
        " '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGdVTz5l583x"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            user_histories, candidates, labels = [x.to(device) for x in batch]\n",
        "\n",
        "            print(f\"User histories shape before model: {user_histories.shape}\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(candidates, user_histories)  # Pass inputs to model\n",
        "\n",
        "            print(f\"Candidates shape: {candidates.shape}\")\n",
        "            print(f\"User histories shape: {user_histories.shape}\")\n",
        "\n",
        "\n",
        "            print(f\"Outputs shape: {outputs.shape}\")\n",
        "            print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "\n",
        "            # Compute loss and update weights\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_train_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO0Ys2P-583y",
        "outputId": "9e145e2f-e384-43b9-d5ce-ce22c2b33601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User histories shape before model: torch.Size([64, 50])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-41a59e87adf6>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-7f480c96f845>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_histories\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass inputs to model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Candidates shape: {candidates.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-313dae6b712b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, candidate_titles, user_histories)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Encode user history (sequence of news embeddings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_histories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0muser_histories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_histories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_length\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten for processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0muser_histories_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnews_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_histories\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Encode all titles at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = NRMS(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    num_heads=NUM_HEADS,\n",
        "    head_dim=HEAD_SIZE,\n",
        "    dropout_rate=DROPOUT_RATE\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_5yu6Kn583y"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/DTU/Kandidat/Semester 9/Deep learning/Final project/nrms_model.pth'\n",
        "torch.save(model.state_dict(), model_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
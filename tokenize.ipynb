{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# define function\n",
    "\n",
    "def generate_user_tokenized_titles(history_df, articles_df):\n",
    "    \"\"\"\n",
    "    Generates a list of tokenized titles for each user based on their reading history.\n",
    "\n",
    "    Args:\n",
    "        history_df (pd.DataFrame): DataFrame containing user reading history with columns:\n",
    "                                   'user_id' and 'article_id_fixed'.\n",
    "        articles_df (pd.DataFrame): DataFrame containing article information with columns:\n",
    "                                    'article_id' and 'title'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokenized article titles as tensors for each user.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Create a vocabulary with special tokens\n",
    "    vocabulary = defaultdict(lambda: len(vocabulary))  # Default dictionary for token ids\n",
    "    vocabulary[\"<PAD>\"] = 0  # Padding token\n",
    "    vocabulary[\"<UNK>\"] = 1  # Unknown token\n",
    "\n",
    "    # Populate the vocabulary with words from titles\n",
    "    for title in articles_df['title']:\n",
    "        tokens = word_tokenize(title.lower())\n",
    "        for token in tokens:\n",
    "            _ = vocabulary[token]  # Assign an index if the token is not already in the vocabulary\n",
    "\n",
    "    # Convert vocabulary to a standard dictionary for better control\n",
    "    vocabulary = dict(vocabulary)\n",
    "\n",
    "    # Step 2: Tokenize each title into a list of indices based on the vocabulary\n",
    "    tokenized_titles = {}\n",
    "    for _, row in articles_df.iterrows():\n",
    "        tokens = word_tokenize(row['title'].lower())\n",
    "        tokenized_titles[row['article_id']] = torch.tensor(\n",
    "            [vocabulary.get(token, vocabulary[\"<UNK>\"]) for token in tokens]\n",
    "        )\n",
    "\n",
    "    # Step 3: Create a list of tokenized titles for each user\n",
    "    user_articles = defaultdict(list)\n",
    "    for _, row in history_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        article_id = row['article_id_fixed']\n",
    "        if article_id in tokenized_titles:\n",
    "            user_articles[user_id].append(tokenized_titles[article_id])\n",
    "\n",
    "    # Step 4: Pad tokenized titles for each user\n",
    "    user_tokenized_titles = {\n",
    "        user: pad_sequence(articles, batch_first=True, padding_value=vocabulary[\"<PAD>\"])\n",
    "        for user, articles in user_articles.items()\n",
    "    }\n",
    "\n",
    "    # Convert user_tokenized_titles from a dictionary to a list\n",
    "    user_tokenized_titles_list = list(user_tokenized_titles.values())\n",
    "\n",
    "    return user_tokenized_titles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2, 3, 4, 5],\n",
      "        [6, 7, 8, 0]]), tensor([[ 9, 10, 11, 12]]), tensor([[13, 11, 14]])]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "# Sample DataFrames\n",
    "history_train = pd.DataFrame({\n",
    "    \"user_id\": [1, 1, 2, 3],\n",
    "    \"article_id_fixed\": [101, 102, 103, 104]\n",
    "})\n",
    "\n",
    "articles = pd.DataFrame({\n",
    "    \"article_id\": [101, 102, 103, 104],\n",
    "    \"title\": [\"I want to sleep\", \"Deep Learning Basics\", \"AI is the future\", \"Understanding the Transformers\"]\n",
    "})\n",
    "\n",
    "# Call the function\n",
    "user_tokenized_titles_list = generate_user_tokenized_titles(history_train, articles)\n",
    "\n",
    "# Display the result\n",
    "print(user_tokenized_titles_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

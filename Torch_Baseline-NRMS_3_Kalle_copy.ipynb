{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constants\n",
    "MAX_SENT_LENGTH = 30\n",
    "MAX_SENTS = 50\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_HEADS = 16\n",
    "HEAD_SIZE = 16\n",
    "DROPOUT_RATE = 0.2\n",
    "NPRATIO = 4  # Number of negative samples per positive\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "articles = pd.read_csv(\"Data/articles.csv\")\n",
    "behaviors_train = pd.read_csv(\"Data/behaviors_train.csv\")\n",
    "behaviors_val = pd.read_csv(\"Data/behaviors_val.csv\")\n",
    "history_train = pd.read_csv(\"Data/history_train.csv\")\n",
    "history_val = pd.read_csv(\"Data/history_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_and_tokenize(titles, max_len=MAX_SENT_LENGTH):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary and tokenizes article titles.\n",
    "    \n",
    "    Args:\n",
    "        titles (list of str): List of article titles to tokenize.\n",
    "        max_len (int): Maximum length for tokenized titles (truncation/padding length).\n",
    "    \n",
    "    Returns:\n",
    "        tokenized_titles (list of list of int): Tokenized and padded titles.\n",
    "        vocab (dict): A dictionary mapping tokens to unique integer indices.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(lambda: len(vocab))  # Default dictionary for token ids\n",
    "    vocab[\"<PAD>\"] = 0  # Padding token\n",
    "    vocab[\"<UNK>\"] = 1  # Unknown token\n",
    "\n",
    "    tokenized_titles = []\n",
    "    for title in titles:\n",
    "        tokens = title.lower().split()[:max_len]  # Simple whitespace tokenizer\n",
    "        tokenized = [vocab[token] for token in tokens]\n",
    "        padded = pad_sequence_to_length(tokenized, max_len, pad_value=vocab[\"<PAD>\"])\n",
    "        tokenized_titles.append(padded)\n",
    "\n",
    "    # Freeze the vocabulary after processing to get accurate vocab size\n",
    "    vocab = dict(vocab)  # Convert to a regular dict to freeze it\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    return tokenized_titles, vocab, vocab_size\n",
    "\n",
    "def pad_sequence_to_length(sequence, target_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads or truncates a sequence to the specified target length.\n",
    "    \n",
    "    Args:\n",
    "        sequence (list of int): Input sequence to pad or truncate.\n",
    "        target_length (int): Desired length of the sequence.\n",
    "        pad_value (int): Value to use for padding shorter sequences.\n",
    "    \n",
    "    Returns:\n",
    "        list of int: Padded or truncated sequence.\n",
    "    \"\"\"\n",
    "    if len(sequence) >= target_length:\n",
    "        return sequence[:target_length]\n",
    "    else:\n",
    "        return sequence + [pad_value] * (target_length - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize titles and build vocabulary\n",
    "articles[\"tokenized_title\"], vocab, VOCAB_SIZE = build_vocab_and_tokenize(\n",
    "    articles[\"title\"].fillna(\"<UNK>\"),\n",
    "    max_len=MAX_SENT_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>impression_time_fixed</th>\n",
       "      <th>scroll_percentage_fixed</th>\n",
       "      <th>article_id_fixed</th>\n",
       "      <th>read_time_fixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13538</td>\n",
       "      <td>['2023-04-27T10:17:43.000000' '2023-04-27T10:1...</td>\n",
       "      <td>[100.  35. 100.  24. 100.  23. 100. 100. 100. ...</td>\n",
       "      <td>[9738663 9738569 9738663 9738490 9738663 97386...</td>\n",
       "      <td>[ 17.  12.   4.   5.   4.   9.   5.  46.  11. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58608</td>\n",
       "      <td>['2023-04-27T18:48:09.000000' '2023-04-27T18:4...</td>\n",
       "      <td>[ 37.  61. 100. 100.  55. 100. 100.  nan  61. ...</td>\n",
       "      <td>[9739362 9739179 9738567 9739344 9739202 97393...</td>\n",
       "      <td>[  2.  24.  72.  65.  11.   4. 101.   0. 699. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>95507</td>\n",
       "      <td>['2023-04-27T15:20:28.000000' '2023-04-27T15:2...</td>\n",
       "      <td>[ 60. 100. 100.  21.  29.  67.  49.  54.  25. ...</td>\n",
       "      <td>[9739035 9738646 9634967 9738902 9735495 97373...</td>\n",
       "      <td>[  18.   29.   51.   12.   10.   10.   13.   2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>106588</td>\n",
       "      <td>['2023-04-27T08:29:09.000000' '2023-04-27T08:2...</td>\n",
       "      <td>[ 24.  57. 100.  nan  nan 100. 100.  73.  26. ...</td>\n",
       "      <td>[9738292 9738216 9737266 9737556 9737657 97377...</td>\n",
       "      <td>[9.000e+00 1.500e+01 4.200e+01 9.000e+00 3.000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>617963</td>\n",
       "      <td>['2023-04-27T14:42:25.000000' '2023-04-27T14:4...</td>\n",
       "      <td>[100. 100.  nan  46.  23.  19.  61.  70.  64. ...</td>\n",
       "      <td>[9739035 9739088 9738902 9738968 9738760 97389...</td>\n",
       "      <td>[  45.   29.  116.   26.   34.   42.   58.   5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>1585</td>\n",
       "      <td>1122370</td>\n",
       "      <td>['2023-05-09T06:50:19.000000' '2023-05-09T06:5...</td>\n",
       "      <td>[100. 100. 100. 100.  nan]</td>\n",
       "      <td>[9755181 9706958 9709329 9706958 9755571]</td>\n",
       "      <td>[ 84. 115. 298.   5.  79.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>1586</td>\n",
       "      <td>1718049</td>\n",
       "      <td>['2023-05-17T05:04:36.000000' '2023-05-17T05:0...</td>\n",
       "      <td>[ 53. 100.  24. 100.  83.  28.  37.  40.  22.]</td>\n",
       "      <td>[9768583 9728166 9768829 9769641 9769994 97696...</td>\n",
       "      <td>[13. 17.  7. 99.  5.  8.  4.  2.  4.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>1587</td>\n",
       "      <td>1178033</td>\n",
       "      <td>['2023-04-29T19:36:47.000000' '2023-04-29T19:3...</td>\n",
       "      <td>[nan 98. 92. 30. 35.  9. 34. 58. nan]</td>\n",
       "      <td>[9742268 9742423 9742270 9742440 9742459 97388...</td>\n",
       "      <td>[ 10.  74.  89.  10.   1.   4.  11.   7. 159.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>1588</td>\n",
       "      <td>395912</td>\n",
       "      <td>['2023-05-12T05:12:34.000000' '2023-05-12T05:1...</td>\n",
       "      <td>[100. 100. 100.  33.  nan]</td>\n",
       "      <td>[9760741 9760998 9760935 9754442 9760935]</td>\n",
       "      <td>[16. 22. 23.  9.  0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>1589</td>\n",
       "      <td>2539047</td>\n",
       "      <td>['2023-05-09T10:37:17.000000' '2023-05-09T10:3...</td>\n",
       "      <td>[ 46. 100.  40. 100. 100. 100.  28. 100. 100. ...</td>\n",
       "      <td>[9755964 9755980 9754081 9755980 9755849 97559...</td>\n",
       "      <td>[ 4. 23. 12.  7. 28.  5.  1. 10. 72. 31.  5. 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1590 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  user_id                              impression_time_fixed  \\\n",
       "0              0    13538  ['2023-04-27T10:17:43.000000' '2023-04-27T10:1...   \n",
       "1              1    58608  ['2023-04-27T18:48:09.000000' '2023-04-27T18:4...   \n",
       "2              2    95507  ['2023-04-27T15:20:28.000000' '2023-04-27T15:2...   \n",
       "3              3   106588  ['2023-04-27T08:29:09.000000' '2023-04-27T08:2...   \n",
       "4              4   617963  ['2023-04-27T14:42:25.000000' '2023-04-27T14:4...   \n",
       "...          ...      ...                                                ...   \n",
       "1585        1585  1122370  ['2023-05-09T06:50:19.000000' '2023-05-09T06:5...   \n",
       "1586        1586  1718049  ['2023-05-17T05:04:36.000000' '2023-05-17T05:0...   \n",
       "1587        1587  1178033  ['2023-04-29T19:36:47.000000' '2023-04-29T19:3...   \n",
       "1588        1588   395912  ['2023-05-12T05:12:34.000000' '2023-05-12T05:1...   \n",
       "1589        1589  2539047  ['2023-05-09T10:37:17.000000' '2023-05-09T10:3...   \n",
       "\n",
       "                                scroll_percentage_fixed  \\\n",
       "0     [100.  35. 100.  24. 100.  23. 100. 100. 100. ...   \n",
       "1     [ 37.  61. 100. 100.  55. 100. 100.  nan  61. ...   \n",
       "2     [ 60. 100. 100.  21.  29.  67.  49.  54.  25. ...   \n",
       "3     [ 24.  57. 100.  nan  nan 100. 100.  73.  26. ...   \n",
       "4     [100. 100.  nan  46.  23.  19.  61.  70.  64. ...   \n",
       "...                                                 ...   \n",
       "1585                         [100. 100. 100. 100.  nan]   \n",
       "1586     [ 53. 100.  24. 100.  83.  28.  37.  40.  22.]   \n",
       "1587              [nan 98. 92. 30. 35.  9. 34. 58. nan]   \n",
       "1588                         [100. 100. 100.  33.  nan]   \n",
       "1589  [ 46. 100.  40. 100. 100. 100.  28. 100. 100. ...   \n",
       "\n",
       "                                       article_id_fixed  \\\n",
       "0     [9738663 9738569 9738663 9738490 9738663 97386...   \n",
       "1     [9739362 9739179 9738567 9739344 9739202 97393...   \n",
       "2     [9739035 9738646 9634967 9738902 9735495 97373...   \n",
       "3     [9738292 9738216 9737266 9737556 9737657 97377...   \n",
       "4     [9739035 9739088 9738902 9738968 9738760 97389...   \n",
       "...                                                 ...   \n",
       "1585          [9755181 9706958 9709329 9706958 9755571]   \n",
       "1586  [9768583 9728166 9768829 9769641 9769994 97696...   \n",
       "1587  [9742268 9742423 9742270 9742440 9742459 97388...   \n",
       "1588          [9760741 9760998 9760935 9754442 9760935]   \n",
       "1589  [9755964 9755980 9754081 9755980 9755849 97559...   \n",
       "\n",
       "                                        read_time_fixed  \n",
       "0     [ 17.  12.   4.   5.   4.   9.   5.  46.  11. ...  \n",
       "1     [  2.  24.  72.  65.  11.   4. 101.   0. 699. ...  \n",
       "2     [  18.   29.   51.   12.   10.   10.   13.   2...  \n",
       "3     [9.000e+00 1.500e+01 4.200e+01 9.000e+00 3.000...  \n",
       "4     [  45.   29.  116.   26.   34.   42.   58.   5...  \n",
       "...                                                 ...  \n",
       "1585                         [ 84. 115. 298.   5.  79.]  \n",
       "1586              [13. 17.  7. 99.  5.  8.  4.  2.  4.]  \n",
       "1587     [ 10.  74.  89.  10.   1.   4.  11.   7. 159.]  \n",
       "1588                              [16. 22. 23.  9.  0.]  \n",
       "1589  [ 4. 23. 12.  7. 28.  5.  1. 10. 72. 31.  5. 1...  \n",
       "\n",
       "[1590 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 15:51:57,536 - WARNING - Malformed article_ids found: '[9738366 9737535 9738173 ... 9766140 9766140 9766140]'. Skipping.\n",
      "2024-12-07 15:51:57,541 - WARNING - Malformed article_ids found: '[9737083 9737083 9738216 ... 9770037 9769994 9768321]'. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: Skipped 2 rows out of 1590 (0.13%).\n",
      "train: Saved 2 problematic rows to 'invalid_article_ids_train.csv'.\n",
      "train: Remaining rows after cleaning: 1588\n",
      "val: Skipped 0 rows out of 1562 (0.00%).\n",
      "val: Saved 0 problematic rows to 'invalid_article_ids_val.csv'.\n",
      "val: Remaining rows after cleaning: 1562\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Setup logging (if not already configured in your project)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def clean_article_ids(article_ids):\n",
    "    \"\"\"\n",
    "    Cleans and parses article IDs from a string representation to a list of integers.\n",
    "    \n",
    "    Args:\n",
    "        article_ids (str): String representation of article IDs (e.g., \"[1, 2, 3]\").\n",
    "    \n",
    "    Returns:\n",
    "        list of int or None: List of parsed article IDs, or None if input is invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check for invalid placeholders or empty strings\n",
    "        if not article_ids or \"...\" in article_ids:\n",
    "            logging.warning(f\"Malformed article_ids found: '{article_ids}'. Skipping.\")\n",
    "            return None\n",
    "        \n",
    "        # Remove brackets and split on spaces or commas, then convert to integers\n",
    "        cleaned_ids = article_ids.strip(\"[]\").replace(\",\", \" \").split()\n",
    "        return list(map(int, cleaned_ids))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to clean article_ids '{article_ids}' due to error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process a single dataset (train or val)\n",
    "def clean_and_report_history(history_df, dataset_name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Cleans article IDs in the user history dataset and reports cleaning statistics.\n",
    "\n",
    "    Args:\n",
    "        history_df (pd.DataFrame): Input dataset with article history to clean.\n",
    "        dataset_name (str): Name of the dataset (for logging and reporting).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataset with invalid rows removed.\n",
    "    \"\"\"\n",
    "    # Clean article IDs\n",
    "    history_df[\"cleaned_article_ids\"] = history_df[\"article_id_fixed\"].apply(clean_article_ids)\n",
    "\n",
    "    # Count skipped rows\n",
    "    skipped_rows = history_df[\"cleaned_article_ids\"].isna().sum()\n",
    "    total_rows = len(history_df)\n",
    "    print(f\"{dataset_name}: Skipped {skipped_rows} rows out of {total_rows} \"\n",
    "          f\"({skipped_rows / total_rows:.2%}).\")\n",
    "\n",
    "    # Save problematic rows\n",
    "    invalid_rows = history_df[history_df[\"cleaned_article_ids\"].isna()]\n",
    "    invalid_rows_file = f\"invalid_article_ids_{dataset_name}.csv\"\n",
    "    invalid_rows.to_csv(invalid_rows_file, index=False)\n",
    "    print(f\"{dataset_name}: Saved {len(invalid_rows)} problematic rows to '{invalid_rows_file}'.\")\n",
    "\n",
    "    # Drop invalid rows and reset index\n",
    "    cleaned_df = history_df.dropna(subset=[\"cleaned_article_ids\"]).reset_index(drop=True)\n",
    "    print(f\"{dataset_name}: Remaining rows after cleaning: {len(cleaned_df)}\")\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "# Clean train and validation datasets\n",
    "history_train_cleaned = clean_and_report_history(history_train, dataset_name=\"train\")\n",
    "history_val_cleaned = clean_and_report_history(history_val, dataset_name=\"val\")\n",
    "\n",
    "\n",
    "def process_cleaned_user_history(cleaned_history_df):\n",
    "    user_histories = defaultdict(list)\n",
    "    for _, row in cleaned_history_df.iterrows():\n",
    "        user_id = row[\"user_id\"]\n",
    "        article_ids = row[\"cleaned_article_ids\"]\n",
    "        user_histories[user_id].extend(article_ids)\n",
    "    return user_histories\n",
    "\n",
    "user_history_train_cleaned = process_cleaned_user_history(history_train_cleaned)\n",
    "user_history_val_cleaned = process_cleaned_user_history(history_val_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 18591\n",
      "Sample Tokens: ['<PAD>', '<UNK>', 'ishockey-spiller:', 'jeg', 'troede', 'skulle', 'dÃ¸', 'prins', 'harry', 'tvunget']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
    "print(f\"Sample Tokens: {list(vocab.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to Data/vocab.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data_folder = \"Data\"\n",
    "vocab_file = os.path.join(data_folder, \"vocab.json\")\n",
    "\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Save the vocabulary to a file\n",
    "with open(vocab_file, \"w\") as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "print(f\"Vocabulary saved to {vocab_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def create_samples(behaviors_df, user_history, npratio=NPRATIO, max_sents=MAX_SENTS, max_sent_length=MAX_SENT_LENGTH):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    for _, row in behaviors_df.iterrows():\n",
    "        user_id = row[\"user_id\"]\n",
    "\n",
    "        clicked_articles = clean_article_ids(row['article_ids_clicked'])\n",
    "        inview_articles = clean_article_ids(row['article_ids_inview'])\n",
    "\n",
    "        if clicked_articles is None or inview_articles is None:\n",
    "            continue\n",
    "\n",
    "        # Fetch, truncate, and pad user history\n",
    "        user_hist = user_history.get(user_id, [])\n",
    "        if len(user_hist) > max_sents:\n",
    "            user_hist = user_hist[:max_sents]\n",
    "        if len(user_hist) < max_sents:\n",
    "            user_hist += [0] * (max_sents - len(user_hist))\n",
    "\n",
    "        # Positive sampling\n",
    "        for article_id in clicked_articles:\n",
    "            candidate = [article_id] + [0] * (max_sent_length - 1) # Pre-pad candidate\n",
    "            samples.append((user_hist, candidate))\n",
    "            labels.append(1)\n",
    "\n",
    "        # Negative sampling\n",
    "        negative_articles = list(set(inview_articles) - set(clicked_articles))\n",
    "        if len(negative_articles) > npratio * len(clicked_articles):\n",
    "            negative_articles = sample(negative_articles, npratio * len(clicked_articles))\n",
    "        for article_id in negative_articles:\n",
    "            candidate = [article_id] + [0] * (max_sent_length - 1)  # Pre-pad candidate\n",
    "            samples.append((user_hist, candidate))\n",
    "            labels.append(0)\n",
    "\n",
    "    return samples, labels\n",
    "\n",
    "train_samples_cleaned, train_labels_cleaned = create_samples(\n",
    "    behaviors_train, user_history_train_cleaned, npratio=NPRATIO\n",
    "    )\n",
    "val_samples_cleaned, val_labels_cleaned = create_samples(\n",
    "    behaviors_val, user_history_val_cleaned, npratio=NPRATIO\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Define PyTorch Dataset\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_history, candidate = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(user_history, dtype=torch.long),\n",
    "            torch.tensor(candidate, dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        )\n",
    "\n",
    "train_dataset = NewsDataset(train_samples_cleaned, train_labels_cleaned)\n",
    "val_dataset = NewsDataset(val_samples_cleaned, val_labels_cleaned)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max user_history index: 9770989\n",
      "Max candidate index: 9779657\n",
      "VOCAB_SIZE: 18591\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    user_histories, candidates, labels = batch\n",
    "    print(f\"Max user_history index: {user_histories.max().item()}\")\n",
    "    print(f\"Max candidate index: {candidates.max().item()}\")\n",
    "    print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define the NRMS Model\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.output_dim = num_heads * head_size\n",
    "        self.qkv_linear = nn.Linear(EMBEDDING_DIM, self.output_dim * 3)\n",
    "        self.fc_out = nn.Linear(self.output_dim, EMBEDDING_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) != 3:\n",
    "            raise ValueError(f\"Expected input to be 3D (batch_size, seq_length, embed_dim), got {x.size()}\")\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_linear(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_size)\n",
    "        qkv = qkv.permute(2, 0, 1, 3)\n",
    "        Q, K, V = torch.chunk(qkv, 3, dim=-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        weighted = torch.matmul(attention, V)\n",
    "        weighted = weighted.permute(1, 2, 0, 3).reshape(batch_size, seq_length, self.output_dim)\n",
    "        return self.fc_out(weighted)\n",
    "\n",
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        self.self_attention = MultiHeadSelfAttention(NUM_HEADS, HEAD_SIZE)\n",
    "        self.dense = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Ensure input is embedded\n",
    "        if len(x.size()) != 3:\n",
    "            raise ValueError(f\"Embedding layer output should be 3D, got {x.size()}\")\n",
    "        x = self.dropout(x)\n",
    "        x = self.self_attention(x)\n",
    "        attention_weights = F.softmax(self.dense(x).squeeze(-1), dim=-1) # attention weights\n",
    "        return torch.sum(x * attention_weights.unsqueeze(-1), dim=1) # weighted sum\n",
    "\n",
    "class NRMS(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.title_encoder = TitleEncoder(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, candidates, user_history):\n",
    "        user_rep = self.title_encoder(user_history)  # Output: (batch_size, embedding_dim)\n",
    "        candidate_rep = self.title_encoder(candidates)  # Output: (batch_size, embedding_dim)\n",
    "        return torch.matmul(candidate_rep, user_rep.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_ndcg(labels, scores, k=10):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG) at rank k.\n",
    "    \n",
    "    Args:\n",
    "        labels (list of int): Binary labels indicating relevance (1 for relevant, 0 for irrelevant).\n",
    "        scores (list of float): Predicted scores for ranking.\n",
    "        k (int): Rank threshold for NDCG calculation.\n",
    "\n",
    "    Returns:\n",
    "        float: NDCG@k value.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    scores = torch.tensor(scores, dtype=torch.float32)\n",
    "\n",
    "    _, sorted_indices = torch.topk(scores, k=k)\n",
    "    sorted_labels = labels[sorted_indices]\n",
    "\n",
    "    # Calculate DCG\n",
    "    gains = (2 ** sorted_labels - 1) / torch.log2(torch.arange(2, k + 2, dtype=torch.float32))\n",
    "    dcg = torch.sum(gains)\n",
    "\n",
    "    # Calculate IDCG\n",
    "    ideal_labels = torch.sort(labels, descending=True).values[:k]\n",
    "    ideal_gains = (2 ** ideal_labels - 1) / torch.log2(torch.arange(2, k + 2, dtype=torch.float32))\n",
    "    idcg = torch.sum(ideal_gains)\n",
    "\n",
    "    return (dcg / idcg).item() if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            user_histories, candidates, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(candidates, user_histories)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        ndcg_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                user_histories, candidates, labels = batch\n",
    "                outputs = model(candidates, user_histories)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # calculations of predictions and metrics\n",
    "                _, preds = torch.max(outputs, dim=1)  # Get predicted class\n",
    "                all_preds.extend(preds.tolist())\n",
    "                all_labels.extend(labels.tolist())\n",
    "\n",
    "                # Calculate NDCG@k\n",
    "                for i in range(outputs.size(0)):  # Iterate through the batch\n",
    "                    ndcg = calculate_ndcg(labels[i].tolist(), outputs[i].tolist(), k=10)\n",
    "                    ndcg_scores.append(ndcg)\n",
    "\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        avg_ndcg = sum(ndcg_scores) / len(ndcg_scores)\n",
    "\n",
    "        # Logging\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {total_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {total_val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Val NDCG@10: {avg_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     43\u001b[0m user_histories, candidates, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_histories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[43], line 47\u001b[0m, in \u001b[0;36mNRMS.forward\u001b[0;34m(self, candidates, user_history)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, candidates, user_history):\n\u001b[0;32m---> 47\u001b[0m     user_rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_history\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Output: (batch_size, embedding_dim)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     candidate_rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtitle_encoder(candidates)  \u001b[38;5;66;03m# Output: (batch_size, embedding_dim)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmatmul(candidate_rep, user_rep\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[43], line 33\u001b[0m, in \u001b[0;36mTitleEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure input is embedded\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding layer output should be 3D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model = NRMS(VOCAB_SIZE, EMBEDDING_DIM, NPRATIO + 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    epochs=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

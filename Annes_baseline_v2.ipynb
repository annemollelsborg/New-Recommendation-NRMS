{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added som stuff to improve accuracy.\n",
    "\n",
    "1. Add Gradient Clipping (training loop)\n",
    "2. Add Learning Rate Scheduler (training loop, end of each epoch)\n",
    "3. In the NewsEncoder and UserEncoder classes, add dropout after the attention layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24724/24724 [00:03<00:00, 7096.58it/s]\n",
      "100%|██████████| 25356/25356 [00:03<00:00, 8415.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load datasets\n",
    "behaviors_train = pd.read_parquet('Data/behaviors_train.parquet')\n",
    "behaviors_val = pd.read_parquet('Data/behaviors_val.parquet')\n",
    "history_train = pd.read_parquet('Data/history_train.parquet')\n",
    "history_val = pd.read_parquet('Data/history_val.parquet')\n",
    "articles = pd.read_parquet('Data/articles.parquet')\n",
    "\n",
    "# Filter valid articles\n",
    "valid_articles = set(articles['article_id'])\n",
    "def filter_invalid_articles(df, col):\n",
    "    df[col] = df[col].apply(lambda x: [a for a in x if a in valid_articles])\n",
    "\n",
    "filter_invalid_articles(behaviors_train, 'article_ids_clicked')\n",
    "filter_invalid_articles(behaviors_train, 'article_ids_inview')\n",
    "filter_invalid_articles(behaviors_val, 'article_ids_clicked')\n",
    "filter_invalid_articles(behaviors_val, 'article_ids_inview')\n",
    "filter_invalid_articles(history_train, 'article_id_fixed')\n",
    "filter_invalid_articles(history_val, 'article_id_fixed')\n",
    "\n",
    "# Generate candidate and label columns\n",
    "def generate_candidates_and_labels(behaviors, history):\n",
    "    data = []\n",
    "    for _, row in tqdm(behaviors.iterrows(), total=len(behaviors)):\n",
    "        user_id = row['user_id']\n",
    "        clicked_articles = row['article_ids_clicked']\n",
    "        inview_articles = row['article_ids_inview']\n",
    "        \n",
    "        user_history = history[history['user_id'] == user_id]['article_id_fixed'].values\n",
    "        user_history = user_history[0] if len(user_history) > 0 else []\n",
    "        \n",
    "        # Prepare user_his (pad with zeros)\n",
    "        user_his = [a for a in user_history if a not in clicked_articles]\n",
    "        user_his = user_his[:50] + [0] * max(0, 50 - len(user_his))\n",
    "        \n",
    "        for clicked_article in clicked_articles:\n",
    "            negative_samples = [a for a in inview_articles if a != clicked_article]\n",
    "            if len(negative_samples) < 4:\n",
    "                continue\n",
    "            negatives = np.random.choice(negative_samples, 4, replace=False).tolist()\n",
    "            candidate = [clicked_article] + negatives\n",
    "            label = [1] + [0] * 4\n",
    "            data.append({'candidate': candidate, 'label': label, 'user_his': user_his})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_data = generate_candidates_and_labels(behaviors_train, history_train)\n",
    "val_data = generate_candidates_and_labels(behaviors_val, history_val)\n",
    "\n",
    "# Save processed data\n",
    "train_data.to_parquet('Data/train_data.parquet')\n",
    "val_data.to_parquet('Data/val_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path, embedding_dim):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_path = 'Data/glove.6B.300d.txt'\n",
    "embedding_dim = 300\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "\n",
    "# Tokenize and embed titles\n",
    "def embed_titles(articles, glove_embeddings, embedding_dim):\n",
    "    def embed_title(title):\n",
    "        tokens = title.lower().split()\n",
    "        vectors = [glove_embeddings.get(token, np.zeros(embedding_dim)) for token in tokens]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(embedding_dim)\n",
    "\n",
    "    articles['embedding'] = articles['title'].apply(embed_title)\n",
    "    return articles\n",
    "\n",
    "articles = embed_titles(articles, glove_embeddings, embedding_dim)\n",
    "\n",
    "# Convert embeddings to a tensor\n",
    "article_embeddings = {\n",
    "    article_id: torch.tensor(embedding, dtype=torch.float32)\n",
    "    for article_id, embedding in zip(articles['article_id'], articles['embedding'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, attention_heads):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=attention_heads, batch_first=True)\n",
    "        self.additive_attention = nn.Linear(embedding_dim, 1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        attn_output, _ = self.multihead_attention(embeddings, embeddings, embeddings)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        scores = self.additive_attention(attn_output).squeeze(-1)\n",
    "        if len(scores.shape) == 1:\n",
    "            scores = scores.unsqueeze(1)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        representation = torch.sum(weights.unsqueeze(-1) * attn_output, dim=1)\n",
    "        return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, news_encoder, embedding_dim, attention_heads):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        self.news_encoder = news_encoder\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=attention_heads, batch_first=True)\n",
    "        self.additive_attention = nn.Linear(embedding_dim, 1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, user_history):\n",
    "        news_embeddings = torch.stack([self.news_encoder(h.unsqueeze(0)) for h in user_history], dim=1)\n",
    "        attn_output, _ = self.multihead_attention(news_embeddings, news_embeddings, news_embeddings)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        scores = self.additive_attention(attn_output).squeeze(-1)\n",
    "        if len(scores.shape) == 1:\n",
    "            scores = scores.unsqueeze(1)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        user_representation = torch.sum(weights.unsqueeze(-1) * attn_output, dim=1)\n",
    "        return user_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsRecommendationModel(nn.Module):\n",
    "    def __init__(self, news_encoder, user_encoder):\n",
    "        super(NewsRecommendationModel, self).__init__()\n",
    "        self.news_encoder = news_encoder\n",
    "        self.user_encoder = user_encoder\n",
    "\n",
    "    def forward(self, candidate_articles, user_history):\n",
    "        batch_size, num_candidates, embedding_dim = candidate_articles.size()\n",
    "        candidate_representations = torch.stack(\n",
    "            [self.news_encoder(candidate_articles[:, i, :]) for i in range(num_candidates)],\n",
    "            dim=1\n",
    "        )\n",
    "        user_representation = self.user_encoder(user_history)\n",
    "        scores = torch.matmul(candidate_representations, user_representation.unsqueeze(-1)).squeeze(-1)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "embedding_dim = 300\n",
    "attention_heads = 4\n",
    "news_encoder = NewsEncoder(embedding_dim, attention_heads).to(device)\n",
    "user_encoder = UserEncoder(news_encoder, embedding_dim, attention_heads).to(device)\n",
    "model = NewsRecommendationModel(news_encoder, user_encoder).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "def create_dataloader(data, article_embeddings, batch_size=32):\n",
    "    def collate_fn(batch):\n",
    "        zero_embedding = torch.zeros(embedding_dim, dtype=torch.float32).to(device)\n",
    "        candidates = torch.stack([\n",
    "            torch.stack([article_embeddings.get(c, zero_embedding) for c in b['candidate']]) for b in batch\n",
    "        ]).to(device)\n",
    "        user_his = torch.stack([\n",
    "            torch.stack([article_embeddings.get(h, zero_embedding) for h in b['user_his']]) for b in batch\n",
    "        ]).to(device)\n",
    "        labels = torch.tensor([b['label'].index(1) for b in batch], dtype=torch.long).to(device)\n",
    "        return candidates, user_his, labels\n",
    "\n",
    "    dataset = data.to_dict('records')\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "train_loader = create_dataloader(train_data, article_embeddings)\n",
    "val_loader = create_dataloader(val_data, article_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.9321749352575204\n",
      "Validation Accuracy: 0.46673201333071945\n",
      "Epoch 2, Loss: 1.4175145518333452\n",
      "Validation Accuracy: 0.5106449715742012\n",
      "Epoch 3, Loss: 1.5894381787737883\n",
      "Validation Accuracy: 0.49057047637718093\n",
      "Epoch 4, Loss: 0.5366730769850249\n",
      "Validation Accuracy: 0.49684375612624976\n",
      "Epoch 5, Loss: 0.4343381876810853\n",
      "Validation Accuracy: 0.5294648108214076\n",
      "Epoch 6, Loss: 0.4858357730237046\n",
      "Validation Accuracy: 0.5420113703195452\n",
      "Epoch 7, Loss: 0.38932927739501005\n",
      "Validation Accuracy: 0.5407567143697314\n",
      "Epoch 8, Loss: 0.4216081239355203\n",
      "Validation Accuracy: 0.5144089394236424\n",
      "Epoch 9, Loss: 0.3566326833235082\n",
      "Validation Accuracy: 0.5157028033718879\n",
      "Epoch 10, Loss: 0.3897931775072478\n",
      "Validation Accuracy: 0.5357380905704764\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for candidates, user_his, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(candidates, user_his)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for candidates, user_his, labels in val_loader:\n",
    "            predictions = model(candidates, user_his)\n",
    "            correct += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f\"Validation Accuracy: {correct / total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24724/24724 [00:03<00:00, 8136.49it/s]\n",
      "100%|██████████| 25356/25356 [00:03<00:00, 8154.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load datasets\n",
    "behaviors_train = pd.read_parquet('Data/behaviors_train.parquet')\n",
    "behaviors_val = pd.read_parquet('Data/behaviors_val.parquet')\n",
    "history_train = pd.read_parquet('Data/history_train.parquet')\n",
    "history_val = pd.read_parquet('Data/history_val.parquet')\n",
    "articles = pd.read_parquet('Data/articles.parquet')\n",
    "\n",
    "# Filter valid articles\n",
    "valid_articles = set(articles['article_id'])\n",
    "def filter_invalid_articles(df, col):\n",
    "    df[col] = df[col].apply(lambda x: [a for a in x if a in valid_articles])\n",
    "\n",
    "filter_invalid_articles(behaviors_train, 'article_ids_clicked')\n",
    "filter_invalid_articles(behaviors_train, 'article_ids_inview')\n",
    "filter_invalid_articles(behaviors_val, 'article_ids_clicked')\n",
    "filter_invalid_articles(behaviors_val, 'article_ids_inview')\n",
    "filter_invalid_articles(history_train, 'article_id_fixed')\n",
    "filter_invalid_articles(history_val, 'article_id_fixed')\n",
    "\n",
    "# Generate candidate and label columns\n",
    "def generate_candidates_and_labels(behaviors, history):\n",
    "    data = []\n",
    "    for _, row in tqdm(behaviors.iterrows(), total=len(behaviors)):\n",
    "        user_id = row['user_id']\n",
    "        clicked_articles = row['article_ids_clicked']\n",
    "        inview_articles = row['article_ids_inview']\n",
    "        \n",
    "        user_history = history[history['user_id'] == user_id]['article_id_fixed'].values\n",
    "        user_history = user_history[0] if len(user_history) > 0 else []\n",
    "        \n",
    "        # Prepare user_his (pad with zeros)\n",
    "        user_his = [a for a in user_history if a not in clicked_articles]\n",
    "        user_his = user_his[:50] + [0] * max(0, 50 - len(user_his))\n",
    "        \n",
    "        for clicked_article in clicked_articles:\n",
    "            negative_samples = [a for a in inview_articles if a != clicked_article]\n",
    "            if len(negative_samples) < 4:\n",
    "                continue\n",
    "            negatives = np.random.choice(negative_samples, 4, replace=False).tolist()\n",
    "            candidate = [clicked_article] + negatives\n",
    "            label = [1] + [0] * 4\n",
    "            data.append({'candidate': candidate, 'label': label, 'user_his': user_his})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_data = generate_candidates_and_labels(behaviors_train, history_train)\n",
    "val_data = generate_candidates_and_labels(behaviors_val, history_val)\n",
    "\n",
    "# Save processed data\n",
    "train_data.to_parquet('Data/train_data.parquet')\n",
    "val_data.to_parquet('Data/val_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path, embedding_dim):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_path = 'Data/glove.6B.300d.txt'\n",
    "embedding_dim = 300\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "\n",
    "# Tokenize and embed titles\n",
    "def embed_titles(articles, glove_embeddings, embedding_dim):\n",
    "    def embed_title(title):\n",
    "        tokens = title.lower().split()\n",
    "        vectors = [glove_embeddings.get(token, np.zeros(embedding_dim)) for token in tokens]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(embedding_dim)\n",
    "\n",
    "    articles['embedding'] = articles['title'].apply(embed_title)\n",
    "    return articles\n",
    "\n",
    "articles = embed_titles(articles, glove_embeddings, embedding_dim)\n",
    "\n",
    "# Convert embeddings to a tensor\n",
    "article_embeddings = {\n",
    "    article_id: torch.tensor(embedding, dtype=torch.float32)\n",
    "    for article_id, embedding in zip(articles['article_id'], articles['embedding'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, attention_heads):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=attention_heads, batch_first=True)\n",
    "        self.additive_attention = nn.Linear(embedding_dim, 1)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        attn_output, _ = self.multihead_attention(embeddings, embeddings, embeddings)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        scores = self.additive_attention(attn_output).squeeze(-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Ensure softmax is applied on the correct dimension\n",
    "        if len(scores.shape) == 1:  # This happens if seq_len is 1\n",
    "            scores = scores.unsqueeze(1)  # Restore shape to (batch_size, seq_len)\n",
    "\n",
    "        weights = torch.softmax(scores, dim=1)  # Shape: (batch_size, seq_len)\n",
    "        representation = torch.sum(weights.unsqueeze(-1) * attn_output, dim=1)  # Shape: (batch_size, embedding_dim)\n",
    "        return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, news_encoder, embedding_dim, attention_heads):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        self.news_encoder = news_encoder\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=attention_heads, batch_first=True)\n",
    "        self.additive_attention = nn.Linear(embedding_dim, 1)\n",
    "    \n",
    "    def forward(self, user_history):\n",
    "        news_embeddings = torch.stack([self.news_encoder(h.unsqueeze(0)) for h in user_history], dim=1)\n",
    "        attn_output, _ = self.multihead_attention(news_embeddings, news_embeddings, news_embeddings)\n",
    "        scores = self.additive_attention(attn_output).squeeze(-1)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        user_representation = torch.sum(weights.unsqueeze(-1) * attn_output, dim=1)\n",
    "        return user_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsRecommendationModel(nn.Module):\n",
    "    def __init__(self, news_encoder, user_encoder):\n",
    "        super(NewsRecommendationModel, self).__init__()\n",
    "        self.news_encoder = news_encoder\n",
    "        self.user_encoder = user_encoder\n",
    "\n",
    "    def forward(self, candidate_articles, user_history):\n",
    "        # Encode candidate articles\n",
    "        batch_size, num_candidates, embedding_dim = candidate_articles.size()\n",
    "        candidate_representations = torch.stack(\n",
    "            [self.news_encoder(candidate_articles[:, i, :]) for i in range(num_candidates)],\n",
    "            dim=1\n",
    "        )  # Shape: (batch_size, num_candidates, embedding_dim)\n",
    "\n",
    "        # Encode user history\n",
    "        user_representation = self.user_encoder(user_history)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Compute scores (dot product)\n",
    "        scores = torch.matmul(candidate_representations, user_representation.unsqueeze(-1)).squeeze(-1)\n",
    "        # Correct shape: (batch_size, num_candidates)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "embedding_dim = 300\n",
    "attention_heads = 4\n",
    "news_encoder = NewsEncoder(embedding_dim, attention_heads).to(device)\n",
    "user_encoder = UserEncoder(news_encoder, embedding_dim, attention_heads).to(device)\n",
    "model = NewsRecommendationModel(news_encoder, user_encoder).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def create_dataloader(data, article_embeddings, batch_size=32):\n",
    "    def collate_fn(batch):\n",
    "        # Embedding for zero (used as padding)\n",
    "        zero_embedding = torch.zeros(embedding_dim, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Process candidates and user history\n",
    "        candidates = torch.stack([\n",
    "            torch.stack([article_embeddings.get(c, zero_embedding) for c in b['candidate']]) for b in batch\n",
    "        ]).to(device)\n",
    "        \n",
    "        user_his = torch.stack([\n",
    "            torch.stack([article_embeddings.get(h, zero_embedding) for h in b['user_his']]) for b in batch\n",
    "        ]).to(device)\n",
    "        \n",
    "        # Extract labels\n",
    "        labels = torch.tensor([b['label'].index(1) for b in batch], dtype=torch.long).to(device)\n",
    "        \n",
    "        return candidates, user_his, labels\n",
    "\n",
    "    dataset = data.to_dict('records')\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "train_loader = create_dataloader(train_data, article_embeddings)\n",
    "val_loader = create_dataloader(val_data, article_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.090467449475482\n",
      "Validation Accuracy: 0.4278376788864928\n",
      "Epoch 2, Loss: 2.679478529688445\n",
      "Validation Accuracy: 0.4629680454812782\n",
      "Epoch 3, Loss: 0.4903111373662521\n",
      "Validation Accuracy: 0.4127818074887277\n",
      "Epoch 4, Loss: 0.4155974502220086\n",
      "Validation Accuracy: 0.5144089394236424\n",
      "Epoch 5, Loss: 0.43726674818795785\n",
      "Validation Accuracy: 0.5282101548715938\n",
      "Epoch 6, Loss: 0.3809046124222338\n",
      "Validation Accuracy: 0.4943736522250539\n",
      "Epoch 7, Loss: 116.83260602514457\n",
      "Validation Accuracy: 0.4403842383846305\n",
      "Epoch 8, Loss: 3.4603307631606604\n",
      "Validation Accuracy: 0.4504214859831406\n",
      "Epoch 9, Loss: 2.6540008100631134\n",
      "Validation Accuracy: 0.447912174083513\n",
      "Epoch 10, Loss: 1.2056929820358488\n",
      "Validation Accuracy: 0.5018623799255048\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for candidates, user_his, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(candidates, user_his)  # Shape: (batch_size, num_candidates)\n",
    "        loss = criterion(predictions, labels)     # Labels: (batch_size)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for candidates, user_his, labels in val_loader:\n",
    "            predictions = model(candidates, user_his)\n",
    "            correct += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f\"Validation Accuracy: {correct / total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
